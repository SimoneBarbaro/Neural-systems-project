import numpy as np


def get_unique_token_occurrences(tokenized_string):
    """
    get occurrences of each token.
    :param tokenized_string: string tokenized.
    :return: a map from tokens to frequencies in the input.
    """
    unique_token_occurance = {}
    # for each unique token in the query, count its occurance in the query, which is used for compute the term frequency.
    for token in tokenized_string:
        unique_token_occurance[token] = unique_token_occurance.get(token, 0) + 1
    return unique_token_occurance


def add_doc_unigrams(index, doc_id, doc_tokens):
    """
    fill index with words from a document
    :param index: index to be filled.
    :param doc_id: document id.
    :param doc_tokens: tokens of document.
    """
    unique_words_in_doc = set()
    for word in doc_tokens:
        if word not in unique_words_in_doc:
            if word not in index:
                index[word] = np.asarray([], dtype=np.uint32)
            index[word] = np.append(index[word], np.asarray(doc_id, dtype=np.uint32))
            unique_words_in_doc.add(word)


class InvertedIndex:
    """
    Wrapper to help work with the inverted index.
    """
    def __init__(self, corpus, preprocessing_fn):
        """
        :param corpus: corpus of documents.
        :param preprocessing_fn: preprocessing function to tokenize corpus.
        """
        self.inverted_index = dict()
        doc_id = 0
        for doc in corpus:
            add_doc_unigrams(self.inverted_index, doc_id, preprocessing_fn(doc))
            doc_id += 1

    def get_doc_ids(self, token):
        """
        get the ids of the documents in the corpus containing a particular token.
        :param token: token to be searched for in the index.
        :return: a list of document ids.
        """
        return self.inverted_index.get(token, np.array([], np.uint32))

    def get_doc_id_list(self, tokens):
        """
        Wrapper to execute get_doc_ids for multiple tokens easier.
        :param tokens: a list of tokens.
        :return: a list of results, each element containing the list that
        would have been returned by get_doc_ids for the corresponding token.
        """
        result = []
        for token in tokens:
            result.append(self.get_doc_ids(token))
        return result


class KeywordScorer:
    """
    Abstract class to score tokens in a query to generate keywords.
    """

    def score_keywords(self, tokenized_query):
        """
        Abstract method that return scores for the tokens in the input.
        :param tokenized_query: a list containing tokens.
        :return: a list containing the scores assigned to the input list.
        """
        raise NotImplementedError


class TfidfKeywordScorer(KeywordScorer):
    """
    Scorer based on tf-idf metric.
    """
    def __init__(self, inverted_index: InvertedIndex, num_documents):
        """
        :param inverted_index: inverted index, used to compute the idf.
        :param num_documents: number of documents in the corpus, used to compute the idf.
        """
        self.inverted_index = inverted_index

        self.N = num_documents

    def score_keywords(self, tokenized_query):
        """
        Implementation of the abstract method.
        """
        unique_token_occurance = get_unique_token_occurrences(tokenized_query)
        unique_token_list = list(unique_token_occurance.keys())
        tfidf_score_list = []
        for token in unique_token_list:
            # compute term frequency
            tf = unique_token_occurance[token] / len(tokenized_query)
            # compute the inverse document frequency
            idf = np.log(self.N / (len(self.inverted_index.get_doc_ids(token)) + 1))
            tfidf_score_list.append(tf * idf)
        return tfidf_score_list


class KeywordSelector:
    """
    Selector of keywords based on scores generated by a scorer.
    The purpose of this class is to group together multiple possible strategies to select keywords.
    The logic of which strategy to be used will be delegathed to the classes using this one.
    """
    def __init__(self, keyword_scorer: KeywordScorer):
        """
        :param keyword_scorer: the scorer object used to evaluate tokens.
        """
        self.scorer = keyword_scorer

    def select_k(self, tokenized_query, k):
        """
        Simple strategy that select a fixed number of keywords taken by decreasing score.
        :param tokenized_query: a list ok tokens.
        :param k: number of keys to be selected.
        :return: a list of the tokens selected from the input.
        """
        unique_token_occurance = get_unique_token_occurrences(tokenized_query)
        unique_token_list = list(unique_token_occurance.keys())
        selected_token_ids = np.argsort(-self.scorer.score_keywords(tokenized_query))[:k]
        return [unique_token_list[idx] for idx in selected_token_ids]
