doc_id,discussion_id,text
1421389,0.0," HSV-1 encephalitis usually presents fulminantly in immunocompetent individuals with fever, altered mental status, seizures, CSF and EEG findings, and temporal lobe lesions on MRI. Our patient's illness began with recurrent herpes simplex labialis, which spread to the left trigeminal nerve and left L5 nerve root over the course of four weeks. Besides her dysesthesias, her symptoms included headache, low-grade fever, weight loss, mild personality change and short-term memory loss, which were no longer exhibited on presentation. Her CSF contained mildly elevated levels of lymphocytes (WBC: 6 cells/μL), erythrocytes (RBC: 63 cells/μL), and protein (49 mg/dL); owing to the fact that cell counts were taken from the fourth tube collected, the probability of a traumatic lumbar puncture is low. Beyond her MRI findings of peri-ventriculitis and basal meningitis particularly over the brainstem, our patient had no temporal lobe abnormalities, although approximately 10% of patients with PCR-proven HSV encephalitis do not demonstrate temporal lobe involvement. In addition, other cases of brainstem encephalitis due to HSV have been reported with viral reactivation possibly occurring in the trigeminal nerve (relevant references are available upon request from the corresponding author). Regarding involvement of white matter, other cases of extratemporal HSV encephalitis have also shown this pattern on neuroimaging, particularly during chronic or subacute phases of illness; often being associated with clinical relapse and viral persistence; and sometimes showing demyelination in addition to edema, inflammatory change, and viral inclusions on a microscopic level (relevant references are available upon request from the corresponding author). Overall, our patient's clinical presentation, borderline pleocytosis in the CSF, and extratemporal findings on MRI were consistent with a more subacute course of HSV-1 encephalitis proven by PCR, presumably due to immunosuppression from CLL. "
1421389,1.0," Patients with CLL often manifest hypogammaglobulinemia and neutropenia early in the course of the disease, with eventual T cell deficiency leading to recurrent sinopulmonary infections with Gram-negative bacteria, fungi, and herpes zoster and simplex viruses. However, HSV-1 encephalitis is a rare presentation of CLL. Other central nervous system (CNS) infections have been reported in patients with CLL, namely progressive multifocal leukoencephalopathy, West Nile virus encephalitis, and Listeria monocytogenes encephalitis. The occurrence of HSV-1 encephalitis in immunocompromised patients has rarely been reported. Classic clinical presentations with atypical laboratory studies have been described in a patient with metastatic colon cancer four weeks after receiving chemotherapy [2], three patients who were immunosuppressed with either Hodgkin's or non-Hodgkin's lymphoma [3–5], and another patient with glioblastoma multiforme on dexamethasone [5]. Atypical clinical presentations of HSV encephalitis have been described in patients receiving steroids, and also occur in 2% of patients infected with human immunodeficiency virus (HIV) with neurological symptoms, usually in association with CD4 T cell counts <200 cells/μL [6]. "
1421389,2.0," The literature exploring the association of MS with CLL is very limited. Neither the development of CLL prior to the development of MS, nor the reverse, have been shown to have an association in retrospective studies ([7]; additional references are available upon request from the corresponding author). Only one study has shown an increased risk of Hodgkin's and non-Hodgkin's lymphoma in first-degree relatives of patients with MS, and this is thought to be related to shared environmental and constitutional factors such as infection with Epstein-Barr virus or expression of the HLA-DR2 allele. "
1421389,3.0," The development of clinical symptoms and signs of MS in our patient so soon after her episode of HSV-1 encephalitis, as well as her earlier atypical peri-ventricular pattern of MRI findings, suggests a possible association between the infection and subsequent diagnosis of MS. On the one hand, it is possible that our patient's initial illness was an early manifestation of MS with asymptomatic HSV-1 colonization in the CNS, and that her initial improvement could be attributable to receiving dexamethasone for 1 day. Of note, detection of HSV DNA in CSF by PCR has a sensitivity of 98% and a specificity of >95% [8]. True positives have been reported in patients who are asymptomatic, and false positives have been reported due to cross-contamination. Additionally, the MRI from our patient's first admission demonstrated basal meningitis, which is an atypical feature of MS (see Figure 3). On the other hand, our patient's recent recurrence of herpes labialis, the positive CSF PCR for HSV-1, and her continued improvement over nine days while receiving intravenous aciclovir support the argument that HSV-1 infection was the reason for her initial illness. Regarding the nature of the illness, CSF PCR for HSV has been recently used by others to diagnose mild cases of HSV encephalitis, to diagnose atypical cases of adult HSV encephalitis involving polyradiculoneuritis and inflammatory syndrome with arachnoiditis, and to define a pathogenic role of HSV in episodes unlikely to be entirely CNS viral infections (relevant references are available upon request from the corresponding author). Furthermore, the fact that the patient's repeat CSF PCR result for HSV-1 was negative on her second admission helps support that her initial encephalitis was related to HSV-1 even if some demyelination due to MS had already begun. Although studies of the CSF such as oligoclonal banding were not performed or indicated during her first admission to explore the likelihood of MS, the patient denied any history of urinary incontinence, vision loss, unilateral blurred vision, double vision, gait disturbance, or paresthesias before her current illness. However, the expansion of previous areas of peri-ventriculitis on the patient's second admission do raise the possibility of her first admission involving concurrent HSV-1 encephalitis and MS. "
1421389,4.0," A recent study of patients with MS has demonstrated alterations in dendritic cell antigens and interferon expression in response to HSV-1 challenge to mononuclear cells [9]. Whether such an impaired immune response to viral infection is present before the development of MS is unknown. Additionally, patients with MS are known to have elevated antibody titers to HSV-1 in the CSF, but this is purported to be non-specific and studies of CSF PCR for HSV-1 in patients with MS have failed to find a statistically significant association [10]. There is evidence of association between infection with Epstein-Barr virus (EBV) and subsequent diagnosis of MS, as well as between CNS infection with human herpesvirus 6 variant A (HHV6A) and concurrent MS [10]; however causal relationships are yet unproven. Our patient did not undergo serological EBV testing or CSF studies for HHV6 during her admissions."
653044,0.0," In the present work we described a lumped parameter model of the cerebrospinal venous network based on a realistic anatomical paradigm, aimed at determining if an extracranial morphological change can cause an intracranial venous flow alteration. Compared to previously published models of the physiologic and impaired cerebrospinal venous circulation [19, 21, 23, 25, 26], our model was designed with a detailed correspondence between its elements and anatomy. The accuracy of the model was evaluated comparing the flow estimates with that obtained with PC-MRI from a group of healthy subjects. We applied the model to mimic obstruction of the main extracranial veins and to estimate the changes in intracranial pressure and flow. We compared the location of flow inversion simulated with IJV obstruction and what we observed with ECD in a group of 112 MS patients. "
653044,1.0," Comparison between model estimates and MRI measures The physiologic flow rates computed by the model were realistic in terms of direction (Fig. 2) and amplitudes (Fig. 3). These agreements were not obvious since we a priori set the arterial input flow directions; the network connections; the venous diameters and lengths and the zero-level pressure at two points of the hydraulic system. We did not impose the venous flow directions or pressures, with the exception of the pressures in subclavian vein and the right atrium. In regards to the flow amplitude, the mean flow rates estimated with MRI were not significantly different from those predicted by the model, with the exception of ISS and ICVl. The best correspondence between predicted and measured flow rates were found for those vessels whose anatomy can be approximated by an almost straight pipe of constant cross sectional area, i.e., where the basic assumption for Poiseuille's law holds, and whose dimension, shape and blood velocity allowed for the acquisition of good quality PC MRI data. The significant difference between ISS (p=0.004) and ICVl (p=0.01) flow rates and those predicted by the model could be due to the small dimensions and low flow velocity of all these vessels and to the curve shape of the ISS. The same reasons could explain the trend for difference found between ICVr and both the SPS (p=0.06). Regarding the SSS, its cross sectional dimensions increased anteriorly to posteriorly; as did its flow rate, consistently with the presence of the superior cerebral veins converging into it. We modeled the SSS as a single resistance and did not model the superior cerebral veins due to their high inter- subject variability in terms of number and size. For these matters the flow rate measured with the anterior, but not posterior, acquisition was significantly different from the model estimate. Furthermore, the SSS curve shape could explain the trend for difference between the model estimate and the posterior MRI measurement (p=0.055). The mean SSS flow rate measured with the posterior acquisition was similar to that reported by Gideon et al. [43]. (443±124 ml/min, p=0.1) and Mehta et al. [44] (388±169 ml/min, p=0.9) on a group of 14 and 15 healthy volunteers, respectively. The model estimate was similar to the cited works. The same reasons could explain why there was a trend for the TS (p=0.09 and p=0.1for the left and the right side, respectively) for slower flow rates measured with MRI compared to that predicted by the model. Another general source of mismatch between our model estimates and the MRI measurements involves the coupled vessels. We assumed a right-left symmetry of the venous network when designing the model, while in vivo asymmetry is described with a wide range of anatomic patterns [45–49]. The IJV, VV, and TS, seem to have right dominance regarding their size and flow [47, 49]. As a matter of principle, different anatomical connections could be additionally implemented with the same approach, even though this would require changing the equations associated to the network. The same model structure can instead be used for a variety of analyses, such as the study of asymmetric vessels, only by changing the dimensions of the vessels. "
653044,2.0," Venous obstruction simulation The simulations obtained using different combinations of IJV diameters showed that the intracranial pressure is stable for a wide range of IJVs diameters and increases for severe IJV stenosis. Our finding is similar to that showed by the recent model of Gadda [28], where a mild effect on the venous sinuses pressure was produced by halving the IJVs conductances, and a strong effect by setting the IJVs conductances close to zero. The previously cited model analyzed the venous sinuses pressure as a single node of the network. Conversely, our model includes the main intracranial vessels, so we could simulate the pressures at their ends for progressive bilateral IJV constrictions. Even with different amplitudes, the relationship between venous pressure and IJV diameter reduction had the same shape for the different intracranial veins. The high pressures estimated at the confluence of RVs and ICVs to GV due to severe IJVs diameter reduction is a theoretical biomechanical finding supporting the hypothesis and the experimental findings of previous works [20, 50–52]. It has been suggested that such a high pressure can damage the brain tissue, with possible blood–brain-barrier rupture [50, 51] or permeability increase [13, 53]. In particular, since the Galenic system drains the periventricular white matter and the basal ganglia, its increase of pressure can damage those brain areas [51, 53]. The intracranial vein hypertension, especially that of the SSS (Fig. 4b), can also impact the cerebrospinal fluid flow hemodynamics. Indeed, the cerebrospinal fluid flow absorption is possible if cerebrospinal fluid pressure exceeds that of the venous sinuses by about 5–7 mmHg [20]. For severe degrees of IJV stenosis, our model predicted a pressure increase in the SSS similar or even higher than the previously reported pressure gradient. A relationship between IJV stenosis, reflux and cerebrospinal fluid flow alterations was recently observed in different CNS disorders, like MS and AD [13, 14, 20, 54, 55]. The cerebrospinal fluid flow alterations were also recently reported to correlate with “dirty appearing white matter” in healthy subjects [56]. The brain edema seen in cases of venous sinus thrombosis or stenosis could also be due to the decrease in cerebrospinal fluid drainage caused by an increase in venous pressure [57]. In a recent review [20], Beggs explained the link between IJV stenosis, its hypertension and the impairment of cerebrospinal fluid flow absorption into the venous blood. Other consequences of severe neck vein stenoses predicted by our model are the reduced intracranial venous flow rate, and even flow inversion. Schelling in 1986 [52] suggested that refluent IJVs or intracranial veins could produce retrograde venous hypertension, i.e., from downstream to upstream the venous network. The resulting venous overload, engorgement, dilatation and leakage were proposed to be possible causes of MS lesions. The location and shape of such lesions were also suggested to be similar to those found through Schlesinger’s experiments with gelatin solution forced in the GV of human cadavers. With the modelization used in this work, blood flow in any vein is determined by the pressure drop between its extremities, being directed toward the end with the lowest pressure. If the pressure difference between the vein ends reverts, the flow becomes retrograde. This general mechanism explains the cases of reflux found through our simulations, as explained for the intracranial, vertebral and azygos areas hereafter. The upstream of one IJV is more influenced by its diameter change than intracranial vessels: its pressure begins to increase for small IJV diameters reduction. When both the IJVs are stenotic, our model predicted retrograde flow for the intracranial vein proximal to the IJVs themselves, in particular for the CS, IPS and SPS. Figure 5 represents how the flow inversion occurs in the IPSl. The pressures of its two ends were plotted over progressive reductions in IJV diameter (Fig. 5a): since they are superimposed on each other, it is clearly visible when one exceeds the other, i.e., for severe bilateral IJVs stenosis. Together with this pressure drop inversion, the IPSl flow inverted its direction (Fig. 5b). The preferred drainage path of CS, IPS and SPS in this case was the anterior cervical plexus (Fig. 7) instead of the IJVs (Fig. 2), since it offers lower resistance to flow. The presence of collateral vessels, as modeled in this work, did not prevent intracranial pressure increase and reflux. Indeed, one or more collateral vessels in parallel to a stenotic IJV have higher resistance to flow as compared to a patent IJV, even if the total cross sectional area is the same [19]. If a clearer picture was needed, the model could be tuned for a single patient-specific case by setting ad hoc geometry, number, position and connections of collaterals. By doing this, it could be tested how a wider or higher number of collateral vessels would guarantee enough drainage and lower intracranial pressures, avoiding intracranial vessels reflux. Future work involving this kind of simulation could provide a better understanding about how collateral flow impacts intracranial pressure. Indeed, collateral number, dimensions and their anastomosis to the IJV are variable and are fundamental as an alternative pathway in clinical cases of IJV obstruction [27, 28]. The VV morphological obstructions had a low impact on the intracranial region, but a high impact on the pressure of the cervical and vertebral areas, besides on the areas connecting them to the intracranial vessels. When the pressure increase was higher in the upper portion of the VVs than in the intracranial region, a retrograde flow could occur in the posterior occipital sinuses. The blood of the last vessels was redirected to the IJVs instead of flowing to the CPp and VVs. For a patent azygos, the pressure of its distal tracts is higher than that of the proximal ones: this is visible at Fig. 6a, for diameter reduction = 0 %. The flow is directed from its distal to its proximal tracts, to the superior vena cava, as shown in Fig. 2. The simulation of the proximal azygos obstruction produced asevere pressure increase (Fig. 6a, pressure between AZY4 and AZY5 and between AZY5 and AZY6). Conversely, the pressures of the distal districts were almost unaffected by the change (Fig. 6a, pressure between AZY11 and AZY12). When the obstruction caused an excess of pressure in the proximal compared to the distal part of an azygos portion, the pressure gradient changed its direction. The blood flow reverted its physiologic direction according to the pressure gradient. This can be observed in Fig. 6b, which shows how the azygos flow rate changes when proximal azygos obstruction is simulated. The AZY5 flow becomes retrograde when its distal pressure (between AZY5 and AZY6) exceeds the proximal one (between AZY4 and AZY5). For this particular case, this happens when the proximal azygos diameter is halved. The same can be observed for AZY11 flow, with less diameter reduction. A similar mechanism affected the thoracic plexus. The thoracic and azygos refluxes were drained to the right atrium through the inferior vena cava instead of the superior vena cava. "
653044,3.0," Jugular vein obstructions: comparison of model simulations and real cases Comparing our ECD data with the model estimates in the most prevalent clinical situation (the IJV obstruction), we found agreements in intracranial reflux topography. Comparing our ECD data with the model estimates in the most prevalent clinical situation (the IJV obstruction), we found agreements in intracranial reflux topography. A quantitative comparison could not be done since we did not measure the IJV diameter during ECD acquisitions. However, the results of the model simulations have to be read as a proof of concept: the percentages of IJVs diameter reductions corresponding to intracranial reflux, or to intracranial pressure increase of at least 200 %, are not intended to be considered absolute thresholds. Indeed, they depend on the chosen “normal” IJVs diameter and other vessel dimensions. Regarding venous dimensions, a high inter-subject variability is described in literature, with different physiological vessels dimensions and flow rates among the studies, and a high deviation from the mean within each study [30, 32, 44, 49, 58]. Furthermore, the venous reflux detected in clinical cases often involves only a portion of the vessel and can be intermittent [42]. This fact can be explained by observing the occurrence of separated flow regions in a single vessel, as studied with a Lagrangian approach to fluid flow analysis [59, 60]. Conversely, with the present kind of modelization, a vessel can be classified as refluent if its flow rate is completely reverted. Local flow separation cannot be described through our approach. However, considering the relatively low values of Reynolds' number (maximum = 715 in the SSS), we may reasonably infer that such peculiar flow structures wouldn't significantly affect the vessel resistance calculations in our model. "
653044,4.0," Limits and perspectives The results of our model and all the simulations provided by it have to be evaluated while taking into consideration the main limitations of our type of modelization: the model is steady-state and it is valid only for analyzing the supine position. Regarding the subject position, a future work for studying a standing subject should consider the effect of atmospheric pressure on IJVs, which collapse, and the role of collateral pathways [28]. As to the steady-state limitation, this means that every pressure and flow of the network is not time-variant, so the modulation of the venous velocity due to the cardiac or the thoracic pump are neglected. A recent work [61] showed that the latter highly influences the venous return of the IJVs, especially for in the deep inspiration, because of the aspiration effect of the pleural cavity. However, the mean flow of the intracranial veins is less modulated by the respiratory phases. The minor effect of respiration and cardiac pulsations on intracranial vein flow was also shown through MRI with the analysis of the frequency components in the range of respiration and heart beat. The flattened flow rate waveform of cerebral sinus veins can be due to their fixed diameter and protective effect of the dura mater [31]. Moreover, the IJV cross sectional area changes over the cardiac cycle [62]. However, due to the high inter-subject variability in terms of IJV cross sectional areas and ranges of variation, the choice of considering an average constant value can be considered a reasonable approximation. Another aspect to point out is that the refluxes observed in clinical cases could be due to the sum of the effects of many morphological alterations (i.e., other abnormalities besides IJV or VV or AZY obstruction), while we separately simulated the effects of the main routes obstruction, obtaining intracranial refluxes only for severe IJV stenoses. Nonetheless, it is possible to use this model to test the effect of many other morphological alterations. Owing to the linearity of the model, the effects of any combination of anatomical changes may be obtained through the superposition principle, i.e., by linearly combining the effects of single anatomical alterations. It is hence straightforward to adapt the model to patient-specific features."
654927,0.0," This is the first multimodal imaging study in which 2 noninvasive and 2 invasive diagnostic techniques for the detection of extracranial venous anomalies, indicative of CCSVI were applied. The main finding of the study is that invasive techniques confirmed that noninvasive DS screening was a reliable approach for identifying patients eligible for further multimodal invasive imaging testing of the IJVs. In 19 of the 20 MS patients, the extracranial venous IJV anomalies indicative of CCSVI diagnosis were confirmed on CV or IVUS. However, it has to be noted that 50% of the screened MS population did not fulfill ≥2 extracranial VH DS criteria and were therefore not eligible to undergo invasive testing, which limited the study ability to investigate specificity of DS vs. invasive imaging diagnostic techniques. Nevertheless, the findings from this multimodal study are important, as they suggests that DS can be used reliably to select those patients who may present extracranial IJV venous anomalies, indicative of CCSVI, while in the same time, it can potentially exclude those patients who should not undergo an invasive testing of the IJVs. However, the noninvasive screening methods were inadequate to depict the total amount of VV anomalies that would indirectly reflect the pathology of the azygos vein identified with invasive testing. These findings are related to the fact that we were not able to directly non-invasively assess the azygos vein. In our opinion and experience [6, 8, 22, 23], there are no reliable, non-invasive imaging modalities at this time that would directly image the azygous vein in vivo. Another important finding is related to the results from the invasive portion of the study, which confirmed the existence of severe extracranial venous anomalies, indicative of CCSVI that significantly impaired blood outflow from the brain. "
654927,1.0," A growing body of evidence suggests that the majority of CCSVI pathology is confined to the intra-luminal portion of extracranial veins, which requires high-resolution DS or IVUS B-mode imaging for the visualization of these anomalies [8, 26, 27]. It has been shown that the presence and number of these anomalies may contribute to a higher number of collateral neck veins and functional abnormalities [8, 27]. While CV is considered to be ""the gold standard - benchmark"" for detecting stenosis in blood vessels associated with altered blood flow, the PREMiSe study showed that CV may not be sensitive enough to reveal the exact nature of narrowed vein segments [25]. CV is a luminogram and brings little or no data regarding the vessel's intra-luminal structures because of dense opacification of the lumen with contrast, which obliterates subtle intra-luminal structures [19]. There are no consensus guidelines with respect to the use of angiographic contrast for extrcranial CV examination [28]. The recent position statement of the International Society for Neurovascular Disease on the use of angiographic contrast for the assessment of IJVs and the azygos vein on CV does not provide clear guidelines on this issue [29]. Angiographic contrast may be used diluted (1:1) or non-diluted. While the diluted contrast may allow a better visualization of endoluminal structures (valve leaflets, webs, etc.) non-diluted contrast allows a better opacification of epidural and other collaterals, as well as a better estimation of overall features of the veins [28]. In the PREMiSe study, non-diluted contrast was used. It could be that use of a diluted contrast could have produced different findings. The PREMiSe study also demonstrated the advantage of IVUS compared to CV in detecting intra-luminal abnormalities as well as the importance of including IVUS during CV examination, especially for the assessment of the azygos vein [25]. It is important to note that sensitivity of IVUS to depict extracranial venous anomalies on DS, indicative of CCSVI, was in better agreement than the CV findings, especially for the azygos vein/VVs territory. However, one of the important limitations of DS screening approach is that the azygos vein cannot be directly imaged. While the sensitivity for detecting VV anomalies on IVUS vs. DS was high, DS did not detect abnormal VV flow in 10 patients who had positive IVUS in the azygos vein. Similar limitations were observed for MRV. These results suggest that currently, available noninvasive indirect screening methods are inadequate in depicting the total amount of intra-luminal pathology of the azygos vein. The sensitivity of CV + IVUS to define total IJV pathology on DS was 100%. These findings support results from the 2 previous studies in which a higher sensitivity of DS to detect extracranial anomalies on IVUS compared to CV was found in the IJVs [27, 30]. These results can also explain findings from some recent reports that found low correspondence between the DS screening assessment and the CV findings [9, 31]. "
654927,2.0," Zamboni et al. proposed a set of 5 VH DS criteria by which MS patients were differentiated from healthy controls with 100% specificity and sensitivity. While the original publication did not provide the exact technical procedures for the protocol application in either a research or routine clinical setting, there were recent attempts to define the standardized CCSVI DS scanning protocol [5, 32, 33]. These revised DS protocols propose the use of quantitative measures for the definition of functional anomalies such as blood flow velocity and volume that could be potentially more reliable in assessing the degree of venous outflow obstruction in the extracranial venous system [32, 33]. They also refine originally proposed VH criteria [5, 32, 34] and propose the use of the central blinded DS reading [33]. In this multimodal comparison of different noninvasive and invasive imaging techniques in phase 2 of the PREMiSe study, we read findings from multimodal techniques in a blinded manner by different experts and using a panel to reach a consensus when there were discrepancies by the readers. They also confirmed the correctness of the exam reading by a-priori comparing un-blinded reading results of the individual imaging modalities on a subject level. We did not consider the assessment of the second CCSVI VH criterion (reflux in deep cerebral veins) for several reasons: 1) the reproducibility of this criterion is lower compared to the other 4 VH criteria; [6, 7] 2) there is no direct anatomical extracranial correlate for performing sensitivity and specificity comparisons with other multimodal imaging techniques; 3) use of this criterion contributes to the highest variability in making a CCSVI diagnosis and 4) the direction of the blood flow in veins connecting cortical with deep veins may vary considerably as a consequence of the physiologic inter-individual variation of the cerebral venous anatomy [11]. Despite this, the results of the multimodal PREMiSe study further support the value of DS VH criteria for the screening of extracranial venous anomalies in territories of left and right IJVs. "
654927,3.0," When CV + IVUS findings were compared to MRV findings, sensitivity was high but the specificity was low, confirming our previous results [23, 24]. Some other investigators used a slightly different grading system for the detection of extracranial venous anomalies on MRV and found similar sensitivity but better specificity compared to CV [35]. Therefore, the use of different MRV evaluation criteria may have yielded different sensitivity and specificity results compared to CV, IVUS and DS in the PREMiSe study. While there is still a lack of standardized guidelines for the detection of extracranial venous anomalies indicative of CCSVI on MRV, the findings from the PREMiSe study indicate that MRV should be incorporated in the armentorium of noninvasive screening techniques. Further work is needed to standardize MRV morphology criteria [6, 22–24, 35–37] and incorporate flow and velocity information in determining subjects at risk for the detection of extracranial venous outflow anomalies with hemodynamic consequences [38–40]. "
654927,4.0," The combination of DS + MRV did not yield better reliability vs. invasive imaging techniques compared to the DS alone. However, phase 2 of the PREMiSe study included only MS patients with ≥2 VH extracranial criteria, which limited our ability to explore the additive value of MRV to DS in improving sensitivity and specificity vs. other invasive imaging techniques. "
654927,5.0," It was proposed that extracranial venous collateral circulation is a compensatory mechanism for impaired venous outflow because it bypasses blocked veins and thereby reduces resistance to drainage [6, 8, 41]. The PREMiSe study showed an excellent correspondence between identifying collateral veins on MRV and CV. Approximately, 70%-85% of patients presented collateral veins on the right and 75%-80% on the left side of IJV on MRV and CV respectively. In addition, 80% of patients presented with collaterals of the azygos vein on CV. These findings confirm that the presence of collaterals on MRV and CV may represent an indirect compensatory mechanism for impaired venous outflow. In the previous study, we found high specificity for distinguishing MS vs. healthy controls based on >1 of collateral veins in the neck [6]. "
654927,6.0," PREMiSe was an endovascular angioplasty study that did not include healthy controls or MS patients without the presence of CCSVI diagnosis on DS. This selection bias of the included population was an important limitation of this diagnostic study, as the sensitivity and specificity findings of noninvasive vs. invasive techniques cannot be generalized to the prevalence of findings to other case–control studies. However, the main aim of this multimodal study was to define and reliably detect extracranial venous anomalies, indicative of CCSVI in the IJVs and azygos vein/VVs of patients using DS and to confirm the presence of these anomalies by using 2 invasive imaging techniques. Another potential limitation of the study is a relatively small sample size, which could skew our findings. Although PREMiSe was a limited pilot trial not powered to detect the prevalence of CCSVI in the general MS population and healthy individuals, it confirmed a general prevalence of extracranial venous anomalies, indicative of CCSVI that we have reported in large cohorts using DS and MRV [5, 6, 8, 22]. "
654927,7.0," Maybe the most important result of the PREMiSe study is that our multimodal imaging findings contradict the number of recent DS studies that reported a prevalence of CCSVI <10% in MS patients [9, 11, 12, 16, 17, 33, 42–44]. In fact, the invasive diagnostic portion of PREMiSe confirmed that 19 of the 20 MS patients screened as CCSVI positive by DS had severe impairment of extracranial venous outflow with significant stenosis in the IJVs and azygos veins. Future, larger, case-controlled, multicenter, multimodal, noninvasive and invasive imaging studies that will include healthy controls, MS patients and patients with other neurological diseases should determine the real prevalence of CCSVI in these cohorts."
432103,0.0," Amoebae are good candidates to be a reservoir of the elusive M. ulcerans, but this relationship has not yet been thoroughly investigated. Here, we study the potential for amoebae to host M. ulcerans both experimentally as by sampling an aquatic environment. Our results show that M. ulcerans can indeed be phagocytosed in vitro by A. polyphaga and that viable bacilli persist for at least 2 weeks. We observed both tight and spacious phagosomal vacuoles containing M. ulcerans in infected A. polyphaga with transmission electron microscopy, as has been described for M. avium-infected A. [29,47] and for macrophages infected with mycobacteria [50], including M. ulcerans [51]. The observed reduction in the number of viable bacilli is probably due to bacilli that are expelled by the amoebae after a phase of intracellular multiplication, as has been reported for in vitro mycobacteria-infected macrophages [52,53]. The kanamycin in the medium therefore probably killed released bacteria and resulted in an underestimation of the capacity of M. ulcerans to grow inside the protozoan cells. Compared to a noninfected A. polyphaga monolayer, infection with the M. ulcerans strains did not result in a higher loss of cells (data not shown) indicating that the infection did not affect A. polyphaga viability. "
432103,1.0," By analysing samples from an aquatic environment in BU endemic and nearby non-endemic communities in southern Ghana, we found several mycobacterium species intracellularly in eukaryotic micro-organisms. Most of the mycobacterium species we identified are potentially pathogenic to humans [54–57]. We did not isolate M. ulcerans, even not by successively passaging IS2404 positive specimens and amoeba cultures in mouse footpads, the method that has led to the only successful isolation of M. ulcerans from the environment [15] (data not shown). We isolated mycobacteria as frequently from an intracellular source as free-living, suggesting that it is quite common for several species of mycobacteria to infect micro-organisms in natural circumstances. The intracellular lifestyle was found significantly more frequent in detritus samples compared to water and biofilm samples. This could be due to the low oxygen levels in this organic debris. For several environmental bacteria (including M. avium) it has been shown that oxygen depletion (and other conditions that typically dominate in animal intestines) triggers the invasion of and enhances the survival within host cells [58,59]. "
432103,2.0," We detected the marker IS2404 in 1 water and 2 biofilm samples collected in a BU endemic and a nearby non-endemic community in southern Ghana. In addition, we detected the same marker in 6 amoeba cultures obtained from other samples. This is the first report of the detection of the marker IS2404, suggestive of M. ulcerans presence, in amoeba cultures isolated from the environment. It is noticeable that we tripled our detection frequency of IS2404 by searching in the amoeba cultures in addition to the original samples. We could not observe AFB in the smears of these amoeba cultures, but one must take into account that M. ulcerans and other IS2404 containing mycobacteria grow very slowly and thus were probably present in very low quantities on the amoeba cultures. On LJ-medium, M. ulcerans colonies only appear after an average of 10 weeks in primary culture from clinical specimens [60]. From environmental sources, M. ulcerans was only isolated once despite numerous attempts [15]. "
432103,3.0," Other mycobacteria were also quite frequently detected in amoeba cultures (in 17.5%), by a PCR assay targeting their 16S- rRNA gene. For these, mycobacterial presence could be confirmed by microscopy in almost half of the positive cultures. However, the AFB were not observed inside or attached to the amoebae. The fact that in our study mycobacteria could still be detected after multiple subcultures of the amoeba cultures, suggests that the mycobacteria were multiplying extracellularly on the agar plates. IS2404 was in fact also detected on one agar plate on which the amoebae did not survive subculturing. Similarly, in a co-culture study of M. avium and A. polyphaga, M. avium was shown to persist and multiply both intracellularly and extracellularly as a sapro- phyte on the excrement of A. polyphaga, and mycobacterial growth was most extensive extracellularly [47]. "
432103,4.0," The successful uptake and persistence of M. ulcerans inside A. polyphaga in vitro and the higher detection frequency of IS2404 in amoeba cultures as opposed to the crude samples from the environment suggest that amoebae may act as a host for M. ulcerans in natural circumstances. However, our data do not reveal a significant role for protozoa in the distribution patterns of BU disease in humans, so we remain sceptical about their involvement in the direct transmission of M. ulcerans to humans. If a protozoan were to be principally responsible for the observed distribution pattern of BU in humans, one would expect either a particular species with a limited distribution to harbour M. ulcerans, or otherwise several species that only do so in areas where BU actually occurs in humans. In this study, however, we detected IS2404 as frequently in amoeba cultures isolated from BU endemic as from non-BU endemic communities. Moreover, 5 different protozoan species from two divergent families were identified in the IS2404 positive amoeba cultures, some of which are known to be cosmopolitan. On the other hand, we cannot completely rule out that some or all of the IS2404 we detected originated from different mycobacterial species than M. ulcerans. "
432103,5.0," More environmental research is needed in Africa if we want to understand the distribution of BU, and to prevent its transmission from the environment to humans. Environmental research of M. ulcerans has been severely hampered by the difficulties of detecting the pathogen in the environment. Our results indicate that perhaps amoeba cultures can serve for improved detection of M. ulcerans in environmental samples. Co-cultivation with an existing amoeba culture is a technique to selectively isolate amoebae- resistant bacteria that are difficult to grow from the environment [61] and has already been proven successful in the identification of new pathogens and their distribution patterns [62]."
2442696,0.0," The idea of venous congestion as a possible contributor to pathogenesis of MS has been discussed for the past 40 years, but remained widely unappreciated by the scientific community [3]–[4], [6,26]. Until recently, no scientific attempt was undertaken to prove or disprove the hypothesis despite its potential therapeutic implications. Therefore, the exceptionally significant results presented by Zamboni et al generated extraordinary interest and controversy in both scientific and social communities [27]. According to the Author, CCSVI may play a significant role in MS etiology by means of a venous congestion and an impairment of iron accumulation [28]. Several repetition studies did not reach the same strict evidences and the reliability of the CCSVI-MS association has already been debated [5]–[7]. Moreover, proposed parameters from Zamboni et al has been considered questionable by some Authors [5,29]. “A swan song for CCSVI” has been sung by two recent large and unbiased studies that suggests no association of CCSVI to MS [7]–[9]. Nevertheless, some haemodynamic alterations of venous system in MS patients may represent a functional epiphenomenon of intracranial microvascular impairment. Despite a slight difference in global CBF e CFV between MS patients and HCs, basal arterial and venous haemodynamic features did not differ among groups. The results of this study suggested such an impairment of postural control of both arterial inflow and venous outflow in MS patients without affecting CCP and CVR to apnea. As suggested by Gonul et al, arterial response to postural variation is a reliable method to assess cerebral autoregulation and may depend on autonomic and neurocardiogenic control systems of cerebral perfusion [18]. Several studies observed an orthostatic intolerance in MS patients but the pathophysiological mechanisms has not been elucidated and the relationship with disease severity and progression is still an object of debate [30]–[34]. Since significant changes of non invasively measured MAP and HR in sitting position has not been observed in our series, an impairment of cerebral microvasculature may be suggested but the preserved endothelial-dependent cerebral autoregulation did not support this hypothesis. Moreover, CPP remains stable after 90 s of sitting position and no differences has been found between MS patients and HCs. In addition, postural adaptation of MFVs was not different among sex, age or disease duration groups while a significant association has been observed with clinical disability. More sympathetic rather than parasympathetic dysfunction appears early in the course of MS and correlates with disease progression and MS clinical disability [35], [36]. A noradrenergic dysfunction may determine an altered vasoconstriction that would explain the reduced postural control of arterial inflow independently from changes in arterial blood pressure (ABP) [37]. The same sympathetic impairment could increase venomotor tone with resulting abnormal compliance of venous vessel wall [30]. Although we did not observed significant differences in global CVF between patients and controls, an altered postural control of venous outflow was more prevalent in MS patients, particularly in PP phenotype and in more disabled patients. According to Monti et al, such an explanation may be a no efficient Spinal Epidural Veins outflow as an additional drainage pathway in the seated position [38]. However, the cerebral venous system has more anatomical variability than the intracranial system and is often asymmetrical. The IJVs are the prevalent extracranial outflow pathways in the supine position while redirection of venous blood flow towards the vertebral veins occurs in the upright position [17,22]. In this study, a significant prevalence of right IJV in supine position and no alteration in available intracranial veins have been observed in line with previous findings [22,39,40]. Nevertheless, we did not find any association among anatomical variability, positive ΔCSA and loss of postural venous drainage control. Therefore, any pathology (stenosis) affecting cranio-cervical or azygous venous system such as a IJV's valve incompetence seems not sustainable. Some methodological issues need to be stressed at this point. First, the patients reached head upright in sitting position on a tilting chair and not on a tilt bed assuming a constant MCA diameter. Second, the angle of insonation may change during head up tilt and could adversely affect the results. Furthermore, this is a single cross-sectional centre study with a single trained sonographer. Although patients with recent relapse or treated with corticosteroids have been excluded, the relative impact of disease progression and disease-modifying treatments are not exhaustively elucidated. As a matter of fact, despite several attempts of standardization, ultrasonographic techniques are operator-dependent and this is particularly true for venous assessment because of a wide anatomical and physiological variability of venous system and a high bias due to manual pressure. In addition, the interpretation of extracranial and intracranial venous studies is hampered by the fact that the interobserver and intra-observer reliability of the method is still variable [41]. On the contrary, the strength of this study is a meticulous blinded design and a relatively good sample size with a high percentage of PP-MS patients and a large amount of measured haemodynamic parameters. Considering RR-MS and PP-MS as two clinical opposites of the same disease, we excluded SP-MS patients in order to avoid possible confounding factors. "
2442696,1.0," In conclusion, our study suggested that the quantitative evaluation of CBF and CVF and their postural dependency may be related to a dysfunction of autonomic nervous system that seems to characterize more disabled MS patients. It's not clear whether the altered postural control of arterial inflow and venous outflow is a specific MS condition or simply represents an “epiphenomenon” of neurodegenerative events. Further studies correlating ultrasonographic hemodynamic data with MR imaging lesion load and with perfusional parameters are needed in order to confirm our hypothesis."
1150,0.0," We explored the correlation between the distribution of Belostomatidae and Naucoridae and the prevalence of Buruli ulcer. We have found a positive gradient between habitat suitability of Naucoridae and Belostomatidae and Buruli ulcer prevalence. Correlation does not imply caus- ation; this result is not proof that the insects are vectors. However, understanding the reasons for the temporal and spatial changes in this correlation will enable a better understanding of the reasons for changes in Buruli ulcer prevalence. "
1150,1.0," There are significant temporal changes in this correlation between habitat suitability of the insects and Buruli ulcer prevalence; in Akonolinga the Buruli ulcer prevalence is correlated to Naucoridae and Belostomatidae distribution in the wet season, but not in the dry season. Buruli ulcer is known to have complex temporal changes in prevalence [30,33,34], as is M. ulcerans [14,35]. It is therefore unsur- prising that, if these insects are implicated in maintaining M. ulcerans in the environment, or involved in transmis- sion to humans in some way (either as host vectors, or host carriers), the correlation between Buruli ulcer preva- lence and their abundance would change in time. "
1150,2.0," We also observe spatial changes in the correlation; water bug habitat suitability is not correlated to Buruli ulcer prevalence in Bankim, 457 km North of Akonolinga. Speculatively, perhaps other routes of transmission may be more important in this region, for example contact with infected plant biofilms, as suggested in Ghana [36]. "
1150,3.0," How do we interpret this result in terms of the Bradford-Hill guidelines? Herein we have focused on the correlation between these insects and the prevalence of the disease in both space and time. While there is a sig- nificant positive correlation for the predicted abundance of the aquatic insects and the prevalence of the Buruli ulcer, this correlation is not consistent from region to re- gion. Previous research has proposed that M. ulcerans is transmitted within a multi-host transmission network [4]. In such a situation of multiple hosts the relative im- portance of any given mode of transmission may be ex- pected to vary in time or space, and our results are consistent with, though not proof of, this hypothesis. The lack of a clear signal between water bugs and Buruli ulcer in Bankim would suggest that may not be key vectors or host carriers in that region. Recent studies have found notable changes in community composition relevant to M. ulcerans distribution, in the Greater Accra, Ashanti and Volta regions of Ghana, for both plant [35] and aquatic insects [37]. This would support the importance of changing biotic communities as a key factor in changing Buruli ulcer prevalence, a priority for future work. We have found that the prevalence of Buruli ulcer is correlated to Belostomatidae and Naucoridae abundance in Akonolinga but not Bankim, we do not know if the plant community composition for these re- gions, and wider aquatic insect community, also correlates to disease prevalence changes on a similar scale. "
1150,4.0," The ecological niches of both Naucorid and Belostomatid water bugs in West Africa are predominantly determined by the distribution of suitable landcover in a 5 km radius, preferring water bodies, artificial areas and rain fed crop- lands. The specific land cover at the point of the site (GLC-AP) was less informative. The observation that the most suitable regional land cover class is water bodies is not surprising, but the high suitability of urban areas is curious. Ecologically this could have a variety of causes; there may be changes in the chemical composition of water in these habitats, a reduction in predation pressure, or a greater abundance of food. The specific reasons will require further research. "
1150,5.0," Our study has been limited in certain points; the low taxonomic resolution of the insects is a current limita- tion in this study. Secondly, an important limitation is that the distribution of M. ulcerans in these insects in these areas is unknown. The distribution of Naucoridae and Belostomatidae infected by M. ulcerans may differ from the distribution of Naucoridae and Belostomatidae generally. However, the insects are known to host the bacillus on their carapace, in their body [10,14,16]) and in their salivary glands [14] in the wild, and the distribu- tion of the insect necessarily sets a limit to the distri- bution of infected insects. A related limitation is the unknown incubation time of M. ulcerans; the time from infection to presentation at the hospital, remains un- known. Finally, we have only addressed a single criterion of the Bradford-Hill guidelines; correlation. We have not aimed for a full discussion of the other criteria, and our findings should not be interpreted as proof of the role of these insects as vectors or key host carriers. Rather, we have discussed the existence of, and change in, a correl- ation between these insects and Buruli ulcer. Future work aims to explore spatial variation in the correlation between Buruli ulcer and the entire plant and animal communities, identifying any similarities between re- gions where the correlation exists, expanding on previ- ous studies [37] which have focused on the community assemblage differences between M. ulcerans endemic and non-endemic regions of Ghana. "
1150,6.0," Despite these limitations, these results are consistent with previous research, which has shown that in Akonolinga the Nyong river is a risk factor for Buruli ulcer [30]. Our results agree with this conclusion; the main focus of suit- able habitats for the insects in Akonolinga is the Nyong river, where the existence of large plants near the river banks provides appropriate habitat for Naucoridae and Belostomatidae to forage, develop and reproduce. Previous research has also implicated aquatic insects as important including detection of M. vectors in Akonolinga [14], ulcerans in the saliva of the insects. "
1150,7.0," In conclusion, we find a positive correlation between the abundance of Naucoridae and Belostomatidae suitable habitat and Buruli ulcer prevalence. This correlation is not constant, and changes in time and space. We interpret this as evidence consistent with that idea that Naucoridae and Belostomatidae may be locally important host carriers of M. ulcerans in certain conditions, their importance chan- ging as the environmental conditions change, which would be expected in the situation of multi-host transmission."
1287608,0.0," This study tested the CCSVI hypothesis with three complementary imaging techniques (US, 4D flow MRI, and CE-MRV) in a large cohort of MS patients and age- and sex-matched controls, including HCs and subjects with other neurological disease. Our study was designed to address limitations of small sample sizes and single modality assessment found in other CCSVI investigations.4–26 To our knowledge, this is the only study that provides a comprehensive assessment of blood flow and luminography using 4D flow MRI in three cerebrospinal venous regions (totaling 14 measurement locations of blood flow), CE-MRV for semiquantitative assessment of vessels in both the IJV and AV, and strictly blinded US exams performed according to Zamboni’s CCSVI criteria.5 "
1287608,1.0," Without question, the CCSVI hypothesis has led to renewed interest in both intra- and extracranial venous flow measurements, namely in the DCVs, the IJVs, and the AV. From an imaging perspective, venous flow assessment for CCSVI is problematic. First, unlike rigid and muscular arteries, veins have thin, nonmuscular walls that make for an easily compressible and distensible lumen. Hence, veins are highly variable in appearance on cross-sectional imaging, which substantially compromises the test–retest and inter-rater reliability of venous caliber measurements on CE-MRV exams.33 Such variability is especially evident in IJVs which can take a number of different cross-sectional configurations (pinpoint, crescentic, flattened, oblong, etc.). Additionally, unlike stenotic arterial flow jets, a collapsing vein may produce slow flow or even flow reversal. In B-mode US images of IJVs, which are inherently 2D projections, this may lead to over- or under-estimation of lumen size, depending on the vessel shape and transducer direction. Underestimation may also occur if the ultrasonographer uses too much transducer pressure, collapsing the vein. Second, venous flow velocity (within a volume) is sensitive to a number of variables including body position,34 head position or rotation,35 respiratory state,36,37 hydration level,38 diurnal changes, and caffeine intake. Finally, the DCVs of interest in CCSVI—which include the internal cerebral and basal veins—are small and located in areas that are often difficult to assess. In flow MRI, this translates to a need for higher resolution scans and careful correction of phase offsets and displacement artifacts that lead to flow measurement errors. The problem is even more vexing in transcranial Doppler US due to the small or nonexistent temporal acoustic window (roughly 10% of individuals have no acoustic window),39 the variability of operator skill, and the inherent methodological problem adequately blinding technicians to subject status. Many of these criticisms relative to Zamboni’s original diagnosis of CCSVI using US (and the subsequent endovascular treatment) have been examined in more detail previously.40 "
1287608,2.0," In general, MRI has several benefits compared with US: MRI is the standard-of-care modality for diagnosis of MS, it is less operator dependent and is conducted according to prospective protocols that can be faithfully repeated by fully blinded operators, and it can be used to noninvasively assess vessels inaccessible to US. Drawbacks of MRI include image acquisition is not real time, ECG-gated acquisitions last several cardiac cycles, and measurements can only be made in the supine position, precluding the assessment of positional flow changes.3 "
1287608,3.0," PC-VIPR29,30 is a particularly powerful technique for flow assessment that utilizes a 3D, time-resolved radial acquisition k-space trajectory. This technique provides several advantages for assessing CCSVI beyond other MRI methods that measure blood flow. First, PC-VIPR is a 3D technique that covers an extensive anatomical volume with high isotropic spatial resolution, allowing entire vascular territories to be rendered during a single scan. Second, compared to a standard 3D Cartesian sampling scheme, the radial acquisition, in which central k-space is oversampled and outer portions of k-space are undersampled, allows for faster acquisition and high temporal resolution. This in turn creates high image contrast and sparse signal representation with background suppression. Finally, the artifacts associated with PC-VIPR do not substantially compromise the images. "
1287608,4.0," In summary, each imaging technique used in this study has its own strengths and weaknesses, and therefore provides complementary information on cerebrospinal venous vasculature. B-mode and Doppler US provides high temporal resolution and allows for strict adherence to Zamboni’s CCSVI criteria. The inherent 2D nature of US, however, may cause over- or underestimation of cross-sectional area. Further, it is impossible to interrogate the entire cerebrospinal venous system. 3D MRI techniques are more easily blinded and do not have these limitations. CE-MRV provides excellent high-resolution visualization of venous structures. This method acquires results at a single time point and thus may fail to capture potential respiratory or cardiac-induced change. Further, there are currently no established standards for normal venous lumen caliber.15,24 Without such standards, inter-rater agreement measures are unlikely to be high. PC-VIPR provides dynamic measurement of 3D velocities in a large volume with high spatial resolution; however, unexpectedly low velocities may result in poor signal-to-noise ratio and potentially compromise flow measurements. "
1287608,5.0," With 4D flow MRI (PC-VIPR), there were no statistically significant differences observed between groups (or age- and sex-matched pairs) for any flow measurement, at any anatomical location. Concordant with other IJV flow studies,6,25,33 larger total flow values are seen for the right IJV compared with the left IJV across all groups. However, large standard deviations are also observed, which possibly stemmed from the grouping of varying age and gender for group comparisons. This grouping encompassed a wide range of participant ages (MS: 19–68 years, HCs: 23–74 years, other neurological disease: 22–68 years) and a higher number of women than men (MS: 49F/27M; HCs: 31F/22; other neurological disease: 29F/14M). At baseline, IJV flow measurements are extremely variable. Individual cerebrospinal venous flow measurements are known to be subject to several normal day-to-day variations, such as related to body position,34 head position,35 and hydration levels.38 Previous work has shown these day-to-day flow changes to be as high as 20%, even while demonstrating good technical reproducibility in cerebrospinal venous vasculature measurements using PC-VIPR.33 A previous study using US and CE-MRV in MS patients suggests CE-MRV provides a more robust assessment of IJV narrowing than B-mode US, which can be limited by operator skill and misleading 2D projections.41 Scoring of the CE-MRV analysis indicated good image quality for both the IJV and AV, with particularly high values for the IJV. Although CE-MRV in the chest was performed during a single breath hold, the images are not cardiac resolved. This cardiac motion likely caused the lower image quality seen for the AVs compared with the IJV. Despite good image quality, CE-MRV results from this study differ from previous work concerning extracranial venous scoring24 in which IJV caliber was assessed using a linearly increasing flattening scale. Our semiquantitative approach to venous lumen morphology has precedence15 but may have made consistent scoring problematic, and resulted in moderate (IJV) and poor (AV) interobserver agreement. That said, no significant differences in scoring were observed between subject groups for either scorer. "
1287608,6.0," Our US studies found that 15.8% of MS patients met at least two CCSVI criteria, but so did a nearly equal percentage of HC subjects (17.0%). These findings are in concert with published CCSVI studies using Doppler and B-mode US which reported high heterogeneity in CCSVI diagnosis (≥2 criteria met). Across the studies listed in Table 1 (excluding Zamboni et al. studies3–5), the percentage of all patients with MS who met criteria range from 06,9,11,18 to 84%23 (across all studies average ± stdev = 20.9 ± 27.6%). The percentage of all HC subjects who met criteria range from 06,9,18,19,23,26 to 45.0%20 (average ± stdev = 9.9 ± 16.0%). Our findings further highlight the high variability in venous anatomy and physiology across subject groups. "
1287608,7.0," There were relatively equal percentages of subjects who met CCSVI criteria by US and MR. However, US seems to be a more conservative measure of IJV stenosis; lower percentages of subjects were found to have IJV stenosis by US than by either MRI measurement. Results across all subjects suggest that some small amount of reflux in the cerebrospinal venous system exists in both a normal population and MS patients, and that there is no significant difference in the percentage of these groups exhibiting reflux. "
1287608,8.0," In conclusion, our study demonstrates no significant relationship between MS and IJV stenosis and abnormal flow. We found no difference between subjects with MS and those with other neurological disease, or HC subjects, with regard to venous anatomy and physiology, irrespective of imaging modality. Our results indicate that the CCSVI hypothesis is not strongly explanatory as a cause of MS."
1128341,0.0," The demographic history of a pathogen population leaves a “signature” in the genomes of modern representatives of that population (18). Reconstructing this history allows us to gain valuable insights into the processes that drove past population dynamics (25). We recognized temporal parallels between the mycobacterial popula- tion dynamics and the timing of health policy changes managing the BU epidemic in the Songololo territory (Fig. 4). The mycobacterial population size increased in the territory during a period of decreased attention to BU that resulted in the loss of specialized personnel. After the start of a national BU program and the implementation of free-of-charge treatment, a strong increase was noted in the number of admitted BU cases which concorded with a detected inﬂection—perhaps a small drop— of the mycobacterial population size. These observations suggest that control strategies at the public health level may have decreased the size of the human M. ulcerans reservoir and that this reservoir is important in sustaining new infections. This hypothesis predicts that even if other environmental reservoirs exist, the number of M. ulcerans infections will decrease if only human cases are treated. "
1128341,1.0," The M. ulcerans phylogeography revealed one almost exclusively predominant sublineage of Mu_A1 that arose in Central Africa and proliferated in the different foci of endemicity of the Democratic Republic of the Congo, Angola, and The Republic of the Congo during the Age of Discovery (15th to 18th centuries). The principal sublin- eage of Mu_A1 was introduced into the Songololo territory around 1865 (95% HPD, 1803 to 1915), and over the subsequent century (1865 to 1974), it established itself and evolved in six distinct groups across the territory. This timing is consistent with in-depth interviews with former patients and observations of healed lesions that suggested that M. ulcerans infections already occurred in the Songololo territory in 1935 and probably even earlier (22). The genome-based time tree of Central African M. ulcerans thus revealed that the Songololo territory became an area of endemicity while the region was being colonized by Belgium (1880s). Early during the Belgian occupation, the Songololo territory was developed heavily to link the oceanic harbor of Matadi by rail with Kinshasa, where the Congo River becomes navigable, opening up the entire interior of the Democratic Republic of the Congo for economic exploitation (26). The Songololo territory was already inhabited long before the arrival of the European colonizers. The Kongo people are believed to have settled at the mouth of the Congo River before 500 BCE (before the Common Era), as part of the larger Bantu migration (27). However, our data reveal that it was only after the start of colonial rule that the epidemic Songololo M. ulcerans clone was introduced, possibly through the arrival of displaced BU-infected humans or as a consequence of the substantial environmental changes brought by the Belgian occupation of the Songololo territory. "
1128341,2.0," Similarly to recent studies that used comparative genomics to investigate the microevolution of M. ulcerans during its establishment in a continent (11) or region (13, 16), the genotypes in Central Africa show strong spatial segregation (Fig. 2). This was illustrated by the regional clustering of M. ulcerans from the different endemic BU foci (e.g., Songololo and Tshela) within the phylogenies. This clustering of cases within foci of endemicity reﬂects a common source of infection within the disease focus. These repeated ﬁndings indicate that when M. ulcerans is introduced in a particular area, it remains isolated, resulting in a localized clonal expansion associated with that area. An inspection of the time tree showed that a clonal complex associated with a focus of endemicity often has been pervasive in that region for a considerable time; in the case of Songololo, for around 150 years. Even within the Songololo territory, we observed that the separated eastern Kimpese health area (tMCRA [BAPS-1], 1941) and western Nsona-Pangu health area (tMCRA [BAPS-2], 1922) groups have most likely remained segregated over a timespan of half a century, indicating that M. ulcerans spreads relatively slowly between neighboring regions. This also indicates that environmental reservoirs of the mycobacterium in that region had to remain localized and relatively isolated. "
1128341,3.0," Unlike the larger geographic scale data, at smaller geographical scales, genotypes start to co-occur. First, four Songololo BAPS groups were found to be cocirculating. Moreover, we observed completely identical genomes originating from patients living in villages separated by distances of on average 17 km, similar to the ﬁndings of recent studies (13, 14). We believe the observed “breakup” of the focal distribution pattern at smaller geographical scales can be explained by the determined low substitution rate that corresponds to the accumulation of just 0.23 SNPs per bacterial chromosome per year. The slow substitution rate severely limits the accumulation of point mutations and as such, lowers the resolving power of the genomes. This explains why (in the most extreme case), over a period of 5 years, identical genomes were discovered in three patients who lived in three different villages separated by 26 km: insufﬁcient time has elapsed for point mutations to accumulate. "
1128341,4.0," Finally, an old debate relating to the role of Angolan refugees on a resurge of BU in the Songololo territory (24) can be settled. As most of these refugees had already lived in the Democratic Republic of the Congo for several years before their diagnosis, and as some young Angolan BU patients were even born in the Democratic Republic of the Congo without even having visited Angola, an introduction from Angola was already believed to be unlikely. Now, an analysis of our phylogenies shows that no typical Angolan genotypes were detected in Songololo, indicating that the refugees were in all likelihood infected in the Democratic Republic of the Congo. "
1128341,5.0," In conclusion, in the present study, we used both temporal associations and the study of the mycobacterial demographic history in a focus of endemicity to implicate human-induced changes and activities over (recent) historical scales behind the spread of BU in Central Africa. We propose that patients with infected discharging BU lesions can contaminate slow-ﬂowing riparian and lentic environments through activities involving water contact and that these patients can constitute an important means of bacterial spread. A total of 74% of BU patients identiﬁed during a cross-sectional study (28) of the Songololo territory had ulcerative lesions (49% category I, 31% category II, and 20% category III), indicating that a high percentage of patients might be shedding bacteria into the environment and potentially indirectly infecting others. We suggest that in BU-affected areas, chains of transmission can be broken and the spread of disease stopped through improved disease surveillance, resulting in treatment during the preulcerative onset of the infection. This view is supported by the decline of BU incidence recorded in some regions of endemicity which proﬁted from such enhanced active surveillance practices (29)."
432129,0.0," Despite the great health and socio-economic burden borne by people affected with BU, little is known about the ecology and mode of transmission of this disease. MU is embedded in an environment that is inherently dynamic, but information on spatio-temporal dynamics of MU persistence and spread is dramatically lacking. The results shown here represent a step forward in the understanding of MU ecology. They provide the first account of MU spatio-temporal dynamics in aquatic communities from a variety of ecosystems within BU endemic regions. We show first that MU is ubiquitous within these regions and can be found in all types of freshwater ecosystems, but swampy areas seem more favorable to MU presence, as demonstrated in Bankim. Then, we confirm that MU is present in nearly all taxonomic groups of the aquatic community, but we show that groups common in streams are minimally colonized. Finally, we demonstrate that MU has distinctive temporal dynamics in each ecosystem and taxonomic group, suggesting that MU occurrence is probably driven by complex ecological interactions between environmental abiotic and biotic factors. "
432129,1.0," We found that MU presence in Bankim was more restricted to the south of this area, especially between the Mape´ Dam and the Mbam River, where BU cases concentrate [57] and more swamps and flooded areas prevail. The construction of the dam has been previously associated to the emergence of cases in the area [8,22] and proximity to the Mbam River was found to be a risk factor in a case-control study [22]. However, our results suggest that swamps created along the road, rather than the flooded areas created artificially by the dam or naturally near the Mbam after heavy rains, are more favorable to the presence of MU. Swamps are characterized by stagnant waters with low oxygen and high temperatures, which may create conditions favorable to MU growth and specific fauna in which to develop [16–19]. Furthermore, while water level and conditions in flooded areas are highly variable throughout the year, swamps are more stable environments, which could influence the differences observed in these two stagnant ecosystems [58]. "
432129,2.0," In contrast, MU is present everywhere across the Akonolinga region and all environments presented very similar positivity, although the highest positivity concentrated near the basin of the Nyong river. While climate, land cover or human modifications of the environment could be behind these disparate regional distributions, it could also reflect a spread of the bacteria over time. Indeed, it is possible that MU initially persists in the most favorable environments (swamps), as in the case of Bankim where cases have been reported for less than 10 years [8], spreading over time to other environments where water conditions and aquatic communities are less favorable and/or intermittent along the year, as in the case of Akonolinga where MU is endemic for more than 40 years [6]. Flying insects could be responsible of this dissemination as previously suggested [38,59]. Out of the two taxonomic orders that are both aquatic in adult stage and capable of flight (Coleoptera and Hemiptera), only Hemiptera was found positive in all types of ecosystem. Indeed, this group was found positive to MU in 65% of the sites, more than any other group of the aquatic community (table S4). "
432129,3.0," MU is present in nearly every group of the aquatic community and no taxonomic group stands among others as the major host carrier of MU. Aquatic vertebrates and invertebrates, as well as semi-aquatic groups, are positive for IS2404 and KR, with similar pool prevalence. This is in line with the idea of multi-host transmission dynamics and more particularly a transmission through ecological webs, where some species can highly contribute to MU transmission without experiencing a significantly larger positivity [32]. Nevertheless, some patterns arise for several specific taxonomic groups. Firstly, the most positive order in terms of pool prevalence are Lepidoptera larvae (caterpillars), an invertebrate with semi-aquatic families mostly living and feeding on riverine aquatic plants [42]. This finding suggests that some aquatic plants might play an important role on MU persistence and development in the aquatic ecosystem or in ecotone areas, and be a source of infection for herbivorous invertebrates. Indeed, some plants could harbor MU in endemic regions [24,60] and they stimulate its growth under experimental conditions [61]. Secondly, groups of aquatic invertebrates that were found mainly in streams such as Trichoptera and Decapoda are among the groups with the lowest pool prevalence. These findings support the hypothesis that MU might not be well adapted to environmental conditions in this type of aquatic ecosystems. "
432129,4.0," Regarding the seasonal dynamics, MU is present in freshwater ecosystems and aquatic organisms throughout the year but there are fluctuations both between seasons and within each season, as previously demonstrated for MU colonization of water bugs [38]. The highest peak in positivity appears in August and October (i.e. over the high rainy season), and then decreases progressively throughout the high dry season (November to February). These findings could be consistent with the idea of a run-off of bacteria into the aquatic environment during periods of intensive rainfall, as previously suggested [24,62]. However, the lack of correlation between rainfall patterns and the dynamics observed for the various ecosystems and taxonomic orders highlights that more complex interactions might take place within the aquatic community. Differences in feeding habits may explain the distinct colonization dynamics of different orders. For instance, while Hemiptera were found positive all year long (except in May), Coleoptera were repeatedly found negative for more than half a year (Figure 5). These two orders share many common features: they have both larval and adult aquatic stages, many are capable of flight, and their abundance dynamics along the year are very similar (Figure S3 and S4). However, while most families of Hemiptera are voracious predators of aquatic organisms (only Corixidae feed on aquatic plants), families of Coleoptera present a large spectrum of feeding habits that include predators, shredders, scrappers, filtering collectors and omnivorous organisms [41,42]. Laboratory experiments support the idea of a trophic transmission of MU through predation [30,34,63,64] and a mathematical model studying MU prevalence within 27 aquatic communities in Ghana suggested that a transmission through ecological webs is more likely than a purely environmental acquisition from contaminated water [32]. Our results support the hypothesis that biotic interactions may play a role in MU transmission and that MU dynamics could result from a complex interplay between environmental abiotic factors and variations in community assemblages. "
432129,5.0," We show that important fluctuations in MU positivity take place within each particular ecosystem. For most sites, we checked for the presence of MU in a given month and site by analysing 6 pools of aquatic organisms. This may be insufficient to demonstrate the absence of the bacteria in the ecosystem, since pool positivity overall was lower than 10%. We attempted to increase the chances to detect MU by pooling all individuals of the most abundant taxonomic orders in the aquatic ecosystem, which allowed us to pool and analyze over 60% of the 238,496 individuals sampled without losing comparability of the results. Furthermore, disparate sampling strategies for each region could be behind the differences found between the types of environment for the two study regions. Bankim was only sampled 4 months of the year as opposed to 12 months in Akonolinga. Therefore, we cannot rule out the possibility that sampling in Bankim may have taken place at appropriate times of the year for swamps but not for the other environments in this region. We tried to avoid this by sampling in Bankim at regular intervals (every three months), therefore capturing a maximum of variability along the year. "
432129,6.0," This study reinforces the idea that MU persists in a wide range of locations [24,40] and taxonomic groups [28] and the pool positivity rates described here (nearly 10% overall) are consistent with previous studies [8,28,38]. This ubiquity of MU and its persistence in the environment throughout the year contrast with the focal distribution and low number of BU human cases in endemic regions. A possible explanation is that while we are likely to be detecting one (or several) of the MU ecovars present in the M. ulcerans Dynamics in Freshwater Ecosystems and Aquatic Communities environment (previously referred to as mycolactone producing mycobacteria), this does not necessarily imply that we are detecting strains of MU with pathogenic potential to cause BU in humans [17,65–67]. Future studies comparing the strain diversity of environmental and human samples with molecular techniques such as SNP typing [68,69] could shed some light on this issue. Furthermore, we rely as previous studies on qPCR amplification of KR and IS2404 sequences as an indicator of the presence of MU, which gives no certainty of whether the DNA detected belongs to viable mycobacteria. The lack of an appropriate technique to culture MU from the environment remains a major limitation of fieldwork studies. Nevertheless, qPCR remains the gold standard for environmental studies on MU ecology [8,27,38,40]. An alternative hypothesis is that while the presence of MU in the environment reflects a potential risk for infection, many environmental and socio-economic factors may need to come together to enable MU transmission to humans. Sero-epidemiological studies have shown that a large proportion of the population living in endemic regions have been exposed to MU, but only a small fraction develop the disease [70]. Therefore, MU might only trigger BU disease under certain environmental conditions (a bacterial concentration threshold and/or contact with a competent, infected vector) or in subpopulations in high contact with potential sources of infection and with increased susceptibility to infection (due to immunity, hygiene, etc.). "
432129,7.0," In conclusion, this study provides for the first time a detailed characterization through space and time of MU presence in two BU endemic regions with distinct environmental conditions. The understanding of MU ecology to date is still limited, especially regarding the conditions that allow this mycobacterium to persist in the environment and be transmitted to humans. Our study attempts to complete previous approaches by sampling multiple aquatic communities over time in order to better understand the influence of aquatic ecosystems on MU presence and its dynamics along the year. The global trend we describe for MU dynamics could be the result of complex ecological processes, with interactions between environmental abiotic and biotic factors that require deeper analysis, something that is beyond the scope of this paper. However, we believe that coupling data produced by such field studies with fine-scale epidemiological data and integrated through statistical and mathematical models could provide a major step forward in the understanding of MU ecology and BU mode of transmission."
432234,0.0," In this paper we present results from a large scale study of M. ulcerans in the environment. Although a number of studies have reported the presence of M. ulcerans in environmental samples from endemic regions [13,16,17,33], study where standardized ecological methods were used to reduce sampling bias, and the first to include longitudinal data from both Buruli ulcer endemic and non-endemic sties. One of the mysteries of Buruli ulcer is the close proximity of endemic and non-endemic villages. For example, whereas the disease is rarely reported from the Ga East district of the Greater Accra region in Ghana, it is endemic in the Ga West district despite the fact that endemic and non-endemic villages may be separated by only a few kilometers (Figure 3). Since the climate, rainfall, plant populations and ethnic groups in Ga East and Ga West are similar it has been difficult to understand the differential occurrence of Buruli ulcer within these regions. "
432234,1.0," The most important finding from this study is that, whereas Buruli ulcer occurs within discrete geographic village foci within endemic regions, M. ulcerans is widely distributed in water bodies in both endemic and non-endemic villages in the Greater Accra and Ashanti regions. This is consistent with its position as an environmental pathogen. We have also been able to repeatedly detect the presence of M. ulcerans within some sites over a 27 month framework suggesting the long term survival and presence of M. ulcerans in specific locations. These results clearly show that the focal occurrence of Buruli ulcer within the Greater Accra and Ashanti regions cannot be explained by the presence or absence of M. ulcerans in the environment. Thus other factors such as demography and human behavior may be important in the epidemiology of Buruli ulcer in these regions. "
432234,2.0," In contrast, there are large geographic areas in West Africa such as the Volta region of Ghana, or drier Northern parts of Ghana, Benin and Togo, where Buruli ulcer has never been reported. It has been assumed that the absence of Buruli ulcer from these regions is based on environmental constraints which restrict the growth of M. ulcerans or potential reservoir species. Results from an on-going project in the Volta region confirm this hypothesis in that we have failed to reveal a single M. ulcerans positive sample out of hundreds of invertebrate, water filtrate or macrophyte samples from 20 sites sampled. The absence of Buruli ulcer in Volta is explained by the absence of M. ulcerans (work in progress). "
432234,3.0," In a clinical setting the use of a single PCR target for detection of a pathogen is widely accepted. However, the use of a single PCR target for identification of bacteria in an environmental sample is rarely adequate. In Ghana, analysis of many IS2404 positive samples revealed the presence of mycolactone producing mycobacterial species (MPMs) other than M. ulcerans as had been predicted [29]. In contrast, in Australia IS2404 PCR appears to be specific for M. ulcerans because of the absence of other MPMs [33]. Here we provide the first evidence for the presence of MPMs in West Africa and show that MPMs and M. ulcerans share aquatic environments. The pathogenic potential of MPM for humans is unknown, although the lower growth temperature of some of these species makes them unlikely human pathogens [26]. The fact that the strain complexity of MPMs and M. ulcerans is greater in endemic areas and greatest within the Greater Accra region is an intriguing finding which needs further investigation. "
432234,4.0," The use of geographic-specific VNTR profiles in following chains of transmission is extremely important since the heteroge- neity of M. ulcerans isolates appears to differ within different West African countries [23,24,25]. For example, data based primarily on patient isolates from Benin led to the conclusion that there was a single West African M. ulcerans clone. However, several biovars have been identified in Ghana [23]. Our results agree with those of Hilty et al [23] in showing the presence of that at least 3 different VNTR profiles in Ghana. Thus it is important when discriminat- ing between M. ulcerans and other MPM that a geographically representative set of patient isolates is used. "
432234,5.0," Our initial concerns regarding the effect of low target copy number on the sensitivity of PCR methods reflected our naivete´ regarding PCR theory. We had not considered that the efficiency M. ulcerans in Aquatic Environments in Ghana of the PCR reaction depends on many factors including the efficiency of primer binding, the length of the product and local DNA conformation or that because the reaction is exponential, the first few targets bound may rapidly become the major products. Experimental results confirm this theory since others have found no difference between the use of IS2404 PCR and that of 16sRNA PCR for detection of M. ulcerans in human samples despite the enormous difference in copy number [34] and results from VNTR analysis of clinical isolates show gel bands with an intensity never reported for IS2404 PCR [23] . It is possible that the fact that VNTR sequences are non-coding segments of DNA may make them more accessible to primer binding. "
432234,6.0," Our studies confirm the presence of M. ulcerans in predacious aquatic insects including Belostomatidae and Naucoridae families reported by Portaels et al [15] and extend these findings by showing that VNTR profiles from these insects match those of human isolates of M. ulcerans. Belostomatids were common in many sites sampled throughout the year. However, even where large numbers of Belostomatidae were collected the rate of M. ulcerans infection was very low. In Ghana, despite repeated seasonal sampling the numbers of naucorids found were very low (paper in preparation). Evidence for the role of naucorids as potential M. ulcerans vectors comes from studies in Cote d’Ivoire [16]. Insect population studies are needed to confirm whether naucorids are more abundant in Cote d’Ivoire than in Ghana. "
432234,7.0," Our results show that M. ulcerans is widely distributed within invertebrate communities in aquatic environments. However, the M. ulcerans-positive, predacious invertebrates are none of hematophagous; thus the frequency with which humans are bitten would be expected to be quite low [35]. Although potential trophic relationships exist between several taxa studied (belostomatids, for example, feed on many other invertebrates and vertebrates and also cannibalize each other), it will take considerably more work to elucidate chains of transmission within the environment. Results presented here are based on determining the presence or absence of M. ulcerans in an environmental sample. Further studies need to be conducted using quantitative PCR methods to determine which species are most heavily infected and thus are more likely to serve as vectors. "
432234,8.0," Although it has been reported that snails and fish may harbor M. ulcerans [17] our results suggest the possibility that many of the IS2404 positive mycobacteria detected are MPM other than M. ulcerans. In our studies M. ulcerans was never detected in fish or snails, although other MPM were identified in later studies. The most consistently M. ulcerans-positive samples detected were filtered water and biofilms on glass slides. This suggests that exposure of open lesions to infected water cannot be ruled out as a potential source of infection. "
432234,9.0," A general problem regarding detection of M. ulcerans in environmental samples is that evidence has come almost solely from detection of M. ulcerans DNA and under-estimation of M. ulcerans could result due to the presence of PCR inhibitors. Our results suggest that current methods are effective in eliminating PCR inhibitors since dilution of samples did not result in the detection of many additional ER positive samples and none of those detected through dilution could be confirmed by sequencing. Despite the broad spectrum of samples we did not find evidence for inhibitors in any particular taxa or sample type tested. Nonetheless, the possibility exists that the number of positive M. ulcerans positive samples may be underestimated. "
432234,10.0," The use of slide biofilms for trapping mycobacteria in the environment has proven particularly useful since it provides preliminary physical evidence for the presence of mycobacteria (AFB staining) along with molecular evidence, and facilitates longitudinal studies. The numbers of slides used and placement of PVC pipes are crucial because of the inevitable loss of slides through changes in water level, or disturbance by animals or humans. There was a decrease of DNA samples from slides giving a VNTR profile matching M. ulcerans between 42 and 98 days in the two water bodies (Figure 5). There was, however, an increase in DNA samples from slides producing a VNTR profile matching other MPM s. These data suggest bacterial community dynamics between M. ulcerans and other MPMs. "
432234,11.0," In summary, we have developed new methods for mapping the distribution of M. ulcerans in aquatic environments and applied these in the Greater Accra and Ashanti regions of Ghana. This work is part of a much larger five year project in which data from water chemistry, LandSat satellite imaging of land cover, and macrophyte and aquatic invertebrate population structure will be used to define the broad ecology of M. ulcerans. The presence of M. ulcerans in both endemic and non-endemic villages within endemic regions suggests that studies of human ecology will be necessary to unravel the mysteries surrounding the transmission of M. ulcerans to in this work is to define the M. ulcerans humans. Our goal environment in order to develop programs for preventing human exposure. The findings presented here show the possibility of tracing transmission of M. ulcerans from the environment to humans. This work represents a small step towards solving the mysteries surrounding human infection."
434379,0.0," The current diagnostic delay experienced by Buruli ulcer patients contributes to morbidity and increased economic hardship [37]. A diagnostic test that is rapid, inexpensive, sensitive, specific and suitable for in-field use would facilitate the timely diagnosis of BU and is a World Health Organization (WHO) research priority. Current laboratory diagnostics including culture, PCR, AFB microscopy and histopathology all have various limitations. We have used comparative genomics to identify M. ulcerans specific sequences to explore the usefulness of the encoded proteins for use in the serodiagnosis of Buruli Ulcer in endemic areas of West Africa. "
434379,1.0," Whilst bioinformatic methods were initially used to identify M. ulcerans specific CDS, eleven of these ‘‘M. ulcerans specific’’ CDS were found in at least one of the M. marinum strains tested (Table 1). These CDS are distributed across the M. ulcerans genome, and were all situated within regions of DNA that are not present in the M. marinum M genome. It is interesting to note that some of these sequences appear restricted to certain M. marinum sequence types that are genetically closer to M. ulcerans than the sequenced M. marinum M strain. For example MUL_4213 was restricted to the ST3 and ST5 strains tested and MUL_0027 was restricted to the ST3 strains tested. Some of these sequences may be useful as genotyping tools to more clearly delineate the relationships between M. ulcerans and M. marinum strains. These findings are consistent with the known high genetic relatedness of M. ulcerans and M. marinum [10,13], and underlies the need for experimental validation of bioinformatic results. "
434379,2.0," Only nine M. ulcerans specific sequences were conserved amongst all mycolactone producing mycobacterial strains tested. However, in agreement with previous findings that African M. ulcerans strains form a close genetic complex with relatively little variation at the whole genome level [38,39,40], all selected chromosomal genes were present in all African M. ulcerans isolates that were examined. As pMUM plasmids are known to vary in size and gene content between M. ulcerans strains [11,33], it is not surprising that only six of 33 non-PKS CDS were completely conserved. However, plasmid CDS may be the most specific to M. ulcerans as they appear to be restricted to mycolactone producing mycobacteria. "
434379,3.0," During the course of this work the sequences of two other pMUM plasmids from MPM became available [11] and revealed a number of differences between these plasmids with regards to the potential antigens chosen for this study. These differences can explain the inability to detect some of the selected CDS in MPM strains. For example, MUP006 orthologs in pMUM002 and pMUM003 contain a different 59 nucleotide sequence, explaining the absence of a PCR product in some MPM strains. Conversely, MUP068 and MUP070 orthologs in pMUM001 and pMUM002 contain different 39 nucleotide sequences. Furthermore, MUP065 appears to be a pseudogene in pMUM001, as its ortholog in pMUM002 and pMUM003 are significantly longer, and MUP067 is deleted from pMUM002 and pMUM003. All of these examples serve to highlight the genetic variability in the non-PKS component of these pMUM plasmids. "
434379,4.0," In this study 44 M. ulcerans specific sequences were cloned with 37 (84%) of these successfully expressed in E. coli. Seven proteins were unable to be expressed in multiple E. coli strains, and the lack of overexpressed protein was confirmed by western blotting (data not shown). Similar studies that have attempted to identify novel CDS in M. avium subsp. paratuberculosis have also shown variable rates of production of 6 x His tagged proteins (between 25 – 69% of cloned CDS able to be expressed) [41,42], indicating that this tag may reduce expression of certain proteins. Furthermore, all proteins in this study were found to be insoluble under the expression conditions used, and six of the seven proteins that were unable to be expressed were predicted to be integral membrane proteins, with overexpression of these proteins possibly leading to toxicity. Both of these problems have been previously described in other mycobacterial protein expression studies [43,44,45]. Clearly, further optimization of the expression of these proteins, including the use of alternate proteins tags or different host-based expression systems (including mycobacteria) may be necessary to obtain useable amounts of protein for future testing. Recently, a Gateway-based vector has been developed for the overexpression of proteins in M. smegmatis and has shown that proteins which were previously insoluble in E. coli could be expressed in soluble form in M. smegmatis [46]. Furthermore, M. tuberculosis proteins have been successfully produced in the methylotrophic yeast Pichia pastoris with enhanced patient serum reactivity compared with those produced in E. coli [47,48]. The future use of either of these systems for the proteins that proved difficult to produce in this study may be warranted to investigate their antigenicity. "
434379,5.0," We were able to confirm that patients with BU had significantly greater IgG antibody responses to seven of 33 purified M. ulcerans proteins as determined by ELISA compared with a control group (Figure 1). The inability of the remaining 26 proteins to discriminate between patients and controls is likely due to either the inability of a particular protein to elicit an antibody response or through shared epitopes with proteins from other sources leading to cross-reactive antibody responses. However, none of the proteins were able to discriminate between patients and control subjects from the same BU-endemic area and are therefore unlikely to be useful as a diagnostic test for M. ulcerans infection. By combining measurements of antibody responses to six antigens from people living in a BU endemic area, we were able to discriminate between subjects resident in endemic areas of Benin and controls living in a non-endemic area. Although it remains a possibility that these data reflect cross-reactive antibody responses to other mycobacteria, we suggest that the most likely interpre- tation of these findings is that many residents of BU-endemic areas have been exposed to M. ulcerans. "
434379,6.0," Somewhat surprisingly, the most discriminatory antigen was Hsp65, which has 94% homology to the same protein from M. tuberculosis, whilst antigens with no orthologues in other mycobac- teria had lower sensitivity and specificity. Hsp65 is known to be an immunodominant antigen in the antibody response to mycobac- teria [35] and we anticipated that all groups would have had significant levels of exposure to other mycobacteria from either BCG vaccination, tuberculosis infection, and from exposure to other environmental mycobacteria. We therefore anticipated that Hsp65 would be a sensitive but non-specific antigen. There may be several explanations for our findings. Firstly, patients and endemic controls were both from a region of high BU prevalence and therefore may have been recently and/or frequently exposed to M. ulcerans, leading to continual boosting of antibody responses that does not occur in the non-endemic control group. The fact that M. ulcerans elicits responses dominated by antibodies to Hsp65 at all stages of the infection in mice supports this hypothesis (35). Alternatively, it is possible that our patient and control groups differed in their exposure to mycobacteria other than M. ulcerans, and therefore the results do not reflect M. ulcerans specific antibody responses. "
434379,7.0," The small heat shock protein, Hsp18, had previously been identified as an immunodominant antigen that may be useful for predicting exposure to M. ulcerans [34]. Results from the current study, however, showed that Hsp18 was not able to discriminate between individuals living in an endemic area and those who reside in a non-endemic area. The reasons for these differences are unclear, however, a possible explanation may be that here, antibody responses were quantified by ELISA, as opposed to qualitative western blot analysis in the study by Diaz et al [34]. "
434379,8.0," Although the other antigens we identified as M. ulcerans specific by our comparative genomic approach did not appear to be as sensitive or specific as Hsp65 in predicting individuals living in a Buruli endemic area, they nevertheless did show that significant antibody responses were detectable, and three antigens (MUL_0513, AT-propionate, KR-B) showed significant ability to discriminate individuals living in a BU endemic versus non- endemic area. Their use in combination may show additive effects and, in conjunction with case-control studies, may permit the development of an assay for detecting individuals exposed to M. ulcerans. More research in this direction would greatly assist efforts to uncover environmental reservoirs and understand transmission pathways of the bacterium. "
434379,9.0," Although a number of M. ulcerans specific sequences were investigated in this study, there were still a large number that were initially ruled out based on predictions of their cellular location or function. It is possible that some of these potential antigens may be good candidates for further analysis. Based on our findings of high response rates among patient and endemic control groups it appears unlikely that serological approaches will be useful for diagnosing Buruli ulcer. Indeed, the development of serodiagnos- tics for other mycobacterial diseases, such as tuberculosis, has proved difficult and many of the currently available serodiagnos- tics for mycobacteria have limited usefulness in the clinic [49,50]. "
434379,10.0," An alternative to BU serodiagnosis may be possible via direct antigen detection approaches. The extent and distribution of antigens within M. ulcerans lesions or within a patients circulation has not been defined, however, there is the possibility that some of the unique proteins described in this present study may be able to be developed as the basis for antigen capture assays. A diagnostic test based on this methodology would overcome the problems of cross-reactivity seen with tests based upon antibody responses, and would be more appropriate for resource limited areas than tests based on T-cell responses. This project has helped define the antigenic repertoire of M. ulcerans and research is continuing in this vein to investigate the feasibility of other diagnostic modalities."
2491053,0.0," Invasive monitoring of pulsatile ICP has previously proven to be a precise tool to select which iNPH patients who will respond to surgical shunting. In this study, we tested the utility of PC-MRI to non-invasively assess a surrogate marker for pulsatile ICP, MRI-dP, obtained at level C2. MRI-dP did not correlate with invasively obtained pulsatile ICP from over-night monitoring, and did not discriminate iNPH patients from healthy subjects. "
2491053,1.0," Increased pulsatile ICP due to reduced intracranial compliance is common in iNPH [7], and patients with this disease should therefore be well suited for a study of non-invasive assess- ment of pulsatile ICP using PC-MRI. In the present study cohort, increased pulsatile ICP expressed by MWA above threshold level was diagnosed in 17 of 22 patients. Surgical shunting yielded a clinical improvement in 16 of 17 treated patients, indicating a high proportion of what may be considered “true iNPH”. "
2491053,2.0," MRI-dP has previously been applied in a proposed method for non-invasive estimates of static (mean) ICP [14]. With this method, the elastance index is estimated by dividing MRI-dP by the per cardiac cycle total intracranial volumetric change (MRI-dV), i.e. MRI-dP/MRI-dV. The linear relationship between elastance and ICP is expected owing to the monoexponential relationship between intracranial volume and pressure[22]. Precise assessment of MRI-dP is thus an important precondition for such estimation of static ICP as well. "
2491053,3.0," Should a non-invasive measure such as MRI-dP be valid, it would have to reflect its gold stan- dard of invasive measurement (pulsatile ICP). Both pulsatile and static (mean) ICP demonstrated in this study large fluctuations during the overnight monitoring, and this phenomenon is also a determining factor as to why clinical decisions based on monitoring of pulsatile ICP are made after analysis of observations from an extended time span (Fig 4). A PC-MRI based method applied to obtain measures of pulsatile ICP through a surrogate parameter like MRI-dP from a short time interval should therefore be interpreted with great care. One previous study could not demonstrate different MRI-dP between Chiari malformation type 1 patients and controls[23], even though elevated pulsatile ICP is frequently observed when measured over-night[24].The aqueductal stroke volume is another, MRI derived, CSF velocity based parameter obtained from a short time interval, which has been proposed to serve as a surrogate for ICP recordings[8], and were also unable to demonstrate any association with its invasive counterpart[13, 25]. "
2491053,4.0," There may be several reasons as to why MRI-dP derived from one simple PC-MRI acquisi- tion does not reflect pulsatile ICP measured overnight. First, MRI-dP is based on acquisition of CSF velocity change at the upper cervical spinal canal, and is thus a surrogate marker for intracranial pressure change. A simplified model of a rigid, cylindrically shaped tube is desir- able in order to make valid assumptions in fluid dynamic equations used in post-processing of PC-MRI data, where attempts made intracranially, with much more complex anatomy, are mostly restricted to the Sylvian aqueduct. However, even though the upper spinal canal is anatomically in close proximity with the intracranial compartment, the geometry is funda- mentally different, and from computational fluid flow dynamics, it has been demonstrated that lower pressure values can be expected at level C2 than in the posterior fossa due to pres- sure gradients that become steep in the cervical canal[26]. "
2491053,5.0," Secondly, the CSF flow at level C2 is the result of a dyssynchronous pressure pulsation caus- ing a pulsating cranio-cervical pressure gradient pumping CSF in and out of the cranial vault. The pressure gradient estimated from MRI is relatively small, and in this study, less than 0.1 mm Hg per cm in all patients. This magnitude corresponds well with previous studies. From numerical studies, where the CSF flow has been modeled in rigid and impermeable surround- ings, the fluid flow has been predominated by velocities orthogonal to the axial plane, and the observed pressure gradients at level C2 in healthy subjects is also less than 0.1 mm Hg per cm [27, 28]. Computations that take into account the elasticity of the cervical spinal cord, tonsillar motion and the presence of nerve roots and denticulate ligaments, all predict pressure gradi- ents of similar magnitudes[29–31]. Hence, the pressure gradient between the cervical and cra- nial compartments is but a fraction of the total pressure pulsation. "
2491053,6.0," Moreover, the pressure gradient computation in this study is based on a number of simpli- fying assumptions such as rigid and impermeable surroundings and laminar flow. CSF flow patterns are, however, complex due to the anatomy of the cervical subarachnoid space[26], and flow velocities in axial sections of the spine have been shown to have non-uniform distri- butions throughout the cardiac cycle as demonstrated from computational simulations[28]. In our study, the pixel-by-pixel analysis revealed a wide dispersion of flow velocities from within the same PC-MRI slab (Fig 8). Averaging of flow velocities from a ROI defining the CSF space therefore does not fully describe the diversity or complexity of velocity changes that occurs through a section of the cervical spine during one cardiac cycle. "
2491053,7.0," Interestingly, it may be noted that the healthy control with smallest CSF ROI area also had the largest MRI-dP among controls (No. 2 in S3 Table), and larger than in many iNPH patients with invasively proven increased pulsatile ICP (S2 Table). Previously, the inverse and almost linear rela- tionship between MRI-dP and CSF flow area has been reported[14], and very steep pressure gra- dients have also been demonstrated in Chiari patients with tonsillar crowding at the foramen magnum[32]. For the patient group in our study, there was a trend towards an inverse correlation between MRI-dP and area of the subarachnoid fluid area at level C2 (ROI area), but this did not reach significant levels (R = -.32, P = .15). However, we notice that measuring reliability (ICC) was almost identical for MRI-dP and ROI area measurements in healthy controls. Thus, while MRI-dP should ideally be a marker of pulsatile ICP only, it may be hypothesized that a narrow CSF space surrounding the spinal cord may influence directly on MRI-dP. However, CSF area is already implicitly accounted for when estimating pressure based on the Navier-Stokes equation. "
2491053,8.0," Finally, but not least, recent studies have demonstrated respiration to have a major influ- ence on CSF flow [33, 34]. With cardiac gated PC-MRI, which can be considered to represent current state-of-the-art methodology in PC-MRI based measurements of intracranial and intraspinal CSF flow quantification, only cardiac beat induced CSF flow is detected. Patient breathing is not controlled for. "
2491053,9.0," The invasive ICP measurements of this study were not performed synchronously with PC-MRI, but overnight, as standardized according to previous empirical observations support- ing this routine. In principle, our data do therefore not in principle contradict that MRI-dP may reflect pulsatile ICP in real time. The comparison with invasive monitoring for a pro- longed time period is, however, highly relevant, because clinical decision making is rarely based on ICP monitoring from a time interval as short as the duration of a PC-MRI acquisi- tion. It has previously also been validated that over-night and daytime measurements of pulsa- tile ICP are comparable, unlike measurements of static ICP[2]. "
2491053,10.0," Another limitation was that technically adequate PC-MRI was obtained in less than 2/3 of enrolled patients, while all examinations in healthy subjects were successful. NPH patients are typically elderly and suffer from discomfort during long MRI scan times, and the experimental PC-MRI acquisitions were performed towards the end of the imaging protocol. In iNPH, dementia is a common feature, which may be a challenge to patient cooperation throughout the scan. MRI-dP from PC-MRI may therefore seem more feasible in subjects that cooperate well rather than in patients where compliance to instructions from the MR technician can be a challenge. The inability to exam patients with the poorest level of cooperation skills may have introduced bias to the study, because more severely affected patients might have been more likely to be excluded. "
2491053,11.0," Moreover, the velocity encoding gradient (venc) was set at a low level for the PC-MRI acquisitions in patients and healthy controls. Because of flow heterogeneities, flow aliasing was quite frequent. We consider, however, this potential source of error to be small because of the aliasing correction procedure and that flow velocity values from each time point represent the average of velocities from the entire region of interest. Pixels with any inaccuracies of flow velocity measurements should therefore not have contributed substantially to the calculated MRI-dP. "
2491053,12.0," As for PC-MRI based measurements of CSF flow in general, a major concern with the tech- nique would be the extraction of data from a limited time interval to make a diagnosis in patients where well-known physiological fluctuations are present. In the present study, all PC-MRI exams may be assumed to have been performed within normal physiological bound- aries, as they were obtained within a normal range for heart rate (Tables 2 and 3), and the sta- tistical analysis did not reveal any systematic differences in heart rate (HR) between PC-MRI and ICP monitoring. This indicates that influence from patient stress and discomfort was not very different during PC-MRI and ICP monitoring, and this source of bias in comparing the methods should therefore have been modest."
1442658,0.0," Multiple sclerosis is an inflammatory disease characterized by demyelination centered around the cerebral veins in the white matter (1, 2). CCSVI has been suggested as a new hypothesis for the etiopathogenesis of MS disease in terms of this topographic relationship. CCSVI-related MS hypothesis suggests that significant decrease in CSF flow occurs through cerebral aqueducts secondary to impaired venous outflow from the CNS (8, 17, 18). The normal CSF circulation depends on the proper balance between CSF ultrafiltration (from lateral ventricular veins) and venous system clearance from CSF opening at the level of dural sinuses (19, 20, 21). "
1442658,1.0," Zamboni et al. (17) have described abnormal venous hemodynamics by identifying multiple extracranial venous structures of unknown origin using extracranial-transcranial color Doppler and selective venography. CCSVI-related MS hypothesis claims that venous reflux leads to iron deposition in the CNS, triggering autoimmune process with disruption of the blood-brain barrier (8). PC-MRI facilitates accurate and noninvasive evaluation of the flow direction and flow rate of intracranial blood and CSF (12, 13, 14, 22, 23). "
1442658,2.0," Zamboni et al. (10) reported a strong relationship between CCSVI and MS. However, Zamboni et al. (9, 11) found a significantly higher prevalence of CCSVI in MS patients, as compared to controls; and consequently, a decrease in net CSF flow (mL/beat) passing through the cerebral aqueduct. They concluded that CCSVI has a marked effect especially on intracranial fluid balance in the pathophysiology of brain. "
1442658,3.0," In their study, Zamboni et al. (10) addressed the questions of whether CCSVI affects the clinical process of MS and whether venous stenosis is a cause or a result of MS. Baracchini et al. (24) and Simka et al. (25) reported a cause and effect relationship between MS and CCSVI; while Patti et al. (26) reported no relationship between MS and CCSVI. However, CSF flow dynamics were not mentioned in these studies. "
1442658,4.0," Sundström et al. (27) performed a study to test the hypothesis of CCSVI and, unlike Zamboni et al. (11), did not detect any difference in internal jugular venous outflow and aqueductal CSF flow between RRMS patients and the control group. Gorucu et al. (28) reported that the net CSF flow volume showed no significant differences between MS patients and controls. In the present study, the net CSF flow volume (mL/beat) and average flow (mL/sec) parameters evaluated with PC-MRI, were not found to be statistically significant between MS patients and the control group. Additionally, MS patients were divided into 2 groups according to the condition of plaques that were detected on MRI: MS patients with active plaques who exhibit contrast enhancement and those with chronic disease non-enhancement. To our best knowledge, PC-MRI study for CSF dynamics in the active phase of the disease has not been reported. If CSF flow is decreased due to impaired extracranial venous drainage in MS patients, based on the study of Zamboni et al. (11), we expected lower CSF flow values in the intense period of active inflammation. Therefore, we determined whether PC-MRI contributes to differential diagnosis of active and chronic disease in MS. Craniocaudal (forward) and caudocranial (reverse) CSF flow volumes (mL/beat) and aqueductal area were higher in the MS patients with chronic plaques, as compared to patients with active plaques; and of these parameters, reverse volume was statistically significant. In addition, these parameters were higher in MS patients, as compared with the control group. "
1442658,5.0," Our study compared flow speeds in addition to CSF flow volumes passing from the cerebral aqueduct in MS patients. No significant difference was found in terms of average velocity. However, peak velocity value was found to be significantly higher in MS patients with chronic plaque, as compared with the control group. "
1442658,6.0," Increased craniocaudal and caudocranial CSF flow volume and aqueductal area in MS patients could possibly be due to cerebral atrophic changes that may occur at every stage of MS. CSF spaces expand with cerebral atrophy and consequently, CSF flow volumes in a cardiac cycle pass through the cerebral aqueduct. In MS patients, peak velocity increase can depend on the volume increases in the CSF circulation. Thus, in MS patients with these findings, absence of a significant change in the net CSF flow volume and average velocity, supports this concept. Likewise, Chiang et al. (29) concluded that the aqueductal CSF flow should not be considered independently of ventricular morphology. Zamboni et al. (9) suggested that impaired CSF dynamics in MS patients contributes to an increase in volumes of the third and lateral ventricles. In addition, reverse volume in the definitive diagnosis of active and chronic period MS disease, was higher in MS patients with chronic plaques. This result can be used in the place of contrast-enhanced MRI. "
1442658,7.0," We did not evaluate the venous outflow abnormalities and the ventricle volumes, which is a limitation of our study. In addition, PC-MRI may result in errors in quantitative measurements, due to non-linear gradients, partial volume effects, and errors in placing ROI (12, 30, 31). In cases with narrow aqueducts, difficulties in placing ROI can increase the error rate (32). For a reliable measurement, the area of aqueducts should be over 1.5 mm2 (14). In our study, the case with the smallest aqueducts area was in the control group with 1.6 mm2. "
1442658,8.0," In conclusion, in the present study, the peak velocity, caudocranial and craniocaudal CSF flow volumes, and aqueductal area were significantly higher in the MS patients. Within the framework of the CCSVI hypothesis, the results of our study did not support the hypothesis that CSF flow decreases in MS patients. Nevertheless, CSF flow, was altered in MS patients. The increase in CSF flow in our study could be due to atrophy-dependent ventricular system dilation and subsequent decrease in the overall cerebral aqueduct resistance. In turn, the altered cerebral flow physiology may be used to differentiate active versus chronic disease. Larger controlled studies are needed to determine the entire scope of CSF dynamics and extracranial venous hemodynamics within the framework of the CCSVI hypothesis on MS etiology and pathogenesis."
433520,0.0," The aim of this study was to assess the role of water bugs as hosts and vectors of M. ulcerans, in the complete environmental context. To this end, we carried out an extensive field study on unprecedented temporal and spatial scales, monitoring the distribution of water bugs harboring M. ulcerans and the dissemination of M. ulcerans in the environment. We assessed water bug diversity and determined the frequency of insect tissue colonization by M. ulcerans in the various seasons, in an area in which Buruli ulcer is endemic (Akonolinga, Cameroon). For the purposes of comparison, the study also covered an area in which Buruli ulcer is not endemic. In short, the specificities of the study are: (i) focusing entirely on water bugs; (ii) the collection of samples from the same water body in all four seasons and (iii) large-scale sample collection in Buruli ulcer endemic and non endemic areas, with subsequent analysis of the captured water bug specimens (7407 specimens collected, 696 pools analyzed). "
433520,1.0," We document here, for the first time, fluctuations in the density of water bug families in an area in which Buruli ulcer is endemic, over the course of a year. The highest density of water bugs is recorded in January, during the long dry season. Variations of water bug density described here are in agreement with those in another study in a tropical area (Costa Rica) [45]. The causes of these fluctuations remain unclear, but several possible factors have been identified, including abrupt changes in environmental conditions or prey density [33,45–46]. Seven water bug families were identified in Buruli ulcer endemic and non endemic areas, including many unknown species. This latter point is not surprising, as determination keys for water bug species are not yet available for West Africa. One key finding of our study was the striking difference between the endemic and non endemic areas in terms of the density of water bugs, with the density in the endemic area more than 10 times higher that in the non endemic area. It can be noticed that this observation is not in accordance with results in another published study [31]. However, the conclusions of this previous study were drawn based on data for a significantly smaller number of specimens (only 200 water bugs; 2% of the invertebrates collected [31]). "
433520,2.0," M. ulcerans DNA was detected in five of the seven families of water bugs in the endemic area. The mean rate of colonization was about 10%. However, large fluctuations were observed in the insect colonization by M. ulcerans (1.4 to 33.9%). As rate of observed for other hosts of microorganisms [47–48], there was no correlation between water bug density and rates of water bug colonization by M. ulcerans. We detected no M. ulcerans DNA in insects from the non endemic area, despite the high rates of colonization reported for the nearby endemic area. One key difference between the endemic and non endemic areas concerned human activity. Despite their close physical proximity, the non endemic area was largely unaffected by human activity, whereas, in the endemic area, the bank of the Nyong River had been deforested, for agricultural and fishing activities (Figure 1B and C). Interventions disrupting the environment have been identified, in several studies, as factors potentially favoring the establishment of M. ulcerans in remodeled environments [13– 15,49]. The observed fluctuations in colonization rate may be accounted for by changes in the level of water in the Nyong River, which falls markedly in the dry season. Our results are supported by the findings of an epidemiological study performed in 1993 [50], in which low water levels in the dry season were found to favor the transmission of M. ulcerans to humans. Several factors may be involved in this phenomenon, including greater access to nutrients (resulting in an increase in M. ulcerans density) and the abundance of aquatic vegetation favoring M. ulcerans biofilm formation [26,30,51] (increasing the level of contact between M. ulcerans and water bugs). The water bugs from the family Corixidae were the only phytophagous insects inventoried here. These water bugs were detected only in January, when they were present in high abundance, and displayed the highest rate of colonization by M. ulcerans (43.7%). The high frequency of colonization by M. ulcerans in Corixidae supports the hypothesis that aquatic plants may be the primary reservoir of M. ulcerans, as previously suggested [26]. This specific family may be involved in spreading the bacillus to other trophic levels, as they are eaten by other water bugs, aquatic invertebrates and vertebrates. These observations suggest that M. ulcerans may colonize different levels within the trophic chain (aquatic plants, invertebrates and vertebrates), as already consid- ered [21,30,52–53]. In addition, three water bug families are known to be good flyers [33,45,54–57] and were identified as M. ulcerans hosts in our study. These water bugs may therefore be involved in disseminating M. ulcerans in the environment, as previously proposed by Portaels and Meyers [34]. "
433520,3.0," We showed that water bug saliva could harbor bacilli. M. ulcerans may therefore be present in the saliva under natural conditions, with the bacilli colonizing the salivary glands of the insect. This could provide a route for M. ulcerans transmission in natural conditions, in accordance with previous experimental demonstrations in laboratory conditions [24,28]. It should be noted that the bacilli present in the saliva of Appasus sp. induced an M. ulcerans-containing lesion following the inoculation of mouse tail. However, it was not possible to isolate these bacilli by conventional culture methods. The lack of appropriate culture media and decontamination procedures adapted to the isolation of M. ulcerans (from the environment) thus remain a major handicap, hindering investigations of the ecology and mode of transmission of this mycobacterium. "
433520,4.0," The various results presented above provide further evidence that water bugs are hosts and vectors of M. ulcerans, and provide insight into the environmental context underlying transmission. However, no definitive conclusion can yet be drawn concerning the precise importance of this route of transmission. The presence, in human sera, of antibodies binding water bug salivary gland extracts may be accounted for by the exposure of humans to water bug bites [27]. Indeed, reports of the exposure of humans to blood-feeding arthropods, which are known to act as hosts and vectors for parasitic microorganisms, are becoming increasingly frequent [38–39]. To gain a complete picture of the transmission route it would have been desirable to explore the relationship between the incidence of the disease in humans and the rate of colonization of water bugs by M. ulcerans, over a one-year period. Such exploration was however hampered by the amount of accessible information, with several important parameters not available : (i) incubation time between exposure to M. ulcerans and the appearance of the first clinical lesions (currently estimated at between a few weeks and several months); (ii) the slow progression of clinical lesions, resulting in patients being diagnosed at different stages of the disease (rarely at early stages) and (iii) the small number of Buruli ulcer patients diagnosed with early lesions between October 2007 and July 2008 in Akonolinga (84 Buruli ulcer patients, including 15 with early lesions). that suggest In conclusion, this study sheds light on the natural history of M. ulcerans within its ecosystem. The observed fluctuations in insect colonization rates there may be a particularly favorable period for the development of M. ulcerans in natural conditions, and a favorable period for the transmission of M. ulcerans to humans (as previously suggested [50,58] and observed for Plasmodium sp. [47,59–60]). It is also noticeable that the results here can be put to advantage for practical applications, with surveillance and prevention purposes [27,61]. More precisely, our work suggests that the detection of M. ulcerans in water bug saliva could be used as an environmental indicator of the risk of M. ulcerans transmission to humans. It would then be possible to set up environmental surveillance (detection of M. ulcerans DNA in water bug tissue and saliva) in non endemic areas close to Buruli ulcer endemic areas. Health messages concerning environmental risk factors could be specifically targeted at populations newly exposed to the risk of M. ulcerans infection, as is already the case for protective factors (wearing long clothing during farming activities and use of bed nets) [16]."
1691937,0.0," Buruli ulcer is one of the neglected tropical diseases in the world for which the mode(s) of transmission continues to be unclear, and for which there remains considerable debate in the scientific literature1,13. While experimental studies have shown that transmission through direct inoculation and water bug bites are both possible under lab-oratory conditions18,20,21,23,29, the relative importance of each of these transmission routes to human populations where BU is endemic is not established. Furthermore, the low number of cases in endemic areas hampers the development of cohort studies while the long incubation period and time to seek treatment of the disease obscure the results of case-control studies. This calls for innovative epidemiological study designs. In this study, we com-pare MU spatial and temporal dynamics in aquatic ecosystems and water bugs with the dynamics of BU cases to determine the potential contribution of each of these routes of transmission to observed disease patterns. Our two different, independent, approaches reinforce the conclusion that MU is primarily transmitted directly from the environment. None of the analyses suggest that BU is transmitted primarily by water bugs despite the common prevalence of MU found in these insects18. "
1691937,1.0," Environmental transmission – non-specific inoculation of MU from contaminated environments to humans–explains almost the entire observed temporal dynamics of BU incidence, whereas the contribution of the water bug transmission in our model is negligible. The fluctuations in environmental MU concentration over time explain the dynamics of BU cases in Akonolinga for the study period. However, the predictions from the best temporal fit seemed to reveal higher frequency variations in the number of cases over time than what was observed in the region (Fig. 2A). These differences are likely to be due to either seasonal changes in human exposure that are not taken into account in our model30 or to methodological differences in the estimation of the time-series for predicted and observed cases (Supplementary Materials, sections S2–S4). In any case, this disparity caused the model to over-predict by two cases in July and August and under-predict by two cases in November and December, and model residuals were normally distributed (Figure S10) suggesting random errors in the model predictions. Unfortunately, temporal fits for Akonolinga could not be tested in Bankim, as environmental data in this region was only collected every three months, which precludes the fitting of robust mathematical models. Future studies are thus needed to assess the generalizability of our temporal results to other endemic regions with diverse envi-ronmental conditions. "
1691937,2.0," The time from infection to treatment in our best temporal fits was consistently 6 months. This implies that ecological processes that favour growth of MU in aquatic environments and boost its concentration at specific times of the year9 might result in an increase in human BU infections, on average, six months later. Nevertheless, individual variability around average values for the incubation period and time to seek treatment could influence the dynamics of the reconstructed time series and the relationships identified in the temporal model. In the absence of individual data for each patient’s incubation period and time to seek treatment for our 10-year time series, a reasonable approach is the use of average estimates for parametrization of deterministic models. Moreover, data from 2012 showed that the time to seek treatment for BU in Akonolinga hospital was relatively low thanks to active case finding from Médecins Sans Frontières in the region (median = 5 weeks; interquartile range = 3–12 weeks)30. Future extensions of this modelling approach could benefit from such individual information for the whole time series and from the use of stochastic models to assess the impact of variability in individual parameter values on model outcomes. "
1691937,3.0," At the spatial level, we show that MU presence is associated with BU incidence even at the very local scale within endemic regions. Previous studies have linked MU positivity in the environment and water bugs with BU prevalence8,18, suggesting that MU positivity in water bugs can differentiate between endemic and non-endemic areas18, and that environmental MU positivity may predict BU prevalence spatially8. Other studies have explored the theoretical implications of these routes of transmission in mathematical models31. However, due to data lim-itations, no studies have considered the contribution of environmental pathways while controlling for potential water bug transmission or vice versa. Because we systematically collected samples over space and time from the whole aquatic community and from water bugs, we were able to study both transmission routes simultaneously. We show that in our study regions, the environmental presence and concentration of MU in a water body can better predict BU incidence in surrounding populations than any of the variables suggestive of water bug transmission. While MU concentration shows a clear linear relationship with BU incidence, the best models suggest a saturation effect for the link between MU positivity and BU incidence. Thus expansion of MU in the environment may not increase human infectious without simultaneous increases in concentration. "
1691937,4.0," We use the positivity and concentration of MU in the whole community of aquatic organisms as proxies for direct environmental transmission, as they represent two ways of measuring spatio-temporal changes in MU envi-ronmental load. Although a diversity of samples has been used in the past to estimate MU environmental load, i.e water8,32,33, soil or mud33–35, aquatic plants8,12,35 and aquatic organisms8,11,18,19,33; it seems clear that communities of aquatic organisms are able to acquire MU from these multiple environmental matrices and transmit it through ecological networks to other aquatic organisms13,36. Aquatic communities are composed of organisms with mul-tiple feeding strategies such as filter feeders that filter water, herbivores that consume aquatic plants, scavengers that eat detritus, and predators that consume other organisms, creating multiple pathways for infection when MU is present in any of those environmental matrices. We thus consider that estimation of MU prevalence and concentration in the community as a whole should provide a good representation of MU environmental load in these various matrices and can represent appropriate proxies for measuring the risk of human infection with MU contaminated environments. "
1691937,5.0," Even though our results are clearly in favour of a higher contribution of environmental MU transmission, this should be taken with caution. A sensitivity analysis performed in the mathematical model shows that if the median time to seek treatment of BU patients was higher than the 1–4 month range that we initially considered based on our data, the water bug transmission could play a role, contributing up to 20% of the infections at spe-cific times of the year in the best model fit (Supplementary Materials, section S7). Furthermore, estimates of MU prevalence in water bugs of the families Belostomatidae and Naucoridae are subject to some uncertainty, due to smaller sample sizes and the presence of other hemipteran families in some of the positive pools tested. Although most hemipteran families found in aquatic sites from endemic regions are capable of biting and have been found positive to MU11,18, their role in transmission is less studied. Similar ecological studies on MU transmission may consider overrepresentation of these two families when performing PCR testing, in order to decrease such uncer-tainty. Finally, other transmission routes such as aerosols and the role of mosquitoes as vectors of MU have been previously proposed and are not considered here37,38. "
1691937,6.0," The extremely low incidence characteristic of the disease even in endemic regions (on the order of 2 cases/1,000/year in high risk areas), limits our ability to study predictors of BU indicence, since stochastic processes have a strong impact in the spatio-temporal dynamics of rare diseases39. Recent analyses of long time-series of BU cases in French Guiana40 and Cameroon30 suggest seasonal patterns in BU incidence and reveal persistent spatial clusters of cases over time that are associated with specific environmental traits41. This is consistent with ecological studies on MU, where similar spatio-temporal patterns are observed for the dynamics of the mycobacterium11,42. We benefit from these novel insights and attempt to overcome the issue of stochasticity in rare diseases by comparing our one-year environmental dataset with BU incidence in human populations from the previous 10 years. For this, we aggregate the human monthly data for the whole region, in order to obtain robust estimates for the temporal patterns, and then we aggregate the entire time-series at the spatial level, in order to obtain robust estimates for the geographical distribution of cases in the region. While an ideal approach would be to associate, for each point in time and space, MU in the environment with the respective BU incidence using stochastic models, the amount of data necessary to have reliable and sufficient estimates from environmental samples is not available today. Nevertheless, the present study uses one of the most exhaustive environmental datasets available today for MU research and reveals new and important insights on this mysterious disease. "
1691937,7.0," The results presented here represent a first step towards integrative disease prevention and could largely improve early detection of BU cases. If the concentration of MU DNA in the aquatic environment can predict spatial and temporal risk of BU emergence at a very local scale, it may be possible to predict the risk of disease emergence through systematic environmental screening of MU. Such a strategy could be used in combination with epide-miological research to identify potential MU transmission hotspots prior to prospective studies, or to control for BU environmental hazard when evaluating behavioural or socio-economic factors in case-control studies. Unfortunately, due to the limited resources and poor infrastructure in BU endemic settings, it is very unlikely that the national public health systems could afford such a screening strategy for disease control, either technically or financially. Unless cheaper and easier tools for environmental screening of MU become available, prevention and early detection of BU cases is likely to benefit the most from a better understanding of the environmental drivers of MU presence and concentration9. Such insight could help identify environmental proxies of MU dynamics over space and time at local and regional scales, allowing the development of early warning systems which could improve disease control."
1982497,0.0," The most common causes of a slower blood flow in CCSVI patients are the obstructed or malfunctioning valves (Figure 3). In our opinion, the main purpose of MRV is not to visualize such a valve, but to demonstrate the influence of such pathological structures on flow characteristics. In addition, MRV can demonstrate anatomical abnormalities (such as hypoplasia or agenesis of a vein) that may make endovascular treatment of CCSVI challenging or even impossible. The most common CCSVI pathology, namely the pathologic flow in the IJV, can be also detected with color Doppler sonography [21]. However, pathologic outflows or anatomical abnormalities in AV or brachiocephalic vein cannot be diagnosed by means of color Doppler sonography. Hence, MRV remains the only reliable diagnostic tool in case of these veins (Figures 4, ​,5).5). Importantly, a standard MRV with gadolinium contrast enhancement, which was used in our center initially, was not found very useful in the diagnostics of CCSVI pathologies. Probably, the advanced MRV sequences, with quantitative measuring of the blood flow, will be even more useful than our protocols. However, for the time being, there are no such widely accepted and standardized MRV protocols. Our MRV protocol is simple and very efficient in diagnosing CCSVI abnormalities. Moreover, it could be implemented in the majority of MR scanners, produced after 2005. "
1982497,1.0," Similarly to the already published four grades of pathologic outflow diagnosed with catheter venography, we herewith propose 4 MRV grades of CCSVI: mildly decreased flow – demonstrated by a mildly increased signal on coronal T2FatSat images; moderately decreased flow – demonstrated by a moderately increased signal on coronal T2FatSat images, and by a slightly decreased signal on axial 2DTOF images; highly decreased flow – demonstrated by a significantly increased signal on coronal T2FatSat images and by a significantly decreased signal on axial 2DTOF images; severely decreased flow – demonstrated by an increased signal on coronal T2FatSat images and by an almost totally decreased IJV signal on axial 2DTOF images (see our images at: http://ccsvimri.blogspot.com/). "
429330,0.0," The chosen setting, Bankim district in the Adamaoua region, contrasts with the previous study site. Akonolinga, the previous study site, was characterized by a relative homogeneity: 80% of the population belonged to the same ethnic group, the region was largely forested and water-related activities were centered on a major river, the Nyong [3]. By contrast, Bankim was home to numerous ethnic groups with wider variation in environmental, cultural and health practices. Bankim’s environment is a highly varied mosaic of savanna, mountain, and forests. Water-related activities took place in the Mappe´ reservoir, the Mbam River and many other smaller water bodies (Figure S1). Our rigorous sampling method, which used census data and repeated appointments with controls, allowed us to capture accurately the heterogeneities of populations and activities in Bankim and to minimize control recruitment associated biases [19]. Case recruitment was performed within the entire district and was not restricted to the most accessible areas. Similar to case-control studies on BU, the present study has several limits inherent in its methodology and context. First, even if we are confident that our sampling method prevented most sampling biases, memory bias could have occurred: cases were interviewed on their activities during the period preceding the onset of disease and could therefore remember them differently from controls, who were interviewed on their activities over the previous year. However, we included mainly cases with disease diagnoses during the two previous years. This ensured that cases and controls would recall their activities on similar time scales and minimized differential bias. Furthermore, having suffered Buruli ulcer disease could have influenced cases’ answers to questions, such as the occurrence of scratch wound as discussed below (rumination bias, [19]). However, most of our identified risk or protective factors are related to less subjective behavioral parameters, which are less likely to be influenced by the disease status. Finally, this study was performed in a region that is distant from national or regional level healthcare facilities, and where the disease was only recently reported. But in 2007, because of the increasing number of cases, Bankim health personnel were trained at the national Buruli ulcer reference hospital in Ayos on case detection, systematic recording and basic management of cases according to WHO guidelines. These trained personnel imple- mented these practices in the district. During the period of the present laboratory case confirmation was set up although only a few cases were confirmed microbiologically. However, clinical diagnosis has been demon- strated to be highly specific when performed by trained health personnel, so that we are confident that almost all clinically- diagnosed Buruli cases were true cases [20]. In a previous study in study, a systematic procedure for Wound Care, Bed Nets and Buruli Ulcer in Cameroon Akonolinga, Cameroon, analysis of probable and confirmed cases yielded the same risk factors as confirmed cases only [3]. "
429330,1.0," Protection using a bed net. This the association between bed net use and protection against BU, previously shown in Akonolinga, in central Cameroon [3]. study confirms The previous study in Akonolinga had shown that the strong association between use of a bed net and protection against BU (OR [95%CI] = 0.4 [0.2–0.8] persisted between cases and controls within the same household (OR [95%CI] = 0.1 [0.320.03]), suggesting that it was not confounded by socio-economic status. A larger proportion of adults was included in the present study which made it more difficult to recruit siblings as familial controls. Statistical power is consequently low in the family-matched case- control study. However, use of a bed net still presents a protective effect in this sub-study, which, although not significant, has the same magnitude (OR [95%CI] = 0.4 [0.1–1.4]) as in the community-matched study (OR [95%CI] = 0.4 [0.2–0.9]). While socio-economical status might have confounded the association in the community-matched study, if for instance, participants using bed nets were less likely to have activities exposing them to M. ulcerans, it is unlikely to be the case in the family-matched study. Indeed, in this study, cases and their familial controls share the same socio-economical level. Here we demonstrate that the association between bed net use and protection against BU withstands generalization in a different climatic, geographic, environmental, and ethnic context. This replicative study greatly increases the positive predictive value of the association between bed net use and decreased risk of BU [15]. Few epidemiological studies have investigated the possible role of a bed net as a protective factor against BU in Africa. In Ghana, no significant association was found between bed net use and protection against BU, but the proportion of bed net users in the general population was low (25%), and the 116 pairs of cases and controls only yielded a power 67% to show an OR of 0.4 or less [6]. Furthermore, categories of bed net use grouped ‘‘sometimes’’ and ‘‘always’’ together, probably because of overall infrequent use. In our study, always using a bed net was protective, while sometimes using one was not. Out of two risk-factor studies in Benin, one did not address the question of bed net use [5], while the other showed a small risk increase for bed net users in univariate analysis which disappeared when adjusting for other variables in a multivariate model [2]. The preventive fraction for systematic bed net use was 32% in this study, and reached 43% in the previous study performed in Akonolinga [3]. If this association proves to be causal, bed net generalization could represent a significant mean of protection against the disease that could lead up to one third reduction in the number of cases. The protective effect of bed nets could result from two different mechanisms. First, using a bed net could protect from direct inoculation of M. ulcerans by an insect vector. This hypothesis is consistent with several results from environmental studies. In Australia, M. ulcerans infection is now considered a zoonosis transmitted from possum to human by mosquitoes [21,22]. Secondly, bed nets could provide a protection by preventing unspecific insect bites. These bites can lead to small lesions after being scratched which would enable M. ulcerans to enter the skin when in contact with contaminated environment. Occurrence of scratch wounds following insect bites was indeed associated with an increased risk of disease in our study. This factor was statistically independent of bed net use as shown by the persistence of both variables in the final model. This result suggests that both risk factors may act independently, some insect bites being not preventable by bed nets, for instance those occurring at day time. The association between bed net use and protection against BU underlines the need to broaden environmental studies to aerial insects and to the domestic environment. It would be relevant to assess the presence of M. ulcerans in Aedes and Anopheles mosquitoes which are important in Africa. Studies on distribution, abundance and M. ulcerans positivity of water bugs should not be restricted to the aquatic environment and seek these insects in the domestic environment. "
429330,2.0," Protection from adequate wound hygiene. The strongest protective association was linked to using soap to cleanse wounds. This variable summed up others that implied practices of proper wound care: use of alcohol and frequent bandage changing were also associated with protection, and usually concerned the same people. This association of a decreased risk of Buruli ulcer and good hygiene practices has been found in Ghana [6], Benin [2] and Cameroon [3], indicating that such practices might be one of the most powerful way of avoiding this disease. This also indicates that while some insects may transmit M. ulcerans, other transmission mechanisms are likely to coexist that involve wounds as a port of entry, such as contamination of open inoculation of M. ulcerans by a wounds by water, or direct wounding object. Several mechanisms could explain the decreased risk of disease associated with wound hygiene. Clean wounds heal faster and could be less likely to become a port of entry for the microbe. Frequent wound cleansing could also prevent the survival of the microbe in the skin: since the initial mycobacterial inoculum is expected to be low (3 months median incubation time, [23]) and since the growth rate is very slow, repeated disinfection of skin lesions, or frequent bodily cleansing could help to prevent microbe colonization and subsequent infection, independently of the actual transmission route. Onset of Buruli ulcer disease after sustaining a wound was reported by 9 cases out of 77 analyzed in this study. This result may well be subject to memory bias. Even so, the role of wounds in the transmission of M. ulcerans infection has received little, if any, attention. Symmetrically, the potential role of good hygienic practices has remained poorly investigated by the research community and remains absent from official recommendations, including the WHO BU factsheet [24]. Hygiene-related protective factors have been documented repeatedly and are thus highly relevant to public health. Using published data, we have estimated the preventive fraction for the use of toilet soap for bathing in previous studies and found was 41% in Ghana [6] and 65% in Benin [2]. The preventive fraction for using soap, antibiotic powder or health center care to treat injuries was 53% in Benin [2], while the preventive fraction for using alcohol to treat wounds was 26% in Cameroon [3]. In this study, the preventive fraction of soap use for cleansing wounds was 34%. While the role of insects in BU transmission remains the cause of much debate and methodological interrogations [25], clarifying the role of hygiene does not seem to have elicited much attention even though most studies confirm its importance. Good hygienic practices could potentially have a major role in community prevention programs. "
429330,3.0," Protection from growing cassava. Our final model also contains an unanticipated protective association with growing cassava. It had also been found in univariate analysis in Ghana [6]. The link between cassava and M. ulcerans infection is not obvious; we therefore suspect in both cases this factor could be associated with an environmental parameter that was not captured by our questionnaires. The main food crop in Bankim is maize, that Wound Care, Bed Nets and Buruli Ulcer in Cameroon which is highly water-demanding, and Bankim farmers report that cassava grows better in dry, higher grounds. We therefore hypothesize that people growing cassava may work in less swampy, thus less risky areas. Furthermore, bitter cassava is favoured by farmers because insects are less likely to attack it than sweet cassava. Bitter cassava contains toxic cyanoside compounds, and its preparation requires that the tubers be soaked in water, ground to flour and then sundried for several days. During this preparation process, the soaking and drying produce a very intense smell which may possibly repel insects from the domestic area where the preparation takes place. Of note are also the several univariate associations found between growing certain food crops and a decreased risk of disease. These associations suggest that nutritional status may possibly increase a person’s ability to resist M. ulcerans infection. Because this study is the first to show that this factor is significantly associated with a decreased risk of Buruli ulcer disease, we consider that further investigations on the agricultural and nutritional practices are needed before drawing conclusions about its possible role in prevention. "
429330,4.0," Bathing for hygiene in the Mbam river was associated with an increased risk of disease. These baths involved people spending long amounts of time near the river for their farming activities and eventually dwelling there during the rainy cultivation season where the waters are high and the banks swampy. Risk increase did not concern leisure baths in the same river, which only took place in the shallow waters during the dry season. This finding is in accordance with numerous previous identifying slow flowing rivers and stagnant waters as an important source of contamination. The identification of the Mbam river as a risk factor rather than the Mappe´ reservoir seems at first counter- intuitive, but the reservoir is probably not the direct source of every contamination: some patients had no history of contact with the Mappe´ lake waters and highest prevalences of BU were not recorded in the fishing settlements on the lake shore but in the villages located between the lake and the Mbam river (figure S1). Of note, M. ulcerans DNA positive water bugs were captured in domestic water collections as well as in the Mappe´ lake in 2008 [16]. The importance of the peri-domestic water collections as a source of M. ulcerans infections remains to be evaluated in terms of M. ulcerans presence throughout the year as well as potential vectors or reservoirs breeding sites. Captures targeting water bugs indicated important seasonal variations of abundance and positivity [8], which warrants regular follow-up of microbe and insect populations. The dam construction may have contributed to BU expansion suitable for proliferation, by providing a broader habitat dissemination and/or survival of M. ulcerans, one of its hosts, or an eventual vector. The dam construction in this region impacted drastically on endemicity of onchocerciasis and loasis [26] as well as schistosomiasis (Mouchet F, personal communication). Increase in BU disease may also result from increased exposure to the environment surrounding the Mbam River where relocated villagers exploit new fields after the dam construction. The second risk factor identified in this study is more difficult to interpret. Reporting occurrence of scratch wounds after being bitten by an insect was found associated with Buruli ulcer disease. This could be seen as an indication that insect bites are involved in the transmission, either through an insect vector or through unspecific skin lesions from scratching representing a port of entry for M. ulcerans. We would however recommend considering this factor with great caution, as it was only reported and actual sensitivity to insect bites was not assessed. Furthermore, BU cases often reported having suffered intense itching before the onset of the lesion, which they often attributed to an insect bite. This event may have led cases to report being more sensitive to insect bites than controls. Finally, several risk factors commonly evoked for BU did not present statistically significant associations with the disease in this study. Statistical power was probably insufficient to investigate properly several associations, such as education level (very low) and short clothing related associations (infrequently used for farm work or fishing). Bankim environment’s heterogeneity probably prevented us from identifying major water sources of exposure: there is a large number of water bodies, and people change both water-related activities and locations throughout the year because these water sources dry up during the dry season. This may explain why in this study, four behavioral factors and only one location-dependent factor had statistically significant associations with BU in the final model."
193334,0.0," Our results confirmed our first hypothesis that the patients with TGA having IJV stenosis/compression at various segments would exhibit significantly lower total FVs in the bilateral IJVs and VVs resulting in important consequences than that exhibited by the control subjects. More importantly, our findings are consistent with our second hypothesis that the prevalence of right IJV non-patency at various segments during the VM would be significantly higher in the study subjects with IJV stenosis/compression at various segments; and therefore, would be higher in the TGA patients than that in the controls, which supports our novel hypothesis of venous pathogenesis involved in the TGA attacks (16). Specifically, an insufficient IJV patency prevents the release of increased intracranial pressure and venous congestion/hypertension in the basilar plexus and cavernous sinus caused by VM-like maneuvers. Moreover, venous stasis and occlusion may cause constriction of cerebral arterioles (16), which further compromises cerebral hemodynamics. "
193334,1.0," Venous Flow Velocity and FV in IJVs This study, our previous study (20), and other studies (22, 23) all revealed that the IJV or BCV compression and stenosis significantly reduce the IJV FVs. These result in the venous drainage being routed through less efficient alternative routes, such as the tortuous path through the spinovertebral venous plexus. This obstruction of the venous drainage may also induce changes in the arterial blood flow, such as arterial constriction, through the venoarterial reflex (25). Such changes may explain the observation in our previous studies (21, 24) that, the patients with transient monocular blindness without carotid stenosis exhibited increased downstream resistance of the retrobulbar arteries (i.e., ophthalmic artery, posterior ciliary artery, and central artery) in association with significantly increased prevalence of compression/stenosis in the bilateral IJVs (26). In this study, we did not measure the venous FV in the spinovertebral venous plexus, because it is undetectable by the ultrasound; therefore, we do not know the changes in the total venous drainage from the bilateral TSs, though the patients exhibited significantly lower total FV in the bilateral IJVs and VVs. "
193334,2.0," Prevalence of IJV Non-patency During the VM Our results supported our second hypothesis that the prevalence of IJV non-patency during the VM would be higher in the TGA patients than that in the controls. We observed a significant difference in the prevalence of right IJV non-patency (patients: 32.1%; controls: 11.6%; p = 0.0128) as compared to that for the left IJV non- patency (patients: 49.4%; controls: 37.2%). An abundance of evidence indicates that VM-induced pressure from the chest and abdomen is mainly transmitted to the intracranial compartment via the epidural venous plexus or vertebral venous plexus. Orthograde IJV outflows emerging shortly after the beginning of the VM, thus, serve as a mechanism for regulating the intracranial pressure and equalizing the pressure within the venous system (10, 11, 14, 16). Our findings indicate that the patients with TGA may experience defective intracranial pressure regulation during the VM-like movements. As described earlier, we restricted our definition of IJV non-patency to complete absence of the IJV drainage at the J3 or J2 segment during the first 10 s of the VM; but, this consideration regarding the IJV drainage might be unexhaustive. Several patients with partial or limited IJV drainage were excluded from this definition, which may explain why we observed a lower prevalence of IJV non-patency during the VM. Further research is needed to develop a more sensitive and specific definition of IJV non-patency during the VM. "
193334,3.0," Incompetence of Jugular Venous Valves Cerebral venous congestion/hypertension, which results from the venous reflux during the VM consequent to the IJVVI, is linked to TGA (4–6). However, previous ultrasound studies using either retrograde flow (27) or air bubbles (28) during the VM explained the involvement of only the proximal region of the IJV in the IJVVI, ignoring the rest of the IJV and the entire BCV, and possibly missing other important IJV/BCV abnormalities. Unsurprisingly, previous ultrasound and non-contrast MRA results have not supported a causal relationship between the IJVVI and TGA (7–9). Other than the IJVVI, we have previously described three ultrasound patterns of IJV abnormalities in the patients with TGA: (i) an isolated reverse flow in the left jugular vein branch (JB), (ii) a segmental reverse flow in the left distal IJV, and (iii) a continuous reverse flow in the left IJV and JB (29). All the three of these IJV patterns are suggestive of the venous outflow obstruction or compression/stenosis of the left BCV (29). Similar to other studies (4–6), even in this study, the overall prevalence of IJVVI was higher in the TGA patients than that in the controls (Table 2); but, the prevalence was higher in the left IJV only, and not in the right IJV. Furthermore, we found that the prevalence of IJVVI was significantly higher in the individuals with left BCV compression/stenosis. This suggests that the IJVVI might occur secondary to the BCV compression/stenosis on the left side. The fact that the IJVVI was more frequently observed on the right side, as reported in other studies (28), raises the question of whether the jugular venous valves are vulnerable in cases of IJV compression/stenosis due to pressure imbalances across the valves or whether the previous air bubble methodology overestimated the IJVVI prevalence (30). Further research is needed to address this question. "
193334,4.0," Assessments of IJV Compression With Different Imaging Modalities It is worth mentioning that despite the patients exhibiting reduced flow velocities and FVs in each of the segments of the bilateral IJVs, the diameters of their IJV segments were comparable to that of the controls (Table 2). Moreover, the patients' right IJVs were slightly wider than that of the controls. This may be partially explained by the Bernoulli's equation, which states that the pressure in the venous lumen is inversely proportional to the flow rate, so that the lumen diameter may be enlarged with flow stasis resulting from the venous stenosis/compression. As described earlier, the FV of the IJV decreases in cases of IJV or BCV stenosis/compression (20, 22, 23). However, discrepancies in the diagnosis of IJV compression/stenosis may be observed due to different types of venous examination (31). Catheter venography has been traditionally regarded as the gold standard for diagnosing venous disorders involving compression/stenosis, but it does not measure the flow velocity or FV; hence, it cannot prove or disprove the MRI or ultrasound findings that indicate decreased IJV FV resulting from compression/stenosis. "
193334,5.0," Study Limitations This study has several limitations, particularly regarding the use of ultrasound to study the IJV. First, there are no ideal, fixed locations for measuring the CSA and TAMV in the J2 and J3 segments; and these measurements may vary if the IJVs are non-uniform in diameter, which may occur in the cases where the IJV is affected by segmental dilatation, narrowing, or compression. However, the FV was calculated by multiplying the TAMV by the CSA, and it was theoretically correct according to the Bernoulli's equation. We measured the CSA and TAMV at the widest available lumen of the J2 and J3 segments to minimize the bias. This allowed us to detect the differences in both the FVs and TAMVs between the patients and controls. Second, it is unexhaustive to consider complete absence of IJV drainage during the VM while explaining IJV non-patency. Several of our subjects exhibited limited or intermittent flow, which suggests partial IJV blockage. However, we excluded these subjects from our definition of IJV non-patency, which may have caused us to underestimate the prevalence of IJV non-patency during the VM. Third, left BCV blockage may disappear during deep inspiration (15), which usually precedes the VM; hence, paradoxical reopening of the left BCV and left IJV occurs during the VM. This causes further underestimation of the prevalence of non-patency."
432326,0.0," Due to the limitations of conventional typing methods for the differentiation of strains belong- ing to the highly monomorphic African M. ulcerans population, use of WGS was suggested to reach sufficient analytical depth for molecular epidemiology studies [32]. Here our comparative genomic analysis of strains from two geographically separated BU endemic areas of Cameroon, the Mapé and the Nyong river basins, identified two phylogenetically distinct lineages of M. ulcerans. These data support previous findings that the spread of local clonal lin- eages between endemic areas only rarely occurs [13,14]. In a previous IS element—SNP based typing study most strains from the central region of Cameroon had the same SNP types as strains from neighbouring Gabon. The IS element—SNP type found in a strain from the Mapé river basin was also present across entire Central and West-Africa leading to the hypothesis that this lineage represents the founder of the other observed IS element—SNP types [15]. Our WGS analysis showed that the strains from the Mapé river basin are in fact more closely related to the M. ulcerans strain circulating in Ghana and Benin than the strains belonging to the Nyong river basin lineage. Additional WGS data with strains from all BU endemic African countries are required to shed more light on the spread and evolution of M. ulcerans in Africa and the origin of the locally observed two distinct Cameroonian lineages. "
432326,1.0," Analysis of the pairwise SNPs distance and nucleotide diversity distribution revealed a lower genetic diversity among the Mapé river basin strains than among the Nyong river basin strains. Epidemiological data suggest that M. ulcerans has expanded in the Mapé river area more recently than in the Nyong river basin. Descriptions of BU cases in the Nyong river basin exist since 1969 [17]. In contrast, clinically suspected cases of BU in the Mapé river area have been reported first only in 2004 [33]. While the disease may have preexisted there, epidemio- logical data strongly indicate that BU incidence has recently increased in the Mapé river basin [16,33]. Recent expansion of a clone may thus explain the more limited genetic diversity of the M. ulcerans lineage present in the Mapé river basin. It was speculated, that this expansion was associated with environmental changes caused by the damming of the Mapé river in 1989 [16,33]. Although it has been shown that cases associate more with the Mbam river as opposed to the Mapé dam directly [11,16], damming may have had an indirect effect on groundwater level and stagnant water bodies in the area. "
432326,2.0," We have compared the genome sequences of strains isolated at different time points over the course of the BU infection of three patients. In only one case, we detected a single SNP in one of the isolates compared to the strain isolated earlier from the same patient. It is not possible to conclude whether this observed single polymorphism is related to a re-infection by a variant strain or to a point mutation that occurred either in the patient or during the in vitro cultivation. However, these data support the expectation of a low mutation rate in M. ulcerans. "
432326,3.0," Our analysis shows that WGS is an important tool for studying the local diversity and popu- lation structure of M. ulcerans in endemic areas and for resolving the evolutionary history of the pathogen. A combination of phylogenetic analysis with geographical information on the patient’s home at the time of disease onset did not reveal a clear distribution pattern of the ge- netic variants. This may in part be related to the limited resolution of the comparative genomic analysis performed here. Resolution of the WGS typing could be further increased by inclusion of repetitive regions of the genome and the virulence plasmid that we so far excluded from the analysis, such as the IS2404 and PE/PPE regions. On the other hand, our sero-epidemiological analyses have provided evidence that exposure to M. ulcerans does not primarily occur at the homes of patients [34], but may rather be associated with more peripheral environmental water contact sites. Furthermore, for patients from the Mapé river basin it has been found that many of them move over long distances (in some cases >15 km) from their homes towards the Mbam river for their farming activities [11,16]. For genomic epidemiology studies it may there- fore be necessary to establish detailed individual movement and environmental water contact patterns to follow the spatial-temporal spread of genetic variants."
2547425,0.0," In this case-control study, involving four types of MS patients with matched healthy controls, we found no evidence to support the hypothesis that CCSVI is associated with multiple sclerosis. Furthermore, given the relatively high age of the patient population in our study the data substantiate the view [30]–[32] that CCSVI is not a late secondary phenomenon of MS and is not associated with disability. Using Doppler ultrasound only one subject (with primary progressive MS) fulfilled the minimum criteria established by Zamboni et al [5], [9] for having CCSVI. However, at this juncture it is important to note that the “Zamboni criteria” as well as the proposed cut-offs are arbitrary in number and essentially an unproven hypothesis [33]. MRI quantitative flow assessment of extra- and intra-cranial venous blood flow revealed no clinically significant differences between the MS subjects or healthy matched controls. We did not find any evidence of CCSVI in any of the healthy subjects. Thus the data from this study do not support the existence of CCSVI. "
2547425,1.0," Our data are consistent with several recent studies [12], [14], [17], [21], [24], [30], [32], [33]which found no evidence for CCSVI in MS patients using ultrasonographic techniques. The results from these studies are in marked contrast to those reported by Zamboni et al [5]–[7] who found that all MS patients exhibited CCSVI. In the ultrasound study by Zivadinov et al [18] the prevalence of CCSVI in MS subjects was 56%, an intermediate between the results reported by Zamboni and the results reported here and others [14], [17], [21], [24], [30], [32]. However, in the study by Zivadinov et al [18] 23% of the healthy control group were also reported to have CCSVI. "
2547425,2.0," MRI techniques were not originally used to define CCSVI, but when they have been employed to examine extra-cranial venous flow in MS patients [13], [15], [16], [21] the data are not supportive of Zamboni's hypothesis. The results from our own MRI assessments are consistent with these other studies in that intra- and extra-cranial venous flow was not significantly different between cases and controls. The observation that right IJV flow was consistently higher than that in the left IJV is in accord with previously published data in both healthy [34] and MS subjects [21]. When one examines this flow asymmetry there is no significant difference in the differential between right minus left IJV flows for cases versus controls. Finally, on radiological examination of the contrast enhanced MRVs we found no evidence of structural abnormalities in the venous architecture of the extra-cranial vessels between cases and controls which is consistent with previous findings [12], [21], [35]. Collectively, these data serve to underscore the absence of any significant cerebrospinal venous abnormalities in MS subjects. "
2547425,3.0," It is well recognized that ultrasonography is an operator-dependent technique and in studies utilizing ultrasound to evaluate CCSVI there are significant differences between the methodologies used and protocols described [12], [19], [20], [36]. In acknowledging this potential limitation, our vascular ultrasonographers and an ultrasound radiologist spent time with Zamboni's group in Ferrara, Italy to observe the technique that was used to define CCSVI. "
2547425,4.0," Ultrasound is also subject to anatomical limitations. Intrathoracic vessels are obscured by bony anatomy and are not accessible. The deep cerebral veins that are accessible have limited fields of view. Despite this the sonographers were able to consistently demonstrate a DCV using the transtemporal approach. Furthermore, the extracranial veins respond to respiration, posture changes and breathing artifacts which demonstrate variable flow patterns making hemodynamic assessment limited and the evaluation of stenosis difficult. We are however confident in our ultrasound results since the MRI determinations that were performed on the same day as the ultrasonography are in accord that there were no significant differences in venous flow between the MS patients and their matched controls. "
2547425,5.0," It thus appears that operator-induced venous abnormalities may account for the prior observation of CCSVI amongst patients with MS being investigated with ultrasound. We believe that the positive ultrasound findings in some previous studies [5]–[8], [18] reflect low specificity with the procedure, likely the product of operator-induced artifact. This would explain the almost bimodal heterogeneity for observing CCSVI demonstrated in a recent meta-analysis [19]. It is also unlikely that there was operator-related poor sensitivity for detecting CCSVI in the study that we conducted given that the MRI measures of flow in the study correlate well with ultrasound measurements of flow that were obtained in the same patient. "
2547425,6.0," Another limitation of the study was our inability to blind the ultrasonographers regarding whether a subject was a MS patient or a control (e.g. MS patients may have required some assistance with mobility). In studies such as this one, an unblinding bias would be expected to increase the chance of observing an association. Given that no association was observed the lack of blinding appears to have been inconsequential. The radiologists reading either the ultrasound or MRI images were blinded to whether the participant was a MS patient or control subject, as well as to the results of the other imaging test. While viewing the MRI might reveal that a subject has MS (e.g. plaques being present) the fact that no association was observed between MRI flow abnormalities and MS patients in the study suggests that unblinding was not a problem. "
232756,0.0,"We introduced a new comparative approach to observation learning of a discrimination task. We quantified task performance in terms of learning speed and ability to generalize, analogous to studies on observational learning of motor tasks, in which performance is quantified in terms of reaction times and generalization across motor effectors23,24."
232756,1.0," We found that zebra finches can learn to discriminatively respond to auditory stimuli by observing expert performers. Experimenter and observers� behaviors were subject to a tradeoff that depended on whether the learning cue was experienced or observed. We inferred this cue dependence thanks to our experiment design in which the stream of auditory stimuli was identical for experimenters and observers. Therefore, any differences in their abilities to learn and to generalize must have been entirely due to the learning cue, which was an aversive air-puff for experimenters and an observable action for observers. Our findings suggest that an experienced cue favors robust generalization, whereas an observed cue favors rapid learning."
232756,2.0," Part of our findings are in line with social learning theories which suggest that to learn from others is a successful strategy with high payoff under a wide range of conditions25,26. However, our findings also suggest a limitation to the ubiquitous success of social learning strategies. Namely, we find that social learning can lack robustness when environmental conditions even slightly change. As in the case of children who perform poorly in exams after neglecting their homework, insights gained through observation seem not to transfer well to new task instances."
232756,3.0," Currently, there is no reason to think that all forms of observation learning will be subject to lack of robustness. For example, it is not clear that male zebra finches would exhibit similar behaviors given the known sex differences in social learning27 also in airpuff paradigms28. Furthermore, it is not clear whether our findings will generalize to other reinforcers including reward and strong punishment (e.g. by electric shock). It is even uncertain whether to be observed played a role for experimenters� robust learning. In the light of all these possibilities, our work raises the question as to whether there exist some forms of observation learning that promote robust transfer to new task instances."
232756,4.0," Our work raises many interesting questions on the behavioral and neurobiological mechanisms used by observers to acquire stimulus-discriminative information. Behaviorally, observers could learn through social mechanisms of action imitation, of observational conditioning, and of stimulus enhancement, or a combination of these. Note that the definitions of these mechanisms are not strict enough to allow a discrete categorization of social learning in any one study29. Our findings de-emphasize some known social learning mechanisms such as perceptual learning (evidenced by PL learners) and simple stimulus enhancement (evidenced by lack of discriminative behavior during pre-testing, Supplementary Figure 3). Our experiments also de-emphasize vocal communication as a mechanism but reveal the importance of vision (-TCOM learners). Overall, the importance of a demonstrating expert suggests that experimenters signal statistical differences between puffed and unpuffed stimuli via their perching behavior such as their rates of leaving the perch. Possibly, observers focused their attention more on the diverse�actions of experimenters and their relationships with the stimuli, which is why observers apparently failed to identify the simplest environmental signal that can explain experimenters� behavior, which in our case was syllable duration."
232756,5.0," Similar speed-robustness learning tradeoffs as the one we find exist in rapidly evolving artificial systems, in which high discrimination performance tends to be associated with slow learning as an unwanted side effect30. The tradeoff we find between robustness in one learning paradigm and speed in another is most closely paralleled by regularization methods that control inference through synaptic weight subtraction. Excellent generalization of experimenters agrees with strongly regularized classifiers whereas fast learning in observers agrees with weakly regularized classifiers. Our work suggests that the benefits of regularization may be inherent to experimenting but not to observing31."
232756,6.0," It is far from clear how a brain could implement dynamic regularization. Our speculative proposal is that the balance between learning and regularizing is controlled by a neuromodulatory signal. Such signals are ubiquitous in the animal kingdom and are well suited to convey the amount of regularization, given that they respond sensitively to external reinforcements and their prediction errors32�36. One possibility is that air-puff reinforcers drive changes in regularization via experimenters� escape actions, which is supported by the representation of action-specific reward values in brain areas innervated by neuromodulatory neurons37. This proposal delineates a possible neural system for comparative studies of learning from experience and from observation. It has been shown that reward prediction error and reinforcement learning algorithms in general, may be utilized by humans in order to understand the social value of others� behavior38,39, to feel vicarious rewards from their success or failure40 or from their approval41. We believe that the computational role of reward prediction error can be extended to that of regularization of learning, mediated by neuromodulator systems such as acetylcholine�or dopamine. Furthermore, subtractive weight depression through heterosynaptic competition has been observed in the amygdala31, which provides biological plausibility to L1 regularisation in the brain. We hypothesize that some form of synaptic depression is seen in zebra finches when they are experimenting, but not�when they are observing."
232756,7.0," The speculative implications of our simulations are that a prerequisite for the evolution of observation learning was a sufficiently large brain capacity that provided rich sensory representations and put few constraints on usable neural resources for sensory processing. Evolution might have chosen traits in observers that are complementary to those associated with experimenting, explaining the apparent differences in what these learning strategies extract from the sensory environment."
1789181,0.0,"In this paper, we propose a biologically plausible SOM-SNN framework for automatic sound classification. This framework integrates the auditory front-end, feature representation learning and temporal classification in a unified framework. Biological plausibility is a key consideration in the design of our framework, which distinguishes it from many other machine learning frameworks."
1789181,1.0," The SOM-SNN framework is organized in a modular manner, whereby acoustic signals are pre-processed using a biologically plausible auditory front-end, the mel-scaled filter bank, for frequency content analysis. This framework emulates the functionality of the human cochlea and the non-linearity of human perception of sound (Bear et al., 2016). Although it is still not clear how information is represented and processed in the auditory cortex, it has been shown that certain neural populations in the cochlear nuclei and primary auditory cortex are organized in a tonotopic fashion (Pantev et al., 1995; Bilecen et al., 1998). Motivated by this, the biologically plausible SOM is used for the feature extraction and representation of mel-scaled filter bank outputs. The selectivity of neurons in the SOM emerges from unsupervised training and organizes in a tonotopic fashion, whereby adjacent neurons share similar weight vectors. The SOM effectively improves pattern separation, whereby each sound frame originally represented by a 20-dimensional vector (mel-scaled filter bank output coefficients) is translated into a single output spike. The resulting BMU activation sequences are shown to have the property of low intra-class variability and high inter-class variability. Consequently, the SOM provides an effective and sparse representation of acoustic signals as observed in the auditory cortex (Hromádka et al., 2008). Additionally, the feature representation of the SOM was shown to be useful inputs for RNN and LSTM classifiers in our experiments."
1789181,2.0," Although the SOM is biologically inspired by cortical maps in the human brain, it lacks certain characteristics of the biological neuron, such as spiking output and access to only local information. Other studies (Rumbell et al., 2014; Hazan et al., 2018) have shed light on the feasibility of using spiking neurons and spike-timing dependent plasticity (STDP) learning rule (Song et al., 2000) to model the SOM. We would investigate how we may integrate the spiking-SOM and the SNN classifier for classification tasks in the future."
1789181,3.0," Acoustic signals exhibit large variations not only in their frequency contents but also in temporal structures. State-of-the-art machine learning based ASC systems model the temporal transition explicitly, using the HMM, RNN or LSTM, while our work focuses on building a biologically plausible temporal classifier based on the SNN. For efficient training, we use supervised temporal learning rules, namely the membrane-potential based Maximum-Margin Tempotron and spike-timing based ReSuMe. The Maximum-Margin Tempotron (combining the Tempotron rule with the maximum-margin classifier) ensures a better separation between the positive and negative classes, improving classification accuracy in our experiments. As demonstrated in our experiments, the SOM-SNN framework achieves comparable classification results on both the RWCP and TIDIGITS datasets against other deep learning and SNN-based models."
1789181,4.0," We further discover that the SNN-based classifier has an early decision making capability: making a classification decision when only part of the input is presented. In our experiments, the SNN-based classifier achieves an accuracy of 95.1%, significantly higher than those of the RNN and LSTM (25.7% and 69.2% respectively) when only 50% of the input pattern is presented. This early decision making capability can be further exploited in noisy environments, as exemplified by the cocktail party problem (Haykin and Chen, 2005). The SNN-based classifier can potentially identify discriminative temporal features and classify accordingly from a time snippet of the acoustic signals that are less distorted, which is desirable for an environment with fluctuating noise."
1789181,5.0," Environmental noise poses a significant challenge to the robustness of any sound classification systems: the accuracy of many such systems degrade rapidly with an increasing amount of noise as shown in our experiments. Multi-condition training, whereby the model is trained with noise-corrupted sound samples, is shown to overcome this challenge effectively. In contrast to the DNN and SVM classifiers (McLoughlin et al., 2015), there is no trade-off in performance for clean sounds in the SOM-SNN framework with multi-condition training; probably because the classification decision is made based on local temporal patterns. Additionally, noise is also known to exist in the central nervous system (Schneidman, 2001; van Rossum et al., 2003) which can be simulated by spike jittering and deletion. Notably, the SOM-SNN framework is shown to be highly robust to such noises introduced to spike inputs arriving at the SNN classifier."
1789181,6.0," The SNN classifier makes a decision based on a single local discriminative feature which often only lasts for a fraction of the pattern duration, as a direct consequence of the Maximum-Margin Tempotron learning rule. We expect improved accuracy when more such local features within a single spike pattern are utilized for classification, which may be learned using the multi-spike Tempotron (Gütig, 2016; Yu et al., 2018). The accuracy of the SOM-SNN model trained with the ReSuMe learning rule may also be improved by using multiple spike times. However, defining these desired spike times is a challenge exacerbated by increasing intra-class variability. Although the existing single-layer SNN classifier has achieved promising results on both benchmark datasets, it is not clear how the proposed framework may scale for more challenging datasets. Recently, there is progress made in training multi-layer SNNs (Lee et al., 2016; Neftci et al., 2017; Wu et al., 2018b), which could significantly increase model capacity and classification accuracy. For future work, we would investigate how to incorporate these multi-spike and multi-layer SNN classifiers into our framework for more challenging large-vocabulary speech recognition tasks."
1789181,7.0," For real-life applications such as audio surveillance, we may add inhibitory connections between output neurons to reset all neurons once the decision has been made (i.e., a winner-takes-all mechanism). This allows output neurons to compete once again and spike upon receipt of a new local discriminative spike pattern. The firing history of all output neurons can then be analyzed so as to understand the audio scene."
1789181,8.0," The computational cost and memory bandwidth requirements of our framework would be the key concerns in a neuromorphic hardware implementation. As the proposed framework is organized in a pipelined manner, the computational cost could be analyzed independently for the auditory front-end, SOM and SNN classifier. For the auditory front-end, our implementation is similar to that of the MFCC. As evaluated in Anumula et al. (2018), the MFCC implementation is computationally more costly compared to the spike trains generated directly from the neuromorphic cochlea sensor. Our recent work (Pan et al., 2018) proposes a novel time-domain frequency filtering scheme which addresses the cost issue in MFCC implementation. We expect the SOM to be the main computational bottleneck of the proposed framework. For each sound frame, the calculation of the Euclidean distance of synaptic weights from the input vector is done for each SOM neuron. Additionally, the distances are required to be sorted so as to determine the best-matching units. However, this computational bottleneck can be addressed with the spiking-SOM implementation (Rumbell et al., 2014; Hazan et al., 2018), whereby the winner neuron spikes the earliest and inhibits all other neurons from firing (i.e., a winner-takes-all mechanism) and hence by construction, the BMU. The spiking-SOM also facilitates the implementation of the whole framework on a neuromorphic hardware. In tandem with the SNN classifier, a fully SNN-based framework when implemented would translate to significant power saving."
1789181,9.0," As for memory bandwidth requirements, the synaptic weight matrices connecting the auditory front-end with the SOM and the SOM with the SNN classifier are the two major components for memory storage and retrieval. For the synaptic connections between the auditory front-end and the SOM, the memory bandwidth increases quadratically with the product of the number of neurons in the SOM and the dimensionality of the filter banks. Since the number of output neurons is equal to the total number of classes and hence fixed, the memory bandwidth only increases linearly with the number of neurons in the SOM. Therefore, the number of neurons in the SOM should be carefully designed for a particular application considering the trade-off between classification accuracy and hardware efficiency."
271920,0.0,"In the current study, we compared bilinguals' and monolinguals' performance during a nonlinguistic learning task that involved mapping tones to symbols within a novel symbolic system that consisted of three distinctive features (timbre, pitch, and duration). Both learning and subsequent processing were examined in bilinguals vs. monolinguals. Subtle differences were evident across the two groups, particularly in the processing domain. These findings suggest that, even when bilingual advantages are not present, bilinguals may differ from monolinguals on tasks that resemble lexical mapping and involve competition resolution."
271920,1.0,"Acquisition of novel tone-to-symbol mappings in bilinguals vs. monolinguals \n  On the sound-to-symbol matching task, bilinguals did not show a learning advantage. These results are consistent with previous findings that bilingual learning advantages may be determined by how the novel information relates conceptually to the previously established language systems. For example, Kaushanskaya and Rechtzigel (2012) suggest that learning of a novel word that is tied to a concrete (vs. abstract) translation equivalent will more widely activate bilinguals' previous two languages and may thus yield a more facilitative context for learning and integration of new knowledge. In contrast to Kaushanskaya and Rechtzigel's concrete learning condition, the current study required participants to map a new symbolic system in the absence of relevant previous conceptual representations. In this sense, the current study may be likened to an extreme version of Kaushanskaya and Rechtzigel's abstract word learning condition, and is consistent with the prediction that bilinguals may only outperform monolingual learners if their previous knowledge can be directly employed to scaffold new learning. \n  Perhaps because our task does not relate to bilinguals' previous language knowledge, the current findings stand in contrast with a previous nonverbal learning task where bilinguals showed advantages in learning a Morse code system (Bartolotti et al., 2011). One possible explanation for the absence in auditory processing advantages in the present study is that the current bilinguals had no previous language-based experience with the pitch, timbre, and duration dimensions in the current study, and in fact performed equivalently to monolinguals when learning these dimensions. Indeed, previous research has suggested that bilingual experience may reconfigure attention to linguistic and extralinguistic cues in the environment based on their relevance in previous learning experiences (e.g., Deutsch et al., 2004; Bialystok et al., 2005; Brojde et al., 2012). While natural language processing relies on listeners' capacity to make distinctions between pitch, timbre (i.e., formant structure) and duration, it is possible that the nature and constellation of these dimensions was too far removed from English-Spanish processing to allow for transfer of skills. For example, it is possible that Spanish-English bilinguals have some previous training on the fine-grained duration dimension that was critical in Bartolotti et al.'s learning task, given their awareness of temporal phonetic characteristics across their two languages, such as voice onset time discrimination in English vs. Spanish (e.g., Ju and Luce, 2004). However, the challenging combination of pitch, timbre, and duration dimensions may have differed from their previous linguistic experiences and clouded potential subtle advantages (e.g., Krumanshl and Iverson, 1992). \n  It is also possible that subtle cognitive differences between bilinguals and monolinguals contributed to advantages on Bartolotti et al.'s learning task but not the current learning task. For example, bilingualism has previously been associated with advantages in auditory working memory (e.g., Adesope et al., 2010), and such advantages in working memory may in turn be linked to statistical learning success (Misyak and Christiansen, 2012). While Bartolotti et al. did not include an auditory working memory task (only a forward digit span task was included), bilinguals in the current study did not differ from their monolingual peers on an auditory backward digit span measure. Yet, auditory working memory skills may in part account for bilinguals who were not successful learners. Specifically, across all linguistic and cognitive measures, follow-up analyses did not yield significant differences between unsuccessful (<65% accuracy) and successful (>65% accuracy) bilingual learners, with only a marginal difference in numbers reversed present between the two groups (p = 0.076). While no significant correlation was present between post-training accuracy and numbers reversed in the bilingual non-learners (r = 0.33, p > 0.1), this working memory difference nevertheless may have contributed to learning outcomes. In fact, a positive correlation between overall accuracy and numbers reversed skills was present across all bilingual learners and non-learners (r = 0.4, p = 0.01). It is thus possible that reliance on working memory within the bilingual group was in part responsible for the larger proportion of weaker bilingual learners. Working memory has previously been linked to learning outcomes (e.g., Papagno and Vallar, 1995), and lower working memory in the current bilingual sample was also associated with lower PPVT scores (r = 0.4, p < 0.05, for similar findings, see Kaushanskaya et al., 2011). No such correlations were found in monolinguals (working memory and training outcomes: r = 0.04, p > 0.5; working memory and vocabulary: r = 0.17, p > 0.4). \n  Beyond auditory working memory, several factors may account for the low tone-to-symbol mapping accuracies in the bilingual non-learners. In fact, combined learning of novel auditory and visual information was challenging for most participants (see post-training accuracies). For example, the length of both the training and processing phases required sustained attention and thus motivation to perform well. It may therefore be that sustained attention skills in general differentiated learners from non-learners, a possibility that can be explored in future research. Interestingly, while Spanish skills and language immersion were not significantly related to learning outcomes in the successful bilingual learners, in the bilingual non-learners higher Spanish skills and less English exposure were related to more successful learning (TVIP: r = 0.68, p < 0.05; English exposure: r = ?0.83, p < 0.01). Of the non-learners, 9 were Spanish-English bilinguals, one was a simultaneous bilingual, and one was an early English-Spanish bilingual. Thus, in the bilinguals who struggled to learn, skill in the native language appeared to support tone-to-symbol mapping. The reason for this observed link between L1 vocabulary and learning performance in monolinguals and bilingual non-learners may be that underlying cognitive strengths facilitate both vocabulary acquisition and better task performance. It is possible that L1 vocabulary is a particularly strong indicator of underlying word learning skills while L2 vocabulary may be more context-specific and thus a reflector of experience more than word-learning skills per se. In sum, the current findings contribute to limiting the scope of bilingual learning advantages. Further they raise new questions on the nature of bilingual learning advantages, as well as pre-requisite cognitive-linguistic skills, perhaps suggesting that aspects of bilingualism may provide richer opportunities for scaffolding during specific new learning contexts, but that bilingual experience may not modulate fundamental learning mechanisms."
271920,2.0," Sound-to-symbol processing differences between bilinguals and monolinguals \n  Studying processing of newly-learned nonlinguistic information can eliminate group differences in content knowledge associated with bilingual status, thus providing an opportunity to compare competition resolution across groups in the absence of proficiency effects. Current findings across bilinguals and monolinguals that were equivalent on learning outcomes suggest subtle processing differences between the two groups. These differences emerged only when we examined the relation between sound-to-symbol retrieval and previous vocabulary knowledge and when the time course of inhibitory control was considered. \n  During the processing task, the bilinguals and monolinguals, who had attained similar skill levels with the new symbol system, also showed similar symbol retrieval efficiency. Moreover, consistent with previous explanations of bilingual retrieval disadvantages (e.g., Ivanova and Costa, 2008; Gollan et al., 2011b), participants who had attained lower learning outcomes on the novel tone-to-symbol system also showed less efficient retrieval skills. This relation between learning success and subsequent retrieval efficiency was present to an equal extent in both bilinguals and monolinguals, mimicking previous patterns from linguistic tasks (Gollan et al., 2008; Whitford and Titone, 2012), and confirming that less robust learning of content influences sound-to-content links and shapes retrieval success. \n  While retrieval efficiency could be in part explained by previous learning success in both bilinguals and monolinguals, a stronger link was identified between previous vocabulary knowledge and tone-to-symbol retrieval in the monolingual group. Specifically, in monolinguals, learners who had stronger English receptive vocabulary skills (as indexed by the PPVT) also were more efficient in retrieving sound-to-symbol mappings in the processing environment where these items had to be identified from competing alternatives. This association between receptive vocabulary knowledge and retrieval efficiency was not limited to trials with competitor items, but was found across competitor and no-competitor trials. These findings suggest that competition resolution during tone-to-symbol mapping was not modulated by previous receptive vocabulary. Rather, it appears that the ability to efficiently identify a newly-learned sound-to-symbol mapping among four alternatives was positively influenced by previous vocabulary. It is thus possible that skills that aid in the mapping of new vocabulary transferred to the novel task. Further, this effect also persisted during monolinguals' priming trials, perhaps suggesting that higher-vocabulary monolinguals used fewer cognitive resources during sound-to-symbol trials, allowing quicker responses on priming probes, or that higher-vocabulary monolinguals deployed attentional processes more efficiently in orienting toward relevant information on the displays. \n  In contrast to monolinguals, no association was found between bilinguals' English receptive vocabulary and their performance on the sound-to-symbol mapping or priming trials. When combined English/Spanish or Spanish-only receptive vocabulary skills were considered, this association was not significantly strengthened. It is possible that, in monolinguals, a more centralized and less distributed lexical system may better capture general word learning skills and related cognitive factors that might contribute to mapping a new symbolic system. It is possible that since, in bilinguals, vocabulary skills are frequently more context-specific due to language immersion tied to specific social settings, it is more challenging to index their core vocabulary knowledge through standardized measures such as the ones employed here. As a result, core knowledge that may point to underlying word learning skills was perhaps not as successfully indexed in the bilinguals. Alternatively, it is possible that bilinguals, due to word-learning experiences across linguistic contexts, may have word mapping skills that are not necessarily associated with their overall word knowledge. Interestingly, in the bilinguals who did not succeed on the learning task, a link between Spanish receptive vocabulary and sound-to-symbol mapping success was in fact evident. These findings must be treated with care given the small sample of bilingual non-learners (n = 11). Yet, they speak to a shared scaffolding mechanism for newly learned sound-to-symbol mappings in monolinguals and bilinguals. Additional research is needed to examine L1 and L2 lexical contributions to novel word learning in bilinguals. In sum, the ability to identify tone-to-symbol targets among competing options was modulated by different yet related variables in bilinguals and monolinguals, with learning success predicting retrieval efficiency in both groups, but with previous vocabulary knowledge predicting symbol retrieval efficiency more in monolinguals than bilinguals. \n  In addition to similarities in retrieval skills, monolinguals and bilinguals also showed similar competition effects within the novel symbol system. Findings of similar competition effects in monolinguals and bilinguals are consistent with previous language studies where linguistic competition resolution was examined and similar competition resolution patterns were found in the two groups (e.g., for lexical competition during word recognition, see Blumenfeld and Marian, 2011; for competition within a sentence context, see Paap and Yunyun, 2014). In the linguistic domain, comparisons of competition effects in bilinguals vs. monolinguals may be influenced by group differences in experience and proficiency, potentially obscuring bilingual advantages in competition resolution. However, the current findings suggest that, based on equivalent training and attainment, competition effects prior to target identification continue to have the same magnitude in the two groups. \n  Further, competition effects were not modulated by previous vocabulary knowledge or by learning success. These results are consistent with previous findings that language-based competition effects may not be modulated by proficiency during naming (e.g., see Marian et al., 2013, for equivalent Stroop effects across trilinguals' languages with varying proficiency levels). Similarly, Marian and Spivey (2003) found comparable lexical competition effects in L1 and L2 during auditory word identification in proficient late bilinguals. Nevertheless, other sources suggest that, in bilinguals, conflict monitoring skills may be honed to better identify ambiguities as they arise (e.g., Abutalebi et al., 2012). It is possible that novel representations must be more established before effects of previous experience on competition resolution can become visible. As might be expected in very novice learners who might show more variability in responses (e.g., Hulstijn et al., 2009), considerable variability across items and participants may have occluded subtle influences on competition effects at this early stage of learning. As such, a possible relation between competition resolution and previous vocabulary can be examined at various proficiency levels in future research. \n  While competition effects were similar in bilinguals and monolinguals, subtle group differences emerged in the nature of inhibition mechanisms that were deployed to resolve this competition. At 200 ms post-target identification, significant competitor inhibition effects were identified for bilinguals, with somewhat less robust inhibition effects in monolinguals. Interestingly, smaller Stroop inhibition effects were associated with less residual competitor inhibition at 500 ms post-target identification for bilinguals, with no such correlations in monolinguals. This correlation is suggestive of a time window where inhibition may be gradually lifted, given the absence of significant inhibition effects at this time. Together, it appears that inhibition effects lingered somewhat longer in bilinguals. These findings are consistent with Treccani et al. (2009)'s findings on a nonlinguistic priming task, and suggest that bilinguals may exert somewhat stronger inhibition effects than monolinguals to separate competitors from targets in novel symbolic processing environments. \n  While in the nonlinguistic domain findings suggest that bilinguals may maintain inhibition longer than monolinguals, a different pattern may be present in the linguistic domain. In a linguistic context that is analogous to the current study, Blumenfeld and Marian (2011) showed inhibition of linguistic competitors at 500 ms post-target identification for monolinguals but not bilinguals. Instead, a correlation was present where bilinguals with smaller Stroop effects showed less residual competitor inhibition at 500 ms post-target identification. Therefore, while the timecourse of inhibition appears identical across nonlinguistic and linguistic domains in bilinguals, monolinguals show more sustained inhibition effects in the linguistic domain. Additional research is needed to better explicate this difference across modalities. It is possible that, as bilinguals become more proficient with content knowledge (as is the case in the linguistic domain), they may show faster competition resolution, also leading to earlier release of inhibition mechanisms post-target identification (e.g., Mishra et al., 2012). Monolinguals may sustain inhibition longer in a linguistic environment to protect against intrusion from similar-sounding words, while bilinguals may release such inhibition sooner to allow for language switches. As was the case for competition resolution prior to target identification, the magnitude of residual inhibition effects post-target identification was not modulated by experiential factors. These findings suggest that, at least for newly-learned symbolic information, the magnitude of inhibition may not be related to linguistic knowledge per se but may relate to participants' domain-general cognitive skills."
271920,3.0," Together, findings from the linguistic and nonlinguistic processing domains suggest that, while bilinguals and monolinguals are very similar in their efficiency of competition resolution (as indexed by response efficiency on tone-symbol mapping trials with vs. without competitors), they show subtle differences in the time course along which they maintain inhibition after word identification. In turn, as in Blumenfeld and Marian (2011), bilinguals and monolinguals showed equivalent magnitudes of target facilitation during the time immediately following target identification. It is likely that target facilitation acts as a competition resolution mechanism that complements inhibition of irrelevant information (e.g., Paradis, 2004), and previous work suggests that it outlasts inhibition effects across time (Tanaka and Shimojo, 2000). The current findings where residual activation was probed at three times post-target identification, confirm these patterns."
2617564,0.0,"The present results replicate and extend reports by Stilp et al. [22] of rapid efficient coding of redundancy among acoustic dimensions in novel complex sounds. Three manipulations, each of which attenuates correlation among attributes, were tested separately to examine the perceptual significance of each. Overall, simple strength of the primary correlation (principal component) is inadequate to predict listener performance. Initial superiority of discrimination for statistically consistent sound pairs was relatively insensitive to truncation of evidence supporting the correlation (Experiment 2) and to increases in the frequency of Orthogonal test trials (Experiments 4, 5). However, increased evidence of an orthogonal dimension provided by greater acoustic/psychoacoustic range (Experiment 3) proved highly salient, resulting in equivalent discrimination performance throughout the experiment."
2617564,1.0," Patterns of performance cannot be explained by independent weighting of acoustic dimensions (AD, SS), as changes in discriminability can only be attributed to the correlation or covariance orthogonal to it. This perceptual adherence to derived statistical structure, and not physical acoustic dimensions per se, is not without precedent. There is good evidence that auditory cortical representations decreasingly correspond to physical stimulus dimensions [37]�[39]. Wang [39] refers to this as �non-isomorphic� transformations of the input. Examples of non-isomorphic stimulus representations in auditory cortex include encoding spectral shape across varying absolute frequencies [38], gross representation of rapid change in click trains with short inter-click intervals versus phase-locking to trains with slower inter-click intervals [40], [41], and encoding pitch versus individual frequency components [42], [43]. Such non-isomorphic transformations may be similar to the loss of acoustic dimensions (AD, SS) seen here, as more efficient dimensions better capture perceptual performance. Results are in agreement with Stilp and Kluender [44], who report efficient coding of redundant acoustic dimensions in the face of unrelated variability in a third acoustic feature."
2617564,2.0," Optimal combination and weighting of individual stimulus dimensions has received considerable attention in vision research. Models of Bayesian inference and ideal perceptual performance have been shown to effectively capture aspects of perception of objects [45], [46], edges [47], movement [48], and slant or orientation [49]�[52]. These ideal observer models have been extended to perceptual combination of sensory cues from different modalities, such as integrating visual and auditory cues to location [53], visual and motor cues to performing certain actions [54]�[57], and visual and haptic cues to height [58], shape [59], and even thoroughly trained arbitrary associations such as one between luminance and stiffness [19]."
2617564,3.0," Three important points distinguish these earlier studies from the present findings in auditory perception. First, such studies often must address inherent weights or biases ascribed to each cue. For example, visual information is habitually weighted more heavily than auditory or haptic information. Here, acoustic dimensions AD and SS were adjusted through extensive control studies to be equally available perceptually, so a priori perceptual weights are equated. Second, many cue weighting studies examine performance as a function of relative noisiness (relative ?) of respective cues. Sensibly, when multiple cues are available but one is or becomes more noisy (larger ?), perceptual weights are greater for less noisy cues that better inform behavior. Optimal cue combination occurs when one cue (typically the one weighted more heavily absent experimental manipulation) is made noisier and perceptual weights shift toward a less noisy source of information (e.g., making the visual signal noisier and observing increased weight attributed to haptic information [58]). Cues AD and SS share equal psychoacoustic variability as measured by JNDs. Third and most importantly, these examples from vision or multimodal research demonstrate optimal weighting of individual physical stimulus dimensions. The present findings indicate optimal weighting of derived dimensions that capture statistical relationships between attributes. This likely suggests a more sophisticated level of processing than that observed for reports of combination or integration of individual physical stimulus cues."
2617564,4.0," Behavioral results were consistently predicted by the PCA network model [26]. Perceptual processes first capture the principal component of variation in the two-dimensional stimulus space at the expense of the orthogonal component [22]. From listener performance and models, it appears that both principal and second components become weighted proportional to the amount of variance accounted for by each. In the stimulus sets tested here, this entailed relatively modest weights on the second component, corresponding to initially reduced discriminability. Following further exposure to the stimulus set, variance not explained by the principal correlation is detected and exploited, improving discrimination of Orthogonal sound pairs back to baseline levels. Only when evidence for the orthogonal dimension was increased through greater covariance not shared with the principal component (Experiment 3) was sufficient weight attributed to the second component, extinguishing early differences in discriminability. Otherwise, given that correlations tested here were attenuated in different manners, simulations primarily varied in how the initial decrease in Euclidean distance between Orthogonal stimuli gets smaller and/or recovery to baseline distances occurs sooner."
2617564,5.0," One shortcoming of Sanger's [26] network model is that it assumes the correlation matrix of the inputs. PCA can operate over either a correlation or covariance matrix, and there are reasons to prefer a covariance matrix for psychoacoustically-normed experimental materials employed here. The predictive power of the PCA model [26] was improved when modified to operate on the covariance matrix of the input rather than the correlation matrix. The modified model provided predictions that better fit listener performance. Further, Eigenvalues from covariance- but not correlation-based PCA analyses closely reflect listener performance (Table 1). Greater Eigenvalues on the second component (orthogonal to the main correlation) predicted better discrimination of orthogonal variation. At least for these stimuli, covariance among acoustic attributes appears to be a better estimate of perceptual performance than correlation, but given markedly different ways to manipulate covariance captured by a particular component in PCA (stimulus addition/deletion, over/undersampling, etc.), further studies are required to better understand this relationship."
2617564,6.0," The particular PCA model investigated here [26] is certainly oversimplified and is unlikely to precisely reflect neural learning mechanisms. Dimensions of AD and SS are almost certainly encoded across a large number of neurons and not the localist representation tested here. A more serious challenge is to identify neurally plausible mechanisms for instantiating PCA-like performance. Conceivably, circuitry of auditory cortical and association areas may provide the required connectivities. Precortical processes might also be implicated, given that PCA has proven practical for depicting correlations across neurons in the vibrissal sensory area of rat thalamus [60]. Lower subcortical auditory nuclei are also candidates given that, relative to the visual system, much more processing (more synapses and hence greater neural recoding) occurs within the brainstem before cortex [37]. Identification of neural substrates supporting perceptual changes demonstrated here and by Stilp and colleagues [22] would facilitate development of more authentic computational models."
2617564,7.0," The present experiments have investigated how listeners adapt to strong covariance structure coupled with varying types of orthogonal variation. This form of structure is particularly amenable to decomposition via PCA, but other models are better suited for a broader array of cases such as those presented by statistical distributions for some speech sounds (e.g. distributions of vowels in formant (F1-F2-F3) space are not orthogonal). For extraction of independent dimensions that are not necessarily orthogonal, techniques such as linear independent component analysis (ICA), which efficiently encodes structure into latent components that minimize mutual information (redundancy) between outputs (e.g., [61]), may provide a better statistical analog to perceptual organization."
2617564,8.0," The present results could provide insights into models of perceptual organization for complex sounds such as speech. While the novel sounds tested here only varied along two complex dimensions, patterns of covariance naturally scale to high-dimensional feature spaces. In complex natural stimuli such as speech, multiple forms of stimulus attribute redundancy exist concurrently and successively [20], [21], [62]�[65]. To the extent that patterns of covariance among acoustic attributes in natural sounds are efficiently coded, the present results may inform how the auditory system exploits different patterns of redundancy to learn and distinguish different speech sounds."
2617564,9.0," While some have suggested the importance of correlations among stimulus attributes are central to perceptual organization for speech [22], [63], [66]�[68], it has been more common to emphasize 1st-order statistics (e.g., probability density) as a means to characterize distributions of speech sounds [69]�[73] or cues [74]�[77]. In experiments that oversampled the Orthogonal sound pair (Experiments 4 and 5), manipulations of probability density had little to no effect on patterns of performance. At least in this particular paradigm, higher-order redundancy (covariance) was more perceptually salient than lower-order redundancy (probability density). Future research that explores relative influences of these different types of statistical structure will inform models of perceptual organization and categorization of speech."
2617564,10.0," Covariance among complex acoustic attributes in novel stimuli is exploited quickly and automatically in the present experiments. Perception only later comes to encode residual variability in ways that reflect optimal statistical weighting of covariance not accounted for by the principal component of the stimuli. Results illuminate stimulus characteristics that support coding of stimulus redundancy that is rapid, unsupervised, efficient, and statistically optimal."
1911559,0.0,"The current study used a novel category?learning task to examine the effects of unisensory and multisensory cues on incidental category learning across middle childhood. As expected, the results indicate a significant improvement in incidental learning from 6 to 10�years of age. In addition, as early as 6�years of age in this study, children demonstrated greater performance on an incidental categorization task following exposure to multisensory (audiovisual) cues compared to unisensory information (visual or auditory alone)."
1911559,1.0," Multisensory information has previously been shown to improve encoding (Bahrick & Lickliter, 2012) and better facilitate subsequent learning compared to unisensory stimulation in children as young as 3 to 4�years of age (Jordan & Baker, 2011). Similarly, on speeded RT tasks, children as young as 4�years of age were able to integrate audiovisual information to improve performance to a greater extent than with the presentation of unimodal stimuli, but were less efficient than older children and adults (Nardini et�al., 2015). Other developmental studies that have examined multisensory integration on tasks that did not require speeded responses also report the pooling of bimodal signals to be sub?optimal until even later in childhood, around 8 to 12�years of age (Gori et�al., 2008; Gori et�al., 2012; Nardini et�al., 2010; Nardini et�al., 2008; Petrini et�al., 2014). In sum, such findings suggest that although multisensory information may be pooled to a certain extent at this young age, mature integration of bimodal signals undergoes a more protracted developmental course."
1911559,2.0," The emphasis in the current study was on incidental category learning during a sustained attention task. This differed from the aforementioned previous studies and their focus on developmental changes in the pooling of redundant cues on explicit learning or perceptual tasks. Incidental acquisition of information occurs across multiple learning tasks in educational environments (Postman, 1964), and is therefore an important area of focus for research examining the role of multisensory stimuli on learning. In the current study, the simultaneous presentation of complementary visual and auditory information, in which both features were informative to family membership, resulted in enhanced performance on the incidental learning of categories across all age groups."
1911559,3.0," Although no significant interaction between age and learning condition was found, others have found that the pooling of multisensory cues may become more advanced with age (Barutchu et�al., 2009; Gori et�al., 2008; Gori et�al., 2012). The emphasis on learning in the current study may therefore underlie the differences in findings from studies examining the development of pooling bimodal cues. That said, despite a lack of reliable difference in the pattern of performance with age in the current study, some age?related changes in the benefits of multisensory cues were identified. For instance, performance on the category identification task following audiovisual learning positively correlated with age, and with a trend for a positive relationship between age and auditory?only learning. In contrast, performance following visual?only learning did not correlate with age. These results are therefore somewhat in line with previous findings that argue for a refining of the ability to use multisensory information across this age span (e.g., Nardini et�al., 2015). This would afford the conclusion that the use of multisensory cues for learning may still undergo some development during the primary school years. Of note, however, is that there was also a trend for improved performance with age in the auditory?only condition, suggesting that these findings may reflect age?related changes in the use of auditory information to support learning. This is particularly supported by our findings that 6?year?olds performed at chance following learning with auditory?only cues, but above chance with visual and audiovisual cues. Others have also reported age?related improvements in auditory processing throughout childhood and into adolescence that may affect responses to perceptual training (Huyck & Wright, 2013). Similarly, differences in the processing of visual and auditory stimuli with age have been seen on multisensory tasks, with children and adolescents, compared to adults, showing reduced processing of auditory distractors compared to visual and bimodal (Downing, Barutchu, & Crewther, 2014)."
1911559,4.0," In this study, therefore, although younger children used visual information (both in the visual?only and multisensory conditions) to the same level as older children, changes with age were seen in the extent to which auditory cues were considered useful for learning. Initially, this could be considered a matter of cue saliency, with the auditory stimuli not having been as salient as the visual information. However, this explanation is contested by our seemingly contradictory findings that children at this age were less able to discriminate between visual targets than between auditory exemplars, but with an equal level of discriminability between the different modality exemplars above 8�years of age. Furthermore, no differences in categorical learning were found between unisensory visual and auditory cues in any group in this study, including 6?year?olds, suggesting that visual and auditory stimuli were equally salient and usable."
1911559,5.0," As an alternative explanation, the findings may allude to a visual processing bias in younger children. This is in contrast to findings of an auditory processing dominance in young children, with a change to visual dominance in older children and adults (Napolitano & Sloutsky, 2004; Sloutsky & Napolitano, 2003). By 4�years of age there is some flexibility observed in terms of modality dominance that is dependent on the task demands, wherein stimuli are only processed in the preferred modality when different sensory cues are of equal salience (Robinson & Sloutsky, 2004). Therefore, children aged 6�years may already demonstrate visual dominance on tasks such as the one presented here. Given that no age and condition interactions were identified, however, such conclusions can only be met tentatively. Indeed, it is also worth noting that neither of the oldest two groups demonstrated this visual processing dominance, despite robust findings of visual modality dominance in older children and adults on other tasks (Koppen & Spence, 2007; Sinnett, Soto?Faraco, & Spence, 2008; Spence, 2009)."
1911559,6.0," As well as an analysis of group differences on an incidental category?learning task, we also reported the findings from the attention trials on the main MALT task. Here, no differences were found across the different MALT learning conditions, suggesting that effects of condition in incidental learning were not related to the attentional aspects of the original task. Although differences were seen between age groups, all groups demonstrated a comparable pattern of performance."
1911559,7.0," Furthermore, although 6?year?olds required more trials to criterion, all participants included in the analyses experienced a total of 50 target exemplars travelling to the two habitats before the category task was presented. Analyses of these learning task parameters therefore only highlight age group differences rather than differences across learning conditions. This is in line with what would be expected on measures of sustained attention in these age groups. As such, age?related differences on this aspect of the task likely reflect improvements in speed of processing visual and auditory information, developmental changes in levels of inhibition (Levy, 1980), as indicated in a reduction in commission errors, and improved attention, as measured by decreasing omission errors, from the youngest to oldest age groups."
1911559,8.0," As well as a measure of incidental category learning, the current study examined explicit categorical knowledge across groups. A difference was found in the pattern of performance in the incidental learning compared to the explicit knowledge tests, with no effect of condition observed in the latter, and the youngest children (6?year?olds) demonstrating particular difficulty in expressing correct categorical information. While no feedback was given on the incidental categorization task, this finding may be related to the participants being made aware of categorical differences both in the incidental task and being posed a question of this nature in the subsequent explicit knowledge task. This may have cued participants to devise a plausible explanation for categorical differences. Thus, being asked to verbally express categorical information before the presentation of the incidental category identification task may have resulted in a levelling of performance across the two different tests. Alternatively, this finding may be reflective of different processing systems for explicit and incidental learning (Gabay et�al., 2015; Tricomi et�al., 2006)."
1911559,9.0," Our results raise the question as to whether similar findings would also be observed not only on other novel categorical learning tasks, but also other learning tasks such as associative learning, and in different domains such as language and numerical learning. Jordan and Baker (2011) found that in young children aged 3 to 5�years, learning to match numerosities was facilitated when given multisensory rather than unisensory information about the number. A key difference in these studies is in the nature of incidental learning in the current task as opposed to explicit mathematical concept learning in the above?mentioned study. A further difference is that our analyses were not concerned with speed of responses, but rather the accuracy of categorical selection. In addition, in the study by Jordan and Baker (2011), audiovisual trials provided a greater total amount of stimulation in comparison to unimodal trials. That is, only on audiovisual trials were participants exposed to both visual and auditory information. This may have resulted in enhanced arousal to stimulus properties and subsequent representations. In the current study, all learning trials (regardless of learning condition) included both auditory and visual events, with learning conditions differing only on the basis of the informative nature of the cues (i.e., the features that could be used for categorical judgements). Findings from the current study therefore refute the assumption that better performance in a multisensory learning condition compared to unisensory is a result of enhanced stimulation from multisensory trials. In conclusion, even in light of the differences in tasks used across studies, the comparable results of improved learning following exposure to multisensory cues compared to unisensory, even in children as young as 6�years, is a robust finding."
1911559,10.0," As mentioned previously, on some tasks, multisensory integration is not as efficient in young children as it is in older children and adults (Burr & Gori, 2012; Gori et�al., 2008; Nardini et�al., 2010; Nardini et�al., 2008), a finding somewhat reflected in the current study. Conclusions from earlier studies imply that combining audio and visual stimuli either at the level of attention or at a neural level of stimuli integration may be more difficult for younger children and therefore not facilitate learning to the same extent as in older children. However, there are likely to be numerous cortical and subcortical mechanisms involved in multisensory integration that may develop at different rates (e.g., Molholm et�al., 2002; Noesselt et�al., 2007; Stekelenburg & Vroomen, 2007). This may underlie the disparity in the reported ages at which mature levels of multisensory facilitation are observed, particularly given that performance on different multisensory tasks may be associated with distinct neural substrates. The examination of multisensory cues on incidental category learning in children younger than 6�years of age would be an important avenue for future research in order to elucidate this further."
1911559,11.0," In the current study, it was only the nature of cues for categorical learning that differed across learning conditions. It is not clear therefore whether multisensory stimulation in some learning contexts would have a distracting effect on performance or would lead to increased focus of attention; particularly when multimodal stimuli are not task?related, as would typically be encountered within a learning environment. For instance, difficulties in encoding unisensory cues have been found when multisensory properties compete for attention (Lickliter & Bahrick, 2004). Given known developmental changes in attention, there may also be differing patterns of response to multisensory distraction across development. Further research should therefore examine the use of unimodal and bimodal noise (distractors), or an increased working memory load within and between modalities on a similar learning task."
1911559,12.0," This study provides important insight into the use of multisensory information in an educational environment on incidental category learning. The intersensory redundancy hypothesis (IRH; Bahrick & Lickliter, 2012) posits that the pooling of multisensory cues presented in synchrony leads to enhanced perception. Given the nature of the current task, theoretical assumptions of the IRH can only go some way to explaining the current results of enhanced category learning following multisensory cue exposure compared to unisensory. Essentially, the current study included complementary but not redundant amodal stimuli in order to better emulate sensory information typically found in learning environments. Even in light of this difference, the results suggest a reliable facilitatory effect of multisensory stimuli presentation between 6 and 10�years of age. Moreover, our results are in accord with findings that multisensory integration (particularly with the integration of auditory and visual information) may undergo a protracted developmental course through the early primary school years. This has particular implications for the deployment of multisensory learning tasks within primary education. In particular, multisensory information may not be as beneficial to younger children when information from a single sense is dominant. For instance, the results are indicative of a relative difficulty in the use of auditory information to support category learning in 6?year?olds, unless combined with complementary visual information. This has implications for the use of auditory information on categorical learning tasks in children below 8�years of age. Where the simultaneous presentation of auditory information with visual cues may better support a representation and subsequent learning, this may be particularly relevant for younger children who demonstrate poorer performance than older children on unimodal auditory tasks."
271124,0.0,"Summary of results \n This study examined the role of tone vs. non-tone language experience, monolingualism vs. bilingualism, and auditory-only vs. auditory-visual training of foreign lexical tone contrasts. The results are summarized under four headings below, followed by discussion of the results."
271124,1.0,"Training�effects of age and language factors \n  Training was effective: there was a general improvement in performance from pre- to post-training. Training was most effective for 8-year-olds; 6-year-olds showed only limited effects of training. Training was more effective if children had tone language experience, an advantage evident in the 8- but not the 6-year-olds. These effects of age and tone language on training were most clearly indexed by the ao rather than the av pre- and post-training tests."
271124,2.0," Language background and training mode \n  There was a differential effect for the type of training: Monolingual children improved markedly with AV training but not at all with AO training, whereas Bilingual children improved markedly with AO training and to a lesser extent with AV training. However, these effects were only apparent when indexed by ao tests."
271124,3.0," Correlation with language measures \n  For children with English as their only, or as one of their, language(s), proficiency on a phoneme deletion task was positively related to Mandarin tone identification in auditory-visual pre-training test trials. As this was before training began, it shows that those children good at manipulating phonemes in (one of) their native language(s) were also good at perceiving what were completely novel phonological elements for the Mono-Eng, the Bi-Eng/Arabic and the Bi-Eng/Thai groups. This advantage did not extend to training, there was no advantage for good phoneme deleters in learning about foreign tones, just in their initial perception of foreign tones. \n  The results bear on a number of issues which are discussed below ahead of a discussion of limitations and suggestions for future research."
271124,4.0," age \n  There are two possible reasons why training was more effective with the older 8-year-olds than the younger 6-year-olds: task difficulty, and reduced sensitivity to foreign sounds. First it may be that the task employed here was demanding in terms of the degree of sustained attention required. For example, while in the procedure used here the pre- and post-training trials were the same, the training trials incorporated variation of both speakers and words. Wang et al. (1999) trained adults on a variety of monosyllabic Mandarin words spoken by a variety of speakers and found especially resilient learning. In an adaptation for children Wang and Kuhl (2003) also found a high degree of learning. However, over their six training sessions they graded the difficulty of the tasks (2 weeks ABX, 2 weeks 2AFC identification, then 2AFC with speaker variation) and within each pair of sessions they trained easier tone pairs first. While the Wang and Kuhl (2003) study and the study reported here shared the variability of speakers and words, here the task would have been more difficult because (i) tasks were not graded and (ii) a single presentation 4AFC identification task was used. It remains for future studies to adapt the procedures here and those in the Wang studies (Wang et al., 1999; Wang and Kuhl, 2003) to derive optimal, L2 training regimes especially for younger, e.g., 6-year-old children. \n  Secondly, irrespective of task difficulty, 6-year-old children may have reduced sensitivity to foreign sounds. Burnham and colleagues (Burnham et al., 1991; Burnham, 2003) investigating what has been called a second period of perceptual attunement, have shown that 6-year-olds, compared with both 8-year-olds and also 4-year-olds, have reduced sensitivity to L2 sounds and suggest that this is an adaptive device which facilitates attention to the difficult task of phoneme-to-grapheme mapping involved in reading. Burnham contends that at 4 years this process has not begun, and by 8 years the process has become relatively automated, whereas at 6 years this attentional filtering is most useful. Whether this explains the results here cannot be fully ascertained without a 4-year-old comparison group, and it remains for future research to investigate this issue further."
271124,5.0," Test trials and generalization of training \n  In this experiment children were given both ao- and av-test trials pre- and then post-training. The training was either with AO stimuli in one group and AV stimuli in another group. In addition, the stimulus words and speakers were different in the pre- and post-training test phase on the one hand and in the Training trials on the other. Therefore, generalization of training can be indexed in two ways. First, any improvement after training, can be considered generalization because the training and test stimuli differed (although there could be an across-the-board improvement because the pre- and post-training stimuli were from the same pool). In this sense then, any performance gain from pre- to post-training, such as those gains found in this study, can be considered as both learning, and generalization of learning. Second, generalization can be indexed by any performance gain across both the ao- and the av-tests, irrespective of whether the training used AO or AV materials. A confounding factor in the interpretation of the results with respect to this type of generalization is that ao-tests proved to be more sensitive indices of performance gain than were av-tests. Nevertheless, it can be concluded that, in general, generalization of training was best for 8-year-olds, and especially for 8-year-olds with tone language experience whether that be monolingual (Mono-Thai) or bilingual experience (Bi-Eng/Thai)."
271124,6.0," Tone language experience \n  Participants with tone-language experience (the Bi-Eng/Thai and Mono-Thai groups) benefitted more from training than those with no tone language experience (Bi-Eng/Arabic and Mono-Eng), irrespective of whether the children were monolingual or bilingual. In addition, those with tone language experience (especially the 8-year-olds) also showed better generalization of training across test type�ao and av. This supports previous findings that tone language experience facilitates adult lexical tone perception (e.g., Burnham et al., 2014) and extends these findings to children. Moreover, these data provide information about two aspects of language learning. First, the data tell us that there is some perceptual or conceptual information about lexical tones that is general across tone languages (or at least for the two tone languages here, Mandarin (the target language) and Thai (the language experience language). Second, the data tell us that any metalinguistic advantage or extra skills learned as a product of learning more than one language is independent of the skills required for learning to perceive lexical tone in a tone language. Each of these is discussed in further detail below."
271124,7.0," Task difficulty and differences between tone languages \n  Mandarin and Thai tone inventories differ on a number dimensions: Mandarin has 4 tones and Thai 5; Thai has 2 level tones and 3 contour tones, Mandarin has 1 level and 3 contour tones; all 5 Thai tones are of similar duration, whereas Mandarin tones differ markedly in duration. Thus Mandarin and Thai are quite distinct with respect to their tones and this has two interesting implications with respect to the results obtained here. Firstly, given these differences, it is reassuring that there was an effect of (Thai) tone language background on the learning of the target tones in Mandarin, i.e., that there was transfer of learning from Thai tones to learning Mandarin tones. Second, the differences between Thai and Mandarin may have played a part in the relatively small performance gains in tone perception here. Further studies in which the background language and target tone language are more similar with respect to their tones, e.g., Thai and Cantonese (6 tones: 3 level and 3 contour, all tones of similar duration), may result in more performance gains. More generally, the relative salience of differences between tones within a particular language and the relative difficulty of discriminating tones in one tone system vs. that in another system is largely unknown (but see Burnham et al., 2017). Recent studies have shown that tone perception develops for a specific tone system (Yeung, et al., 2013) and that non-native tone language speakers have difficulty with tones that are similar or overlap with their native tone systems (Hao, 2012). Much more research is required on what particular parameters make particular tones or tone systems easier or more difficult to learn."
271124,8.0," Monolingual and bilingual children \n  While monolingual vs. bilingual status of the children did not in itself facilitate tone learning in children, it did contribute to the mode of training that was most effective, as measured by the performance gain between pre- and post-training ao-test trials. The Auditory-Visual (AV) mode of training was the most effective for monolingual children, whereas for bilingual children Auditory-Only (AO) training, and, to a lesser extent, AV training resulted in performance gains. The source of this difference is not clear. One possibility is that exposure to a greater range of linguistic differences and devices, as would be the case for bilingual children, allowed them to (i) learn from a range of parameters, including auditory information alone or auditory and visual information combined, and (ii) learn that, even though there is visual information for tone (Burnham et al., 2001a,b, 2014; Smith and Burnham, 2012) the auditory information is by far the most salient. This is speculative and requires more definitive evidence."
271124,9.0," Phonological awareness \n  English phoneme deletion ability (in the Mono-Eng, Bi-Eng/Arabic, and Bi-Eng/Thai groups) was positively related to pre-training auditory-visual test trial performance. Although there was no relationship here between reading and tone perception, the results are reminiscent of those of Burnham et al. (2011) who found a significant relationship between Thai children's reading ability and their phonological and tonological awareness, and between Australian English children's reading and their phonological awareness. Thus here, the ability to manipulate phonemes is related to the ability to perceive foreign speech sounds and in Burnham et al. (2011) reading ability is related to the ability to manipulate (foreign) phonemes and tonemes. Further research is required to investigate the nature of any three-way relationship between reading, phonological awareness, and foreign speech sound (and of especial interest here, lexical tones), the findings of which could be relevant to children's propensity to learn a second language, especially a tone language."
271124,10.0, Limitations and future directions \n A number of limitations can be noted.
271124,11.0,"Training and test \n  The post-training test implicitly tested for generalization across speakers and words, and, in addition, these trials provided implicit tests of generalization from training mode (be it AO or AV) to test mode, as both ao and av tests were given irrespective of training. The downside of this is that tests between trained and untrained stimuli and voices could not be conducted. It is possible that children, even the younger 6-year-olds, may have performed better on trained than untrained stimuli and voices. This should be remedied in future studies. The upside is that any improvement as a result of training indicated generalization of training. So the performance gains obtained here, while modest, are robust. \n  A related point concerns variability. As discussed above, variability improves the robustness of learned distinctions (Wang et al., 1999), but variability should be optimized for the age and maybe the language background of the children. Here it was not. \n  A final point on this theme is that for both clusters of results�the age and tone language experience cluster, and the training mode and monolingual/bilingual language experience cluster�the ao-tests were more sensitive measures of improvement than were the av-tests. And, even though auditory and auditory-visual modes differentially affected training outcomes in monolingual and bilingual children, the indexation of such training was still generally better on ao-tests. The reason for this is unclear. In future studies it would seem that ao-tests should be preferred."
271124,12.0," Phonological awareness \n We included English language tests of phonological awareness here and found a positive relationship between phoneme deletion and pre-training auditory-visual test performance. Future studies should investigate this further by including reading tests across languages, phonological awareness tests across languages, and also tests of morphological awareness (McBride-Chang et al., 2003) and even executive function, in order to determine predictors of good lexical tone learning."
271124,13.0,"Instructions \n No specific instructions were given. Children were simply told to pay attention to both the auditory and visual aspects of the speakers as we wished to determine whether children naturally pick up relevant lexical tone cues in an experimental setting. In real-life L2 learning situations such experimentally objective procedures may not be desired; indeed any relevant cue could and should be made available. In this regard, Chen and Massaro (2008) tested Mandarin perceivers' Visual-Only (VO) identification of the four Mandarin tones. (Remember that Mandarin language adults are worse than English language adults in VO tone perception�Smith and Burnham, 2012; Burnham et al., 2014). Initially the Chen and Massaro adults performed only just above chance and were better for the 55 and 214 tones than the 35 or the 51 tones. In a follow-up test adults were told about visible movements in tone perception and instructed to pay attention to movements of the neck, head, and mouth. Visual-Only tone perception improved significantly. Further work on perceivable visible cues for tone perception is required to facilitate L2 tone learning regimes."
271124,14.0,"Tone difficulty \n The Chen and Massaro (2008) results also raise the issue of the relative difficulty of identification of individual tones and discrimination of tone pairs. Although the results of this study reported here were based on perception across all Mandarin tones, the data also showed some differences of how the participants of different ages and language backgrounds learned the Mandarin tones in this study. Details of performance on the different tones for each language background group and each age are shown in Table C (Supplementary Material) and some comments on these are provided here. Generally, high Static tone (T55) was the easiest tone to learn for monolingual non-tonal group while the Dynamic tones (either T241 or T51) were the most difficult. The results for the monolingual tonal group were exactly the opposite: the Static (T55) the most difficult to learn while the dynamic tones (T214 and T55) were the easiest. The data is a little less definite for the bilingual language background groups. Nevertheless, it appears that 6 year-olds in both bilingual groups found the Static tone (T55) the easiest to learn while the other Dynamic tones (T35, T241, and T51) were similarly difficult; whereas the 8 year-old bilingual groups found that the Dynamic (T214) was the easiest to learn. The fact that the participants WITH A TONAL BACKGROUND found the generally difficult DYNAMIC tones T35 and T214 (Chang, 2011) in Mandarin relatively easy to learn in this study is quite interesting. However, as the task used in this study was tone identification, some distinctive contours, rising and dipping, of these two rising tones might help in identifying them. The results might well be different if participants were asked to discriminate between these two rising tones; the task might be much more difficult. Future work must take into account such differences, but at the moment there are no objective criteria for determining difficulty of tone perception within and between languages. We (Burnham et al., 2017) are currently collecting data on the perception of tone pairs from three different tone languages by adults from five different language backgrounds in order to leach out some such criteria."
1303382,0.0,"In this study, we compared frequency discrimination in the same groups of participants for two tasks designed to address different problems with the standard 2-interval psychoacoustic task designs. Our results suggest the 3I_2AFC design is less susceptible to non-auditory specific differences among participants than the 2I_6A_X design. These findings were surprising given that this latter design is often cited as minimising problems with sensory memory trace formation [12], auditory attention [24] and formation of perceptual anchors [34]. In the following, as part of addressing the issue of task susceptibility to task external effects like musical training, we consider why different results were obtained to those reported by France et al. [12]. We conclude by addressing the question of whether or how frequency discrimination may support language development."
1303382,1.0," 3.1. Performance Variability on the 2I_6A_X Task \n  An appealing aspect of the 2I_6A_X task design was that it did not appear to be susceptible to non-auditory specific differences among individuals [12] suggesting it could be reliably employed in studies involving heterogeneous populations. We did not, however, replicate this observation. A number of possible explanations come to mind to explain our different findings.  \n  Firstly, in the earlier study [12] a two-step process of threshold estimation was applied. Rough estimates were obtained of discrimination threshold and then a final threshold was estimated using an initial ?F close to the expected final threshold together with a more refined (i.e., smaller step size) adaptive staircase procedure. The small initial ?F for the second estimation would have also limited the range in which variation could be observed and all participants would have had considerable experience on the task at the point of final threshold estimation, minimising training effects. This approach to threshold estimation, better approximates standard psychophysical procedures and it is likely that had we adopted a similar strategy, we would have observed lower thresholds and reduced individual variation across the groups. However, we were primarily interested in task designs that could be reliably applied to address clinical or developmental questions, where one rarely has the luxury of applying such techniques.  \n  Secondly, although France et al. [12] provide little information about their participants, it is clear they were high functioning. The mean IQs for both the reading disabled and normal readers were more than 1 standard deviation above the population mean. The majority of the participants were therefore likely to have been students, who, as our own data demonstrate, tend to perform better than the general population on psychophysical tasks. Moreover, they tend to be more homogeneous in terms of education and socio-economic status which will also limit the impact of such individual differences on thresholds observed.  \n  Thirdly, and related to the preceding point, there is an interesting literature suggesting 2I tasks stress cognitive abilities [22] more than other tasks designs [21,24,25]. We did not expect the 2I_6A_X design to be similarly demanding since the inclusion of a stream of six repeated standard tones should have minimised cognitive demands, first by enhancing sensory memory for the standard relative to the target tone and second by cueing the presentation of the target tone to enhance temporal focus of attention [24]. However, in an analogous analysis to that used by Bishop et al. [21] to compare backward-masking in child populations using 2I or 3I tasks, we observed how the participants who performed worst on the 2I_6A_X task demonstrated the greatest improvements on the 3I task. Moreover, despite designing our protocol to maximise experience on the 2I_6A_X task prior to testing, some participants had final thresholds that were greater than the initial starting point of ?F = 160 Hz (Figure 1) though they got sufficient numbers of catch trials at this level correct. Together these observations suggest the poor performance on the 2I task was less about perceptual limitations, than about task-specific cognitive demands. This raises a question regarding why this design should be so cognitively demanding. In response to this, we can only provide the anecdotal evidence of our participants. Many commented that they found it more difficult than the 3I_2AFC task. Some noted how they had to concentrate harder when doing the task, while others commented that the target tone was always different, because it was longer as well as higher than the preceding six tones. Target and standard tones can have a variable perceptual quality particularly around threshold which contributes to the difficulty of correctly identifying the target tone. The comments of our participants suggest the presentation format for the 2I_6A_X task may have complicated the auditory decision-making process by promoting confusions in auditory percept.  \n  Finally, although we have focused entirely on issues to do with task design, it is also possible that differences in the stimuli may have also contributed to the variations in individual differences apparent between the two tasks. Roving of the standard frequency in a frequency discrimination task, as we did in the 2I_6A_X task, makes it cognitively more demanding, resulting in greater individual differences in performance [35]. Differences in stimulus duration may also have contributed since the tones in the 2I_6A_X task were longer than those used in the 3I_2AFC task. Our design does not allow us to assess these stimulus-specific effects, though work by Banai and colleagues suggest they may be outweighed by effects specific to the task [22]."
1303382,2.0," 3.2. Contributions to Task Performance of Different Environmental Factors \n  Environmental effects such as socioeconomic status (SES) and more particularly, active musical training, but not passive music listening, had a significant effect on threshold estimates for both the 2I_6A_X and 3I_2AFC tasks. Musical training may support frequency discrimination by developing a more sophisticated sense of how different sounds relate to each other. The SES measure used here incorporated among other things differences in education which may impact of an individual�s ability to develop different strategies to cope with varying task demands. The 2I_6A_X task was particularly sensitive to these factors with the relative impact on task performance increasing with increasing ISI. This supports our earlier conclusion, that it is a cognitively demanding task and further suggests that it became more cognitively demanding as the time interval between standard and target increased meaning listeners became increasingly reliant on other (non-auditory) skills to support processes involved in their decision-making. The increasing contribution of musical training to performance on the longer ISI conditions in the 2I task further suggests that musical training may enhance skills associated with auditory imaging [36] which in turn would support sensory memory for the standard tone. This idea is reminiscent of rehearsal mechanisms which support information storage in the Baddeley and Hitch [37] model of verbal short-term memory. \n  We are not the first to note how musical training makes a significant contribution to frequency discrimination abilities. Micheyl et al. [20] have demonstrated excellent frequency discrimination abilities in professional musicians. However, our data suggest, very little musical training is required to enhance frequency discrimination abilities, since none of the participants in this study were professionally trained.  \n  Bishop [15] has previously noted how exposure to music in the home was an important environmental factor for predicting performance on a test of rapid temporal processing in a study investigating auditory and cognitive abilities in twins. We did not replicate this finding; nonetheless, both our study and that of Bishop [15] demonstrate the sensitivity of the auditory system to environmental factors like musical training or listening."
1303382,3.0," 3.3. Relationships between Frequency Discrimination and Nonword Repetition \n  Auditory sensory memory is hypothesised to support verbal short term memory and we were ultimately interested in assessing the suitability of the 2I_6A_X task for further studying this component of verbal short-term memory especially since France et al. [12] had reported a relationship between frequency discrimination thresholds for the 2I_6A_X task and digit span�a measure of short-term and working memory. We therefore predicted that we would also observe a relationship (particularly for the long (1000 ms) ISI condition) with the nonword repetition task (another measure of short-term memory). However, no evidence of a relationship with nonword repetition was observed for any ISI condition in the 2I_6A_X task. By contrast, a small but significant association was observed between nonword repetition and JNDs estimated using the 3I_2AFC task. These observations add to a body of literature reporting similar such associations between frequency discrimination and performance on a range of tasks including, reading and phonological decoding skills [4,38], nonword same/different discrimination [22], as well as nonword repetition [39]. Such associations suggest frequency discrimination may impact on speech perception and hence on ability to develop language and literacy. However, we do not think the relationship is quite so direct. Gathercole and Baddeley [40] found no association with speech perception and repetition of short nonwords. In a similar vein, Rosen and Manganari [41] were unable to demonstrate a clear link between deficits in auditory processing and speech perception skills in a group of children with dyslexia. Finally, Halliday and Bishop [39] showed how, despite having deficits in frequency discrimination, children with mild to moderate hearing losses did not obligatorily demonstrate difficulties in reading or nonword repetition."
1303382,4.0," Overall, while we did observe a link between the frequency discrimination and nonword repetition using a task that was relatively robust to the individual differences assessed in this study, our study does not rule out the involvement of a third higher cognitive capacity which is separately relevant to both nonword repetition and frequency discrimination. Similar arguments have also been put forward by Halliday and Bishop [39] and Banai and Ahissar [22]. Halliday and Bishop proposed auditory attention as a possible candidate for this third cognitive capacity, while Banai and colleagues have argued that whatever the capacity, it may not be specifically auditory in nature, and is likely to be mediated by higher level processes involving the engagement of the pre-frontal cortex which associates both with attention and with memory."
2521573,0.0,"There is considerable controversy regarding what exactly the nonword repetition task is tapping into that makes it such a good predictor of language learning. In this study, we employed stimuli that were designed to explore the functioning of a hypothesized phonological storage system. We exploited the electrophysiologically-measured mismatch response to test for sensitivity to change at different syllable positions in good and poor nonword-repeaters. We predicted that if nonword repetition ability was determined by factors ancillary to the phonological loop, then depending on which factor was primary, we would see either:"
2521573,1.0, (a) Significantly reduced MMN responses for poor nonword-repeaters to the deviants at all four syllable positions which would point to deficits in early auditory discrimination; or
2521573,2.0," (b) No group or syllable-position effect if differences in nonword repetition derive from factors extrinsic to the phonological loop, such as differences in motor ability or in vocabulary knowledge."
2521573,3.0," Alternatively, if a syllable-specific group difference emerged, this would suggest that factors associated with a capacity-limited storage system were impacting on the efficiency of information processing and change detection."
2521573,4.0," In the context of our three predictions, two findings are particularly noteworthy. First, the two groups of nonword-repeaters had similar early consonant change discrimination abilities as indicated by the MMN response i.e., accuracy of early encoding of incoming auditory information was similar between the groups. In the context of children with SLI, Gathercole and Baddeley [13] first argued that deficits in nonword repetition ability could not be wholly attributed to differences in speech discrimination ability. Our findings with adults with poor nonword repetition skills are consistent with that view. Second, in the poor nonword-repeaters, the LDN response was abolished for the consonant change occurring at the third syllable of the CV-string, resulting in a significant Group × Deviant interaction. As noted above, this pattern of results is not consistent with the idea that the group difference on behavioral tests of nonword repetition is explicable solely in terms of factors such as pre-existing vocabulary knowledge or differences in articulation skills."
2521573,5.0," Rather different conclusions were reached by ?eponien? et al. [23], who compared responses to deviant changes using a similar paradigm to ours to investigate the discrimination of just noticeable differences in two nonsense syllables �ba-ka� versus �ba-ga� in young good and poor nonword-repeaters. Contrary to our own conclusions with adult participants, their data suggest a role for discrimination deficits in young poor nonword-repeaters. However, their stimuli, unlike ours, were difficult to discriminate by design and shorter in length. Moreover their participants were younger. It is likely that the different conclusions arrived at by ?eponien? et al. reflect a range of factors including the subtlety of the acoustic differences to be discriminated and maturational differences between the participants in the two studies."
2521573,6.0, Gathercole and Baddeley [13] suggested three possible candidates directly associated with the phonological loop as impacting on its function: quality of initial encoding into the loop; storage capacity; and the rate of fading of the memory trace once encoded there.
2521573,7.0," At first glance, poor encoding seems inadequate to explain the results, because of the intact MMN responses seen in poor nonword-repeaters at all syllable positions, indicating adequacy of the early stages of speech discrimination. A simple storage account is also hard to reconcile with the results. If, for instance, poor nonword-repeaters could retain few syllables in memory, we might expect to see more pronounced deficits in their discriminative responses for both the third and fourth syllables, whereas the LDN attenuation was found for the third syllable only."
2521573,8.0," The notion of a rapidly fading memory trace has been proposed to explain deficits in verbal working memory [22], [37], but seems implausible to account for our results for two reasons. First, in an earlier task, using pure tone stimuli with variable inter-stimulus intervals, Barry, Hardiman, Line, White, Yasin, and Bishop [38] showed that, although parents of children with SLI have less durable sensory memory traces than parents of typically-developing children, these differences did not associate with differences in nonword repetition ability. Second, in this paradigm, the temporal gap (and hence opportunity for decay) was held constant between each standard syllable and its deviant analogue. If it was only rate of memory trace decay that distinguished the two groups of participants, one would not predict the observed syllable-specific position effect that was observed here."
2521573,9.0," It seems then that whatever differentiates good from poor nonword-repeaters is associated with information-processing under conditions of high input load. The position-specific group differences were not reflected in the early MMN response. They only became apparent in the LDN response. This, together with a lack of correlation between the two mismatch response types, suggests that the LDN provides different information about the processing of the auditory input. In the light of previous research, we suggest that the LDN is an index of formation of a phonological representation. This corresponds to a process of encoding into short-term memory, giving a more robust representation that can persist long enough to allow comparison between deviant and standard nonwords."
2521573,10.0," Why should this encoding process be selectively impaired for the third syllable of a four syllable nonword? One interpretation is in terms of the demands the task places on rapid processing of sequential information. The notion of differences in rate of processing of incoming auditory information derives from the SLI literature, where it has been hypothesized that ability to rapidly process incoming auditory stimuli is deficient in people affected by a language or literacy disorder [39]. The results from both behavioral and electrophysiological studies probing the validity of this hypothesis have been fairly mixed, but in a meta-analysis of studies investigating mismatch responses to syllables in children with language or literacy problems, Bishop [40] concluded that deficits in auditory processing were more likely to be observed when stimuli were presented in rapid succession."
2521573,11.0," Previous MMN studies in populations with language impairments have mostly focused on single syllables, but it may be that deficits in rate of processing only become apparent when processing multiple syllables. This hypothesis was tested by Kujala, Halmetoja, Näätänen, Alku, Lyytinen, et al. [41] who assessed the ability of participants with dyslexia to discriminate changes in vowel durations embedded in three syllable CV-strings (e.g., �ta-ta-ta� versus �ta-taa-ta�). They observed no deficits pre-attentively to the change in vowel duration. However, when the participants were required to attend to the stimuli, they were less accurate at locating the change in syllable duration and they showed a significantly reduced N2b component in response to duration changes in the final syllable of the CV-string. These findings are somewhat reminiscent of our own findings. As such they are of interest given the overlap between dyslexia and SLI and given the fact that deficits in nonword repetition have been reported for both disorders [42]."
2521573,12.0," Within the context of the behavioral literature on SLI, Gathercole [43] observed that nonword repetition by language-impaired children was more accurate when nonsense syllables were presented singly with short intervals between them, than when they were presented in a string. Again this supports the notion that ability to rapidly process incoming sequences of stimuli embedded within other complex stimuli plays an important role in nonword repetition and hence in language learning."
2521573,13.0," One problem for this account of findings is that one might expect to see effects on peak latencies of MMN and/or LDN, reflecting the slower rate of syllable-processing in the poor nonword-repeaters. This was not observed. Nevertheless, in other regards, the hypothesis makes sense of the specific pattern of results obtained here. A slower rate of processing of incoming speech would have a cumulative effect across the CV-string up to the final syllable where, because there is no subsequent syllable, perceptual analysis could be completed with a consequent recovery in LDN amplitude. In effect, this is an explanation in terms of deficits in encoding. It maintains that despite adequate early discrimination processes, poor nonword-repeaters fail to generate a robust phonological representation memory. These problems however, only become evident at rapid rates of stimulus presentation."
2521573,14.0," A radically different type of encoding account is suggested by the literature on perceptual grouping. Horváth, Czigler, Sussman, & Winkler [44] demonstrated that mismatch responses can be elicited by both global and local features of stimuli. In our design, we treated the global stimulus �ba-bi-bu-be� as the standard to be compared with deviants differing on one syllable. However, this stimulus contains within it a local repeated phoneme, /b/, which potentially acts as a standard. Thus on hearing �ba-bi-du-be� the response to the third syllable might be influenced both by the deviance from the standard, but also by the deviant /d/ after a train of preceding /b/ consonants, both within the same syllable and from the preceding nonwords. Limited ability to segment individual phonemes has been mooted as a possible cause of poor nonword repetition [21], raising the possibility that poor nonword-repeaters fail to engage in local processing and so are influenced solely by global mismatch. Although this explanation fits well with prior theoretical speculations about nonword repetition, it does not readily account for the pattern of results obtained here, because when progressing through the four syllables of the nonword, one would expect to see a steadily increasing impact of local mismatch, as the number of prior standards increases. We cannot rule out the possibility that such a process contributes to the profile of results obtained here, but it would need further testing with materials designed to evaluate this explanation. If local processing is involved in mismatch generation only in good nonword-repeaters, then a clear prediction is that mismatch responses will be reduced (and resemble those seen in poor nonword-repeaters) if a different consonant were used for each syllable of the standard."
2521573,15.0," Overall, the results from this study suggest a link between two different theoretical accounts of factors affecting language learning. The auditory temporal processing account of Tallal [45] has a long history, but evidence has been mixed [46]. Most empirical studies have considered discrimination of pairs of tones or speech sounds, whereas the current study would suggest that, as the impact of slow processing is cumulative so that longer sequences of sounds are needed to reveal a deficit. The notion that phonological short-term memory is important for language-learning also has a long history. Within the context of these this theory, but the focus has been on explaining poor nonword repetition in terms of storage limitations or of rapid decay of representations. Encoding explanations have tended to be dismissed on the grounds that such problems should be apparent in short nonwords with one- or two-syllables. We suggest that this view is mistaken, because encoding is affected by the presence of adjacent syllables, and so will become apparent only in the later syllables of longer nonwords. The problem of poor nonword-repeaters seems to reflect an inability of encoding mechanisms to keep pace with incoming input. This would explain why nonword repetition is a more sensitive index of language difficulties than more conventional verbal memory span tasks, which typically adopt a slower rate of presentation."
2521573,16.0," Though the focus of this study has been on factors affecting nonword repetition, our participants were heterogeneous with respect to the language learning status of their child. The poor nonword-repeater group included a sizable minority of parents of typically-developing children, just as the good nonword-repeater group included many parents of children with SLI. However, as a further analysis of the data showed (Figure 3), the effects reported here are specific to nonword repetition ability and not necessarily associated with having a language impairment child per se. One must therefore ask, given the composition of our participant groups, what implications do our findings have for current understanding of the etiology of developmental disorders such as dyslexia and SLI?"
2521573,17.0," Much of the research published to date has focused on finding a single underlying cause for a developmental language disorder, but it is becoming increasingly clear that a deficit in any one single underlying cognitive skill is unlikely to explain the broad range of phenotypes captured under simple umbrella terms such as SLI or dyslexia. Instead as Bishop [47] has argued, these disorders are more likely to develop out of a confluence of risk and protective factors, some of which are heritable. In the context of this study, deficits in the ability to process rapidly presented incoming auditory input seem to represent one such risk factor."
2521573,18.0," In sum, previous suggestions for language specialization in the human brain have focused mainly on categorical speech perception, speech production, processing of serial order, and syntactic processing [1]. Our data suggest that human language learning capacity is boosted by being able to process sequentially-presented verbal material rapidly enough to permit the accurate recognition of syllables occurring at the rate of 5 per second, without earlier syllables interfering with the processing of later ones. We conclude that without this ability, it is hard to learn polysyllabic words or to discriminate the non-redundant information contained within a rapidly changing speech stream."
2562735,0.0,"The current study provides the first evidence in a direct comparison that distributional training of speech sounds is less effective in adulthood, when new languages must be mastered, than in the first months of life, when infants start acquiring native speech sounds. Specifically, an earlier study [4] showed that Dutch 2-to-3-month-old infants who are exposed to a bimodal distribution encompassing the Southern British English vowel contrast /æ/?/?/, have a larger MMR amplitude, and thus supposedly discriminate the two test vowels [æ] and [?] better, than infants exposed to a unimodal distribution. The current study demonstrates that this bimodal advantage is smaller (if at all present) in Dutch adults than in Dutch infants."
2562735,1.0," The presence of a bimodal advantage in Dutch adults is uncertain, because the difference in test vowel perception between bimodally and unimodally trained adults was not significant. It may be hypothesized that this non-significance was due to a ceiling effect (i.e., top discrimination) in both groups. After all, in the Netherlands English is a compulsory subject of study in middle school and high school, and it is also a language that can be listened to easily on television and other media. However, such a ceiling effect is unlikely. The MMR amplitudes in both groups were rather small (with 95% confidence intervals close to zero), suggesting relatively poor discrimination (cf., the amplitudes in adults listed in Table 1). Moreover, it has been shown that despite their experience with English, Dutch adults have trouble distinguishing the English vowels that were used in the current study [6]�[9]. Similar results have also been obtained with other languages: for instance, adult native speakers of Spanish have considerable trouble in discriminating tokens of Dutch /?/- and /a/, irrespective of the length of exposure to the Dutch language [56]."
2562735,2.0," Notwithstanding our efforts to make a sound comparison of the effect of distributional training in infants and adults, it is clear that future research is needed to replicate our results and to confirm the feasibility of our approach. For confirming this feasibility, it will be particularly important to ascertain that infant MMRs truly reflect behavioral discrimination just as adult MMRs do (section 1 below). Relatedly, future research should aim to get a much more detailed understanding of the neural processes underlying infant and adult MMRs, so that differences between them can be explained better (section 2 below presents a tentative rough explanation for the current results)."
2562735,3.0," 1. Measuring learning in adults and infants \n  The comparison of the effect of distributional training in adults versus infants was based on the MMR amplitudes. Our approach featured a minimization of methodological differences between testing infants and testing adults, and a normalization of the MMR amplitudes prior to statistical analysis in order to filter out possible residual differences between adult and infant MMRs. We presented a range of feasible normalization factors to account for the scarcity of information available for estimating such a factor in the literature, and to accommodate different possibilities of calculating such a factor. \n  Still, an important concern in our approach remains, which, notably, also applies to other outcome measures (such as looking times) in other paradigms. This concern is that the MMR may not reflect the same processes in adults as in infants. In particular, it is important to ascertain in future research whether the infant MMR indeed reflects behavioral discrimination. This has been assumed widely on the basis of evidence in adults (for a review see [23]), but has never been verified experimentally. In this context it is noteworthy that a discrepancy between behavioral and neurophysiological measures also exists in the literature on auditory thresholds. These thresholds appear to be much higher in infants than in adults in the behavioral literature [57], but less so in research where auditory brainstem responses have been measured [58]. It has been suggested that this discrepancy occurs due to the co-existence of a mature auditory system and an immature system necessary for making efficient use of this auditory system; the discrepancy can then arise when behavioral measures tap the immature system, while neurophysiological measures tap the mature system [58], [59]. To examine whether the infant MMR truly reflects behavioral discrimination, it seems therefore important to relate behavioral measures (such as high-amplitude sucking measures for the youngest infants, and eye-tracking measures for older infants) with MMR recordings."
2562735,4.0," 2. Top-down influence on bottom-up learning \n  It is not certain whether the observed smaller effect of distributional training in adults than in infants is due to a weakened distributional learning mechanism, which is generally considered to represent a purely stimulus-driven, and thus bottom-up learning mechanism [1], [2], or rather to strengthened top-down processing, or perhaps to both of these factors. Top-down processing refers to the modulation of stimulus-driven neural activity in lower-level areas (e.g., the primary auditory cortex) by higher-level linguistic representations (e.g., phonological word forms). In 2-to-3-month olds such top-down influence is lacking, because they do not have such higher-level representations yet [60]�[64]. \n  The first scenario (i.e., a weakened bottom-up learning mechanism) matches the decline of neural plasticity in the course of childhood, which has been related to an increase in the difficulty of �learning� with age [65], and which has been included in successful computer simulations of distributional learning [1], [2]. The second scenario (i.e., strengthened top-down processing) is in accordance with the observation that distributional learning of human speech sounds can be measured in adult rats [66], thus suggesting that it is a low-level mechanism that remains in place after neural plasticity has reduced to adult levels. In this scenario, distributional learning can be observed in the rats, because, similarly to the 2-to-3-month olds, they do not have linguistic representations that could modulate lower-level neural activity. \n  A top-down influence of higher- on lower-level representations may already emerge after 4 to 5 months of life, as implied by research on the histological structure and development of the human auditory cortex [67]�[69]. This research shows that the six cortical layers that children and adults have, are not present from birth but develop in the first year of life and become visible in post-mortem tissue around 4 to 5 months. Crucially, the division into multiple layers seems to be a prerequisite for top-down influence from higher- to lower-level cortical areas [70]. A look at the functional organization of the layers may clarify this. Roughly, layer IV receives input from the thalamus and projects primarily to layers II and III (�supragranular layers�), which in turn target other parts of the cortex; layers V and VI (�infragranular layers�) receive input from the supragranular layers and project to the thalamus and other subcortical structures [71]. This functional division suggests that in order to make top-down influence from higher- to lower-level representations possible, the infant cortex must first develop supragranular layers, so that incoming signals can reach higher-level areas, where higher-level representations can be formed, and it must develop infragranular layers that receive top-down influence from these higher-level representations. At 4 to 5 months, rudimentary layering becomes visible in the tissue [68]. Although it is possible that some top-down influence from higher-level to lower-level cortical areas occurs before this time via layer I, which is the only layer that is clearly visible in post-mortem tissue at birth [67]�[69], the infrastructure for canonical top-down cortical influence thus emerges just before infants begin to perceive speech sounds in a language-specific way, which is from 6 months of life ([72], [3]; review in [4]). This opens up the possibility that this language-specific speech perception relies on top-down influence of higher-level speech sound representations. At the same time, neural plasticity is still high at 6 months (e.g., [73]), so that the possibility remains that the onset of language-specific speech perception (also) relies on bottom-up learning. \n  If in adults the distributional learning mechanism tends to be �suppressed� by top-down influence of higher-level native linguistic representations, the previous significant effects of adult distributional training might have been obtained because the experimental setting (entailing the absence of a natural language context) reduced the influence of these representations on perception. Alternatively, the way the training stimuli were presented may have attracted participants� attention to the differences between the speech sounds in the tested contrast. If this is true, the observed effects of distributional training would be due to �attention�, which can be related to top-down processes in the brain (e.g., [74], [75]) rather than to distributional training, which is a strictly bottom-up mechanism. \n  In this respect it is noteworthy that for the adult Spanish learners of the Dutch vowel contrast /?/~/a:/ in [20]�[22], enhanced bimodal training in particular seemed effective. Here the acoustic difference between the minimum and the maximum value along the presented continuum of the training distribution was made larger. From previous research in the second-language literature where other training paradigms than distributional training were used, it is known that widening the acoustic distance between presented stimuli in the training phase can draw participants� attention to the differences between these stimuli and improve subsequent discrimination and categorization performance [76]�[78]. Thus, it is possible that the previous observations of �distributional learning� in adults were related to attention instead."
2562735,5.0," All in all, distributional learning as a mechanism for learning speech sounds seems to be weaker later in life than in infancy. The reduced prominence in adulthood may be due to fainter bottom-up learning as well as to the presence (versus the virtual absence in newborns) of higher-level linguistic representations and of a cortical infrastructure that enables top-down influence of these representations on bottom-up learning."
1695972,0.0,"General discussion \n In the present work, we used a test-training-retest procedure in two groups of participants who performed one hour of phonetic discrimination training, or were passively exposed to the same stimulus material, with the aim to (1) infer putative changes in the brainstem and auditory cortex as a function of short-term training, (2) estimate whether these short-term changes are reflected in neural facilitation or adaptation, (3) and to describe mutual interdependences between auditory cortex and brainstem. Results demonstrated that the brainstem but not the auditory cortex distinctively altered its response properties after short-term training. Most notably, this functional change was manifested in terms of neural adaptation and restricted to the frequency range (i.e., f1) corresponding to the trained stimulus attribute (i.e., F1). Since this frequency-specific neural adaptation was negatively correlated with the behavioral improvement of the participants during training, results point to a close relationship (~36% explained variance) between behavior and the underlying brainstem physiology."
1695972,1.0," Brainstem responses \n  Nowadays, it is generally acknowledged that the human brainstem constitutes a highly plastic entity13 that can alter its response properties as a function of both long-8 and short-term training34. For example, Carcagno and Plack44 evaluated the FFR before and after ten hours of pitch discrimination training consisting of differentiating complex tones with a static-, raising-, or falling pitch contour, and found a more robust phase locking of the FFR to the static and dynamic f0 after training. Furthermore, neural activity in the brainstem has previously been shown to be specifically modulated as a function of long-term language experience as reflected by increased f0 magnitudes in Chinese compared to English speakers in response to iterated rippled noise with Mandarin pitch contours45 or high rising Mandarin lexical tones46. However, until now, only two EEG studies specifically addressed causal changes in the brainstem induced by speech discrimination training11, 12. In a first study, Russo and co-workers11 reported that after long-term training (i.e., 35�40 sessions of one hour each) children suffering from learning disabilities exhibited brainstem responses that were more resistant to the detrimental effect of background noise than before treatment. Similarly, Song and colleagues12 demonstrated that native English-speaking participants who learned to incorporate foreign lexical pitch patterns varying in f0 (i.e., 8 sessions à 30?minutes, accomplished in 14 consecutive days) were characterized by a more faithful representation of f0 stimulus contour. \n  In the present work, we provide evidence for short-term changes in the human brainstem after only one hour of phonetic discrimination training. However, contrary to previous studies that used professional musicians as a model for long-term training8, 47, results revealed functional changes that were manifested in terms of neural adaptation and not facilitation. Interestingly, a similar neural adaptation at the processing level of the brainstem has previously been reported by Slabu and colleagues48 in the context of a passive oddball paradigm. Thereby, the authors revealed a reduction of FFRs to deviant stimuli compared to standard ones, leading to suggest that the brainstem is able to encode statistical regularities34 by suppressing responses to rare stimulus events. Even though in the present study the �deviant� stimulus (i.e.,/go/) presented during brainstem measurements occurred with a low probability during the training session, the experimental manipulation we used precludes that results were driven by stimulus statistics34 or even by repetition suppression28. In fact, the PG was passively exposed to the same stimulus material as the TG, however, without showing a modulation of brainstem responses in pre-post comparisons. In addition, since brainstem changes were restricted to the solely discriminative physical attribute enabling to distinguish the trained stimuli, namely F1, results clearly point to feature-specific changes possibly reflecting increased neural efficiency28. This perspective is further supported by the negative correlation we revealed within the TG between percent f1 signal change and behavioral improvement during the training session. \n  Neural adaptation constitutes an intrinsic organizational property of the auditory system across the entire hierarchical tree, ranging from the periphery to the auditory cortex (for a review consider49). In this context, it is noteworthy to mention a previous EEG study targeting at evaluating the encoding of statistical regularities while participants learned to segment complex tone patterns embedded in concatenated sound sequences. Interestingly, the authors revealed decreased brainstem responses to the patterned compared- to a pseudo-random condition after only fifteen minutes of task34. However, by looking at brain responses of the single participants, Skoe and colleagues34 noticed that neural adaptation and facilitation can go hand in hand with remarkable inter-individual differences. Furthermore, the authors revealed a positive relationship between brainstem physiology and behavior, such that better performance was related to greater neural enhancement. Notably, our results are comparable with those of Skoe and colleagues34 in that the TG demonstrated decreased f1 magnitudes after short-term learning compared to the PG. Otherwise, in contrast to Skoe and co-workers, we revealed a negative instead of a positive relationship between the magnitude of brainstem responses and behavioral improvement. From a physiological perspective, the adaptation we revealed at the processing level of the brainstem can be explained at least by three different phenomena. The first possibility is that short-term training may have altered the response properties of brainstem neurons by uncoupling neural entities that were not relevant for discriminating task-specific acoustic features, resulting in activation of fewer neurons, and consequently neural adaptation28. A second possibility is that the observed brainstem changes may have been indirectly mediated by performance feedback. In fact, since in the present study only the TG received such a feedback, it is thoroughly possible that reward and motivation may have modulated brainstem activity. This perspective is supported by previous work showing that the human reward system is responsive to high-order rewards (i.e., intellectual, artistic, or altruistic pleasures)31 and that feedback confirming reward expectation can modulate activity in auditory-related brain regions32, 33. Finally, since active learning requires a stronger engagement of attention functions compared to passive listening, we cannot rule out that this variable may have played a role in mediating neural adaptation29, 30. Such an influence of attention could, for example, have been mediated by the cortex through corticofugal projections. In fact, such a contribution of the cortex to auditory learning mechanisms via the corticofugal system has previously been demonstrated in animals by using both ablation and pharmacological interventions49, 50. \n  A disadvantage of the EEG technique is that it does not enable to exactly determine the specific origin of the brainstem signal measured. However, currently there is evidence showing that neurons situated in the inferior colliculi are highly frequency-selective51, 52 as well as sensitive to the direction of frequency modulation53, 54. Since the TG was specifically trained to recognize subtle F1 signal changes only in one direction (i.e., always from/gu/to/go/, in the range between 364�480?Hz), we may speculate whether this specific experimental manipulation may have altered the response properties of neurons being selective to the direction of frequency modulation or rather frequency-selective neurons per se. In addition, since we did not reveal group differences in stimulus-response cross-correlations (i.e., lag and signal tracking), results suggest that during short-term training the brainstem is more likely prone to change its response properties to the spectrum of the trained stimulus attribute than to the waveform periodicity. This result is somehow in opposition with those previously reported by Russo and colleagues11 who revealed an increased temporal alignment of FFRs after training, as reflected by increased quiet-to-noise inter-response correlations. However, in this previous work the authors measured children with learning disabilities that were trained for a much longer period of time (namely 35�40?hours) compared to the present work. The same is true for the work of Anderson and colleagues55 where the authors evaluated the impact of an 8 weeks computer-based auditory training program in elderly subjects, and reported earlier brainstem peak latencies in both quiet and noise conditions after treatment. Taken together, these previous results substantiate the suspicion that brainstem changes in timing parameters may necessitate longer training periods."
1695972,2.0," MMN responses \n  A further goal of this study was to evaluate the functional malleability of the auditory cortex as indexed by altered MMN responses. In addition, based on previous studies indicating that neuronal entities which are sensitive to temporal and spectral acoustic attributes lie side by side in the auditory cortex56, we evaluated putative transfer effects57 from phonetic discrimination training to temporal aspects of speech processing. Reconstructed sources revealed MMN maxima originating from posterior superior temporal areas across groups (i.e., TG and PG), conditions (i.e., spectral and temporal manipulation), and time points (i.e., T0 and T1). This finding is in line with previous literature58 and points to a main contribution of the auditory cortex to MMN responses. In the present work, we did not reveal group differences in the modulation of MMN responses (i.e., MMN area and latency) as a function of treatment, leading to suggest that the auditory cortex was not specifically modulated by training. Interestingly, previous training studies consistently revealed increased MMN responses that were accompanied by an improved behavioral performance, however, especially after multiple training sessions lasting several days or weeks22, 59, 60. In particular, Ylinen et al.60 measured native Finnish (i.e., quantitative language) and English speakers before and after 10 training session of 25?minutes each consisting of learning to discriminate spectral and temporal cues of English vowels. As a main result the authors reported that after training the Finnish speakers were better able to discriminate spectral vowel cues, as reflected by increased MMN responses. In a further EEG study, Tamminen and colleagues59 applied a three-day phonetic-listen- and repeat training in a sample of Finnish speakers who learned voicing contrasts in fricative sounds (i.e., fricatives are not differentiated by voicing in Finnish) and revealed significantly increased MMN responses after the second but not the first training day. Taken together, these previous results lead to suggest that functional changes in the auditory cortex can most reliably be induced by multiple training sessions. Therefore, we may speculate whether a consolidation period is necessarily required for inducing detectable plastic changes in the auditory cortex61, 62. \n  An alternative explanation that may account for the apparent insensitivity of MMN responses to training is that the constant serial order of the cortical and subcortical EEG measurements (i.e., FFRs were always collected first) may possibly have blurred neural facilitation through a superimposed signal adaptation. However, since between the two measurement points (i.e., T0 and T1) the two groups were additionally exposed to acoustic stimulation for one hour, we should have observed such an effect in pre-post comparisons (i.e., a significant percent MMN change against zero), irrespective of group affiliation. Finally, based on the fact that phonetic discrimination learning is an active perceptual process that operates under the influence of attentive functions, future training studies should evaluate short-term changes in the auditory cortex by combining active and passive oddball paradigms."
1695972,3.0," Cortical-subcortical coupling mechanisms \n  To the best of our knowledge, until now only four studies conjointly recorded FFRs and AEPs while participants were exposed to CV syllables36, 37 or vowels15, 35. In particular, Musacchia and colleagues36 measured musicians and non-musicians while participants were repeatedly exposed to the syllable/da/, and reported a positive relationship between subcortical f0 amplitude and cortical P1-to-N1 slope. Otherwise, Bidelman and colleagues35 measured young and older adults while the participants categorized vowels that spanned a perceptual continuum from/u/to/a/and revealed that older adults were characterized by slower and more variable speech classification performance than younger listeners. This differential behavioral performance was reflected by reduced brainstem amplitudes, increased cortical AEPs, as well as by a negative relationship between f1 and cortical N1/P2 amplitudes. In a second study of the same group15, the authors recorded cortical and subcortical brain responses in older adults with and without music training while the participants categorized vowels along a continuum. Even though the authors did not find between-group differences in terms of cortical (i.e., P1-N1-P2 complex) or subcortical (i.e., f0 amplitude) brain responses, musicians showed a closer relationship between neural activity and behavioral performance. Finally, Parbery-Clark et al.37 investigated the effect of background noise on both brainstem and auditory cortex activity, and reported a relationship between subcortical response fidelity and cortical N1 magnitude that was predictive of speech-in-noise perception. In the present work, we did not find evidence for a relationship between auditory cortex and brainstem changes as a function of training. However, this may rather be a byproduct of unmodulated MMN responses as a function of training rather than an evidence for the inexistence of cortical-subcortical coupling mechanisms. In this context, it is also important to mention that our experimental design profoundly differed from the previous studies mentioned above. In fact, Musacchia and colleagues36 as well as Bidelman et al.15 measured musicians, a specific group of subjects that has previously repeatedly been shown to constitute a suitable model for evaluating the influence of long-term training on auditory processing16, 63, 64. Otherwise, the group of Parbery-Clark37 evaluated cortical-subcortical coupling mechanisms in normal hearing young adults while performing a speech-in-noise perception task, an experimental condition which is well known to place stronger demands on cognitive control mechanisms that have a modulatory influence on brainstem activity through the corticofugal system34."
2004704,0.0,"Cattle in SA are normally not affected by BTV outbreaks. Clinical signs have been reported in only two outbreaks of BT in cattle: in 1996–1997 in the Delareyville District in the North-West Province and in Donkerhoek in Bronkhorstspruit District, Gauteng Province (Barnard, Gerdes & Meiswinkel 1998). The prevalence of BTV in cattle in SA is, however, not known. To obtain BTV antibody negative cattle for this study and in a recent study by Steyn et al. (2015), 2168 cattle were bled in different geographical regions in SA (Gauteng, Mpumalanga and North-West Province) and a total of 96.5% of the cattle tested positive to BTV antibodies. It is therefore clear that although cattle have a high infection prevalence, they do not normally show any clinical signs and consequently are also not, in general, vaccinated against BTV in SA. It can therefore be concluded that cattle can be considered amplifying or maintenance hosts of the virus (MacLachlan et al. 1994). The trial was conducted during winter when midge numbers decrease significantly, therefore minimising the risk of natural infection with BTV."
2004704,1.0,"During this trial, cattle showed no adverse effects of stress, with the exception of two of the cattle that were inoculated with the vaccine and the wild-type virus and developed a mild fever and loss of appetite for 1 day. Animals were viraemic from 2 to 39 days post-infection. No virus could be detected in four animals after day 33 post-infection using virus isolation, but two animals were viraemic up to 39 days post-infection. Only one of the animals with the prolonged viraemic period showed some mild signs as discussed earlier. This is in agreement with previous studies in which it has been reported that cattle can be viraemic for periods between 14 and 100 days post-infection (Sellers & Taylor 1980; Singer, MacLachlan & Carpenter 2001)."
2004704,2.0,"When the isolated viruses were compared to each other and to the vaccine strains using PAGE, the majority of the samples were similar to BTV 4 the wild-type strain. This might be because BTV 4 was inoculated intravenously resulting in high titres of BTV 4 in the blood compared to the vaccine serotypes that were inoculated subcutaneously according to the guidelines of the manufacturer. Another reason might be that the fastest multiplying viruses showing plaques on cell cultures were preferentially selected; thus, the virulent field strain was isolated rather than the attenuated vaccine strains (Dungu et al. 2004)."
2004704,3.0,"This study illustrated the generation of reassortant viruses between two vaccine strains of BTV in cattle. Segment 8 of MLV BTV 8 reassorted with MLV BTV 9. Segment 8 codes for the NS 2 which is mainly responsible for assembly of the virion (Kar et al. 2007). It can therefore be hypothesised that the reassortant virus described in this study may replicate faster to facilitate survival. Reassortment is not limited to vaccine serotypes or closely related serotypes. Maan et al. (2012) illustrated that reassortment can occur between unrelated viruses, for example, from the western and eastern lineages. Whole genome analysis of a serotype 2 isolate from India illustrated that segment 9 was unique and belonged to the eastern topotype, while segment 5 belonged to a western topotype. Another example from India was the reassortment between a field isolate of BTV 21 and 16. Whole genome sequencing confirmed segment 6 of BTV 21 clustered with BTV 16 and showed a 97.6% similarity (Shafiq et al. 2013). A study in 1987 illustrated reassortment between two field virus isolates after a Holstein Bull was inoculated with BTV 11 and 17. The isolated viruses were compared with electrophoretic profiles and 16 unique profiles were identified (Stott et al. 1987). More recent studies in Europe illustrated how BTV 16, a field isolate from Italy, and BTV 2 vaccine strain exchanged segment 5 (Batten et al. 2008) and the circulation of a BTV 6 MLV strain was detected in the eastern Netherlands and later in parts of Germany in cattle that displayed mild non-specific clinical signs of BT (Maan et al. 2010)."
2004704,4.0,"Whole genome sequences were performed on 116 BTV isolates from Europe, the Mediterranean region and African countries collected during 1958–2012. The isolates were compared to four MLV serotypes and compared to the 26 serotypes available on GenBank, which revealed reassortment between field isolates and vaccine strains as well as reassortment between field isolates (Nomikou et al. 2015)."
2004704,5.0,"The impact of BTV reassortment on the epidemiology of the disease in SA has not been studied, but reported studies indicated that it is possible that new emerging viruses can infect novel species (Purse et al. 2005). In a recent study by Weyer et al. (2017), it was confirmed that individual outbreaks of African horse sickness (caused by AHSV) in the Western Cape Province, SA, were caused by virulent revertants of the virus. Outbreaks were caused by AHSV serotype 1 live-attenuated vaccine and reassortants with genome segments derived from AHSV serotypes 1, 3 and 4 from a live-attenuated vaccine used in SA."
2004704,6.0,"It is not possible to predict to what extent the characteristics of new emerging viruses can change and what the limitations of such changes will be. The new virus can be more pathogenic especially if reassortment results in reversion to virulence, can cross the placenta increasing its overwintering capabilities, or susceptibility to the vector resulting in more efficient spread of the virus (reviewed by Wilson, Darpel & Mellor 2008)."
342394,0.0,"Bluetongue disease constitutes one of the major veterinary problems in sheep and North American white-tailed deer [12,13]. However, in focal areas of endemicity, goats and cattle develop subclinical infection [6,8,10]. In our laboratory, a lot of research efforts have been made to facilitate rapid molecular detection and differentiation of orbiviruses, including BTV, in susceptible native breeds of livestock [11,13,15,17,24,27-33]."
342394,1.0,"Previous epidemiological surveys showed high prevalence rates for BTV infections in Iran (93.5%) and Southern Turkey (88%) [34,35]. In Sudan, earlier serological surveys indicated that BT infection is generally widespread and occurs in all domestic ruminants, with as high as 61.5% prevalence rate among sheep in Juba, South Sudan [22]. A subsequent serological survey for antibodies against bluetongue in cattle in Khartoum State showed high prevalence rate of 51.1% among the examined animals [23]. In the present study, a seroprevalence of BTV infection in cattle of North Kordufan (19.4%) is markedly lower than previously reported prevalence rates compared to other states of Sudan. In this study, the prevalence of BT disease is comparable to those reported amongst sheep in El-Dien (20%), Western Sudan, and Atbara (21.9), Northern Sudan [22]. The presence of BT disease in Sudan and the risks these infected cattle pose for native breeds of sheep, necessitate the importance of an improved surveillance system for this viral pathogen in Sudan. In the present investigation, the final models of BTV seropositive cattle indicated that only three independent risk factors were statistically significant. When assessing age as risk factor, there was a significant difference between the BTV infection rate and the age of the animal. It was shown that the calves started to get infected with BTV after the age of 2 years. At this age, the animals are usually released onto the pasture for grazing, where they are likely to be exposed to infected vectors and subsequent BTV infection. We believe that the association of BTV infection rate and age is probably attributed to frequent exposure of older cattle to infected Culicoides vectors. In contrast, young calves (<2 years) are usually kept indoors and are well taken care of by the animal owners for protection against infectious diseases particularly, the insect and tick-borne infections [36]. Our result is in agreement with previous epidemiological surveys, which reported higher risks of older animals for BTV infections [37]. It should be noted that the BTV-specific antibodies detected among cattle in North Kordufan State indicate natural infection as there is no vaccination program for the disease in the country. In addition, all cattle included in this study are aged over one year. Therefore, it is assumed that detected antibodies no longer persisted and that antibody indicated direct exposure to BTV. In addition, there was association between body condition and BTV seropositivity. In our study, poor physical conditions are associated with highest prevalence of infection. It is probable that emaciated cattle have the tendency to attract the infected Culicoides insect vector of BTV. However, if the status of emaciation causes infection by BTV or vice versa needs further investigation and cannot be derived from this study. Regarding the management risk factors, there was a significant difference between the use of insecticide and the seropositivity to BTV infected cattle. It is, therefore, recommended that routine application of insecticide or insect repellents be considered to prevent BTV infection in susceptible livestock. In contrast, the risk assessment studies indicated that there was no significant difference between BTV infection and the rest of the individual or management risk factors included in the study. It is worth mentioning that gender has no significant difference for BTV infection among male and females as both sexes are equally infected with BTV. Likewise, there was no significant difference between localities and BTV infection rate, suggesting wide distribution of the Culicoides vector all over the localities of North Kordufan State. Highest and lowest rates of BTV positivity were recorded in the localities of Shiekan (15%) and Ennuhud (27.5%), respectively. The high level of BTV positivity in Ennuhud is attributed to the substantial rainfall events and development of irrigation projects, which provide suitable habitat for the insect vector in this locality. However, the locality of Shiekan is situated in the semi desert zone and hence has restricted rainfall."
2087253,0.0,"Infection with BTV is common in a broad band across the world, which until recently stretched from latitude ~35°S to 40–50°N. Since the 1990s, the range of BTV has extended considerably north of the 40th and even the 50th parallels in some parts of the world (Maan et al. 2007; Coetzee et al. 2012)."
2087253,1.0,"All attenuated BTV strains have pronounced reactogenic properties. These properties are characterized by the appearance in the vaccinated animal of a quite moderate to severe reaction temperature, between 6 and 10 days after vaccination. The response to the vaccine is followed by viremia. These phenomena are regarded as positive signs of infection, indicating the development of an immune response (Kercher et al. 1957)."
2087253,2.0,"One of the most important steps when preparing a live-attenuated vaccine is the assessment of its level of attenuation in the target animals (sheep and newborn mice). These models, which are used to evaluate the attenuation of BTV, are sufficient to test the attenuated strains (Franchi et al. 2008). Therefore, we determined the levels of attenuation of the BTV strains with 10 serial passages in sheep. We evaluated the reversion of the two vaccine strains by passaging them 10 times through susceptible sheep, and thus showed that the pathogenicity of these attenuated strains was not restored. However, these strains replicated in the sheep and could be isolated from the blood of the animals 2–12 days after vaccination."
2087253,3.0,"The MLV vaccine administered at doses not exceeding 2.0 log10 TCID50/mL, produced long-lasting viremia in the sheep, whereas virulent strains accumulated in the blood at levels up to 5.50 log10 TCID50/mL. Although this viremia makes the MLV more immunogenic, because it continues to producing an immune response long after the initial injection, the possibility of reversion to virulence cannot be ruled out given the limitations of the present study. In particular, previous research has shown that the virus can revert to virulence after infecting its natural vectors (Batten et al. 2008). In this case, the viremia produced by the vaccine strain persisted longer than that induced by the pathogenic strains. Vaccination with MLV and inactivated viruses produces neutralizing and group-specific antibodies (Oura et al. 2009; Hamers et al. 2009) protecting the vaccinated animal against challenge. Live-attenuated vaccines usually elicit a strong immune response, with the development of antibodies, and are usually efficacious after a single dose (Patta et al. 2004; Dungu et al. 2008)."
2087253,4.0,"Neutralizing antibodies have been shown to be an essential component of the protective immune response against BTV (Huismans et al. 1987; Roy et al. 1990) and similar titers, (1.5–2.5 log2) have been observed after vaccination with killed commercial vaccines in ruminants, which were ultimately protected from BTV-8 challenge (Bréard et al. 2011). Our experimental data are consistent with those of other researchers. Therefore, we have established that 7 days after vaccination with the attenuated vaccine, virus-neutralizing antibodies to BTV are present in the sera at a titers of 1.0 log2. This antibody titer is protective against the homologous virulent strain of BTV. Although further research isrequired, particularly regarding the safety of the vaccine in pregnant sheep, this bivalent vaccine has been shown to be safe and efficacious, and will be a valuable tool in preventing the spread of bluetongue to Kazakhstan."
761719,0.0,"It is worth mentioning that the situation of diagnosis of this virus in neighboring countries and the Middle-East (except Turkey and Occupied Palestine) is not better than our country. In such country as Saudi Arabia, Syria, Yemen and Pakistan, the presence of virus has been documented only relying on serological tests.6,7,16 According to recent studies, there is an evidence of occurrence of BT disease in tropical and subtropical countries (such as Iran). In such areas generally, the disease appears sub clinically and does not attract attention. In such circumstances, the presence of the virus often confirms via serological evidence. It should be mentioned that in such foci, in spite of unrevealed disease and manifestation, sometimes sudden incidence of acute forms of the disease sustain a loss.17-19"
761719,1.0,"In Iran, identification of BTV in suspected cattle and sheep based on clinical manifestations was performed. However, there are some limitations and problems. First, it should be considered that clinical expression of BTV regarding strain and virus intensity, cattle race and environmental condition varies from per acute to subclinical. Second, symptoms of disease in sheep can be mistaken with those of other viral much diseases and even some of the non-viral diseases.19 The objectives of the used C-ELISA test were both to confirm the BTV infected status of sheep in suspicious holdings in west and northwest of Iran (Diagnostic test). Thus, its use has been more and more often abandoned and replaced by C-ELISA tests, which are rapid and easier to use, more sensitive and specific.20 In this study, the prevalence rate of BT antibodies in sheep was 40.87%. In other countries, the prevalence was as follows: 21.40% in Kazakhstan,6 29.59% in southeastern Turkey,14 62.69% in India,21 54.10% in Saudi Arabia,22 and 48.70% in Pakistan.16 Climatic factors play an important role in the occurrence of BTV infection in animals and influence the size of vector populations and periods of their seasonal activity.23 An analysis of climatic data was used to model the potential distribution of Culicoides imicola in Europe, predicting that might have spread from Spain, Greece and Italy to some areas along the Croatian coast as well as to the coastal areas of Albania, Serbia, Montenegro, Bosnia and Herzegovina.24-27 Although more than 1000 species of Culicoides spp. are known worldwide, relatively few of these species have been incriminated as vectors of BTV.28 Culicoides from western Turkey in relation to bluetongue disease of sheep and cattle was reported.29 Species of vector insects that transmit BTV differ amongst regions, and are especially poorly characterized in the portions of Asia that are devoid of C. imicola, the traditional African-Asian vector of BTV.2,7,29 Animals entering the western border of Iraq to Iran can be a cause of high titers of antibodies against the bluetongue virus in west of Iran. The prevalence of BT antibodies in sheep in the northwest of Iran was 33.75%. Although BTV infection of sheep is clearly widespread in northwest of Iran, the specific virus serotypes and vector insects that occur within the region remain uncharacterized, as they are in adjacent countries such as Kazakhstan.6 The highest prevalence of BTV in sheep was in West Azerbaijan (64.86%) and Ilam (42.65%)."
761719,2.0,"The results show that BTV infection is present in live-stock animals in province. Some similar studies have been carried out in different area of country that mostly reported prevalence in the similar study. For example, 76.44% in East Azerbaijan,30 34.70% in West Azerbaijan,7 33.33% in Kerman,31 45.90% in Kurdistan, 32 53.37% in Isfahan.33"
761719,3.0,"Furthermore, BTV infection in sheep apparently is largely subclinical.7 During the BTV epidemics in Europe in 2008, Williamson and colleagues34 considered clinical signs for diagnosis of the disease. The results showed low specificity of this method. These researchers believe that sometimes clinical signs of BTV in sheep are mistaken with those of such diseases as foot and mouth disease (FMD), ovine rinderpest, contagious ecthyma, and hemonchosis.35,36 Iran located in the southeast of Europe makes it an important potential source of BTV strains and serotypes that might incur into adjacent areas.5,37"
761719,4.0,"In conclusion, seroprevalence of BTV has been never before reported in many area of Iran (Ardabil and Ilam). As per our knowledge, this is the first study was evaluated the prevalence of antibodies to BTV in sheep in some provinces of Iran. The results showed that a high incidence rate of BT antibodies has been detected in sheep in Iran that indicate serological evidence of exposure to infection was widely distributed in some provinces. There are no restrictions on the movement of animals from one region to another within the country. Thus, outbreaks may also occur due to transportation of animals. Consequently, a well-defined control strategy for preventing and controlling the BTV may be based not only on vaccination plans and vector eradication but also restriction on the movement of animals from one region to another within the country. As a vaccination for BT is not implemented in Iran, a seropositive result indicates BT infection in domestic population.4,31 Further researches on the isolation and identification of BT virus in sheep are encouraged."
921211,0.0,"In this study, we showed that NS4 is an IFN antagonist and a virulence factor for BTV. BTV NS4 deletion mutants replicate as efficiently as wild-type viruses in cell lines that lack a competent innate immune system and are lethal to IFNAR−/− mice (8). In contrast, here, we demonstrated that NS4 facilitates BTV replication in vitro in IFN-competent cells and in vivo in sheep, the animal species most affected by bluetongue."
921211,1.0,"BTV induces a type 1 IFN response in vivo and in vitro (38,–42). Our data strongly suggest that NS4 is required by the virus to help overcome the cellular innate antiviral responses. Reporter assays revealed that NS4 can inhibit transcription from a variety of mammalian and viral promoters while having little if any direct impact on mRNA editing or translation. However, NS4 does appear to influence the extent of the host IFN response, as supernatants of primary sheep endothelial cells infected with BTV8ΔNS4 contained more IFN than supernatants collected from BTV8wt-infected cells, a result that may explain the more efficient growth of BTVwt in IFN-competent cells. These results were supported by RNA-seq analyses of nascent RNA isolated from infected and mock-infected cells. Genes relating to the innate immune system were among the most upregulated genes in BTV8ΔNS4-infected cells, even if some of them were also found to be upregulated in BTV8wt-infected cells, suggesting that NS4 is not by itself sufficient to entirely inhibit the host IFN response. Most of the genes that were specifically upregulated in BTV8ΔNS4-infected cells compared to BTV8wt-infected cells were ISGs, including those encoding well-characterized antiviral factors."
921211,2.0,"Overall, the data obtained in this study indicate that NS4 exerts its effect upon the antiviral host response at a point upstream of RNA editing and translation. Interestingly, NS4 possesses features of a transcription factor of the bZip family, with a basic domain followed by a leucine zipper motif (67), and it has been suggested to have the ability to bind DNA, based on DNase protection assays (9). Our reporter assays, showing that NS4 inhibits transcription from a variety of promoters, including IFN-β and an ISRE-containing promoter, suggest that the protein may inhibit cellular transcription. In a previous study, we showed that NS4 displays nucleolar localization in infected cells (8). Thus, a direct interaction of NS4 with transcription factors and/or chromatin resulting in the downregulation of cellular transcription is a possible mechanism of action for the protein. In support of this hypothesis, we found that the NS4 protein of a BTV-1 strain that did not show nucleolar localization was not able to modulate gene expression in reporter assays. It is also possible that NS4 inhibits IFN-β activation before transcription. Nuclear translocation of IRF-3 and NF-κB appeared to occur at similar levels in both BTVwt- and BTVΔNS4-infected cells, suggesting that NS4 does not prevent PAMP recognition by host cells. However, it is possible that IRF-3 or NF-κB could be inhibited by NS4 in the nucleus. It is important to note that, in a previous study, we showed that NS4 confers a replication advantage on BTV in cells pretreated with type I IFN, highlighting its ability to counteract the innate immune response after IFN activation. Hence, it is less likely that NS4 blocks PAMP recognition and subsequent signaling. More studies will be necessary to fully dissect how NS4 counteracts the host innate immune system. Obviously, NS4, like other nonstructural proteins of different viruses, might have different functions and antagonize the IFN response via multiple mechanisms. For example, the NS1 protein of influenza A viruses inhibits IFN production by blocking IRF3 and NF-κB activation and mRNA maturation (68, 69)."
921211,3.0,"It has been known for at least 3 decades that BTV induces host protein synthesis shutdown (22, 64, 65). Recent studies suggest that BTV NS1 favors viral RNA translation and therefore competes with cellular protein synthesis (22). A key difference between mammalian and BTV transcripts is the lack of a poly(A) tail. The NSP3 protein of rotaviruses (also members of the Reoviridae) acts in a similar fashion and outcompetes the poly(A) binding protein of cells, thus biasing translation of viral transcripts (70, 71). It remains possible that a similar mechanism may exist for BTV. Our data, obtained in metabolically labeled cells, confirm that BTV induces host protein synthesis shutdown and also establishes that this occurs largely independently of NS4. Hence, other BTV proteins might also play a similar role in the shutoff of host cell protein synthesis or function in different ways as IFN antagonists. For example, NS3 is believed to modulate IFN induction downstream of RIG-I and upstream of IKKε activation. Therefore, NS3 and NS4 may potentially act synergistically to counteract the innate immune system (28). In addition, it has also been shown that BTV inhibits the IFN signaling pathway by downregulating key components of the JAK/STAT pathway: JAK1 and TYK2 (43). The downregulation of JAK1/TYK2 may be the result of a specific interaction with a BTV protein and/or due to the general host protein synthesis shutdown induced by the virus. We did not find either of the two genes to be differentially expressed in BTV8wt- or BTV8ΔNS4-infected cells."
921211,4.0,"In this study, we showed that BTV8ΔNS4-infected sheep display lower levels of viremia, lasting for shorter periods, than the viremia observed in animals infected with BTV8wt. The levels and duration of viremia in infected animals are essential factors for successful transmission between the mammalian hosts and the intermediate arthropod vectors. As mentioned above, BTV infection can result in a lethal hemorrhagic fever in some animals, while in others, the virus induces a mild febrile illness or even a clinically unapparent infection. The data obtained in this study provide evidence that viral proteins modulating the host innate immune response play a significant role in viral pathogenesis. This study lays the foundations for a deeper understanding of the mechanisms by which BTV antagonizes the IFN system, in turn helping us to define the molecular determinants of BTV virulence."
1200121,0.0,"From this study it is revealed that antibodies to BTV is highly prevalent in sheep and goats of south west Ethiopia. The results of the present study revealed the first confirmation of BTV antibody in sheep and goats from South west Ethiopia. Although, seropositivity to bluetongue virus has been reported earlier in small ruminants in Ethiopia, it has covered only a small region without mentioning the association of risk factors [9, 10, 13]. In the present study 30.6% (129/4220 serum samples of small ruminants were found positive to antibodies to BTV. This findings is in agreement with the results of Gulima [10] who reported 34.1% seropositivity to BTV in indigenous sheep of Northwestern Ethiopia. The seropositivity to BTV in small ruminants (28.6%) in India [14], domestic livestock (23%) in Kazakhstan [15], sheep in Iran (35.9%) [16], sheep in Iran (34.93%) [17], goats in Iran (39.47%) [18], small ruminants in Turkey (29.5%) [19], sheep in Iran (37.7%) [20] and again sheep in Iran (33.75%) [21] has also been reported from other parts of the world."
1200121,1.0,"The present finding is found to be relatively lower than the previously reported seroprevalence to BTV in small ruminants by different authors in different countries such as 41.17% in small ruminants in Southern Ethiopia [13], 46.67% in sheep in central Ethiopia [9], 78.4% in small ruminants in Grenada [22] and 45.7% in small ruminant in India [23]. However, 30.6% seropositivity to BTV in small ruminants in the present study is comparatively higher than the reports of other researchers like 6.57% in sheep in Southeast Iran [21], 6.96% from small ruminants (13.7% in goats and 5.70% in sheep) in Algeria [24], 2.63 and 5.3% in small ruminants in Kerala and Karnataka (India) [25, 26] respectively. In the present study, the difference in the seropositivity might be due to difference in animal species, age of sampled animals, immune status of sampled animals, agroclimatic zones, ecology, and types of culicoides vectors."
1200121,2.0,"In the current study, seropositivity to BTV in goats was higher than sheep. Higher seroprevalence among goats compared to sheep indicating that goats play an pivotal role in the epidemiology of BTV. Whereas, sheep which are highly susceptible animals to BT show distinct clinical signs and die of the disease. It is also an established fact that goats with minimum clinical manifestation maintain high titer of BTV and may be the potential source of infection to other susceptible animals [13, 14]."
1200121,3.0,"It has been recorded that BT virus infection is increased with increasing age of animals and this is in agreement with the reports of Yilma and Mekonnen [13]. Assessing age as a risk factor, there was a statistically significant association (P < 0.05) with the prevalence of BTV specific antibody in the study animals. It was shown that the younger animals started to get infected with BTV after the age of included at category of adult level. At this age, the animals are usually released into the pasture for grazing, where they are likely to be exposed to infection by vectors and subsequent BTV infection. Young age groups are usually kept indoors and are well taken care of by the owners from contracting infectious diseases, particularly the insect and tick-borne infections [27, 28]."
1200121,4.0,"This study was also assessed the effect of agro ecology on the seroprevalence of the BTV antibody and found that animals at low altitude are more prone to BTV infection than high altitude. This is in agreement with different authors [18, 22]. The prevalence correlated with the probable distribution of the Culicoides vector. In addition environmental changes can influence the incidence, distribution and evolution of infectious diseases, particularly those transmitted by arthropod vectors [29]."
921401,0.0,"Most infections of susceptible hosts by pathogenic viruses result in clinical manifestations that can vary greatly in their severity. For some viruses, such as avian influenza virus, for example, low and highly virulent strains are distinguishable by clear genotypic differences (76). Nevertheless, in some circumstances, even infection of susceptible hosts with highly pathogenic viruses can result in mild or unapparent clinical symptoms."
921401,1.0,"Bluetongue is a disease characterized by a highly variable clinical spectrum (21,–24). Understanding the basis for this variability is complicated by the fact that BTV exists in nature as many diverse strains representing different serotypes, topotypes, and reassortant viruses often cocirculating in the same geographical area. In addition, BTV can infect a variety of ruminant species, each with different genetic and immunological backgrounds. Furthermore, BTV is transmitted by different species of Culicoides in diverse ecological contexts. There have been several studies concerning naturally occurring bluetongue or experimentally induced disease, clearly indicating that factors related to both the mammalian host and the virus can influence the outcome of BTV infection (55). However, it is not always straightforward to compare data from different studies. Thus, the weight given to different host or virus factors in determining the clinical outcome to BTV infection can differ in heterogeneous ecological or experimental settings."
921401,2.0,"In this study, we dissected both host and virus factors that can affect the clinical outcome of BTV infection. The use of a uniform experimental framework has allowed us to rigorously interrogate both experimental questions addressed in past studies (55), as well as to explore hitherto unanswered questions. First of all, as suggested previously (24, 32,–36), we confirmed that while both sheep and goats are fully susceptible to BTV (in this case BTV-8) infection, the former are more susceptible than goats and more likely to develop clinical disease. The levels of viremia in BTV-infected goats were not different from (if anything, higher than) those observed in infected sheep. These data confirm that BTV is able to replicate to high levels in goat tissues but cellular damage, either induced by the virus or the host immune responses, does not likely occur. We do not know if goats would be more susceptible to disease if we had used higher infectious doses. We have used 2 × 106 PFU of BTV in our experimental infections, and this is likely far more infectious virus than is transmitted in nature by infected midges. In addition, studies in sheep using as little as 101.4 TCID50 were able to induce infection in this animal species (66). In two previous studies, also using BTV-8 isolates from the Netherlands, some of the experimentally infected goats developed mild clinical signs, fever, and viremia (34, 36). However, in both studies, goats were infected intravenously (34, 36), and in one of them animals were infected at day 62 of gestation (36). Another study used BTV-4, which was isolated in embryonated chicken eggs and passaged seven times in BHK21. Only 1 of 11 goats (of two different breeds) infected with this virus showed transient pyrexia, but at the same time 10 of 12 inoculated sheep did not show fever or signs of disease either (32). Thus, this study confirmed that the mammalian host species is certainly one of the main factors that determine the clinical outcome of BTV infection."
921401,3.0,"We did not find major differences in the susceptibility of sheep breeds from the Mediterranean area (Sardinian and Italian mixed breed) and Northern European breeds (Dorset poll) to bluetongue, despite their distinct geographical, historical, and breeding backgrounds (47). Thus, variations in the susceptibility to bluetongue of different sheep breeds might not be as pronounced as originally thought. It is also important to stress that bluetongue itself was first discovered in European breeds imported into South Africa (77). Those breeds showed a higher susceptibility to bluetongue than local animals, although the influence of herd immunity on the latter could have also played a role. It is therefore difficult to weigh the influences of the host's genetic background, previous BTV exposure, or the insect vector on the susceptibility to the disease in that particular context."
921401,4.0,"We have also analyzed the influence of divergent viral serotypes, and closely related but distinct strains within the same serotype, on the clinical outcome of bluetongue. BTV-8NET2006 is considered to be a highly pathogenic virus (both in terms of morbidity and mortality) and the cause of one of the largest outbreaks of bluetongue in history (48,–51). However, in our experimental setting, we did not find any difference in virulence between BTV-8NET2006 and another serotype, such as BTV-2IT2000, which was isolated in Italy in the year 2000 from a naturally occurring case of bluetongue in sheep. Another study, comparing the virulence of BTV-1 isolated from Algeria and a 2006 isolate of BTV-8 from Belgium, concluded that the former was more virulent than the latter (78). Although in that particular study the cell culture passage history was not described and viruses were inoculated subcutaneously, it appears that the overall data suggest that in itself BTV-8NET2006 is not necessarily more virulent than other BTV serotypes, such as BTV-2 or BTV-1, that have been circulating in Europe in the last decade. It is likely that other factors, such as the rapid spread of the infection to an extremely large number of fully susceptible and naive hosts (never previously exposed even to heterologous BTV serotypes), contributed to the number of severe cases of disease observed during the Northern European outbreak caused by this strain of BTV."
921401,5.0,"The BTV-8NET2006 strain was isolated from samples collected at the beginning of the European outbreak of this virus. Since the original cases identified in 2006 in central Europe, BTV-8 moved in subsequent years toward several surrounding geographical areas (including southward). Interestingly, in Northern Italy and in Sardinia, BTV-8 (termed in this study BTV-8IT2008) was detected only at the serological level in a few animals, but it was not associated with clinical disease (G. Savini, personal communication). We showed conclusively in our study that BTV-8IT2008 was less virulent than BTV-8NET2006. BTV-8IT2008 accumulated several nonsynonymous mutations in structural and nonstructural proteins (including VP1, VP2, NS1, and NS2) already implicated in attenuation of tissue culture-adapted BTV-2, BTV-4, and BTV-9 (57). Thus, this study formally proves the appearance of less virulent strains during a BTV outbreak. The comparative smaller number of severe cases of bluetongue in areas where it is endemic might depend upon several factors, including the levels of herd immunity, the decrease in virulence of circulating BTV strains, and, possibly, the long-term selection of genetically resistant individual animals."
921401,6.0,"Finally, we further investigated the observation that experimental infection of sheep with blood collected from naturally occurring cases of bluetongue appears to induce, in general, more severe clinical cases compared to the disease induced in sheep infected with viruses isolated in tissue culture or embryonated eggs (20, 61). Indeed, we have confirmed in our experimental framework that sheep inoculated with BTV-8NET2007(blood) displayed a more severe disease and higher levels of viremia than those infected with the virus isolated in cell culture [BTV-8NET2007(1KC-2BHK)]. It is unlikely that factors present in the infected blood could be the cause of more severe clinical signs in sheep. Importantly, the highest levels of fever and the most severe clinical signs in sheep infected with BTV-8NET2007(blood) were observed between days 6 and 11 p.i., when the levels of BTV in the blood were at their highest."
921401,7.0,"Virus passaging in tissue culture can lead to adaptive changes in the viral genotype that could in turn affect viral virulence. However, we found only 2 synonymous mutations between the consensus sequence of BTV-8NET2007(blood) and the cell culture-isolated virus BTV-8NET2007(1KC-2BHK). Both mutations were present in approximately 10% of the variants of BTV-8NET2007(blood), and interestingly they were both selected in two independent experiments. It is possible that these silent mutations in some way affect viral virulence. In addition, the sequencing methods used did not cover the noncoding regions of each segment, and therefore we may have also missed other important mutations. However, overall there appears to be very little (or no variation at all) at the consensus sequence level (at least for BTV-8) of viruses isolated from blood or minimally passaged in cell culture. RNA viruses have the highest error rates (10−4 to 10−6 per nucleotide site per genome replication) of any microorganism due to their RNA-dependent RNA polymerase lacking proofreading activity during RNA synthesis (79, 80). As such, RNA viruses exist as a population of variants, genetically closely related but distinct from their consensus sequence. It is rational to argue that the opportunity to quickly adapt and generate diverse viral populations is critical for the survival of RNA viruses (74) in the face of selective pressures, including the innate and adaptive antiviral responses of the host. For example, poliovirus mutants with a high-fidelity polymerase (and thus low population diversity) display an attenuated phenotype in mice, despite possessing identical consensus sequences to the virulent wild-type viruses (81,–83)."
921401,8.0,"We found that BTV-8NET2007(blood) contained the largest number of high-frequency variants. However, when BTV-8NET2007(blood) was passaged in insect KC cells, the resulting viral population [BTV-8NET2007(1KC)] showed the overall highest number of variants, even higher (∼ 60%) than those in the blood before tissue culture isolation. A severe genetic bottleneck was observed after viral passaging in mammalian BHK21 cells, with the resulting viruses [BTV-8NET2007(1KC-1BHK) and BTV-8NET2007(1KC-2BHK)] showing the smallest degree of variability."
921401,9.0,"These data suggest that BTV virulence is affected not only by changes in the viral proteins selected at the consensus level but also by the genetic variability of the population as a whole. This hypothesis is also supported by previous observations made in a limited number of genes before the advent of deep sequencing (84, 85). In a study that analyzed segment 2 of a virulent strain of BTV-1, Gould and Eaton (84) showed that the consensus sequence did not change after a single passage in tissue culture that resulted in viral attenuation. In addition, Bonneau and colleagues (85) showed that the number of variants observed in segment 2 and 10 of plaque-purified BTV-10 increased during transmission of the virus between ruminants and insect vectors but without changes to the consensus sequence."
921401,10.0,"Thus, “flat” populations containing a relatively small number of variants appear to be less virulent than more variable populations."
921401,11.0,"In addition, our data also suggest that Culicoides cells might function as a natural source of new BTV variants. BTV is an arbovirus and as such must adapt rapidly to replicate in hosts as different as a warm-blooded mammal and insects. An increased variability of replication in Culicoides cells might allow BTV to adapt faster to different selective pressures present in the invertebrate and vertebrate hosts. These data also reinforce the notion that it is critical to avoid the use of modified live vaccines that induce even transient viremia in vaccinated animals. The transmission of vaccine strains in the Culicoides population might then lead to the emergence of “new” strains with the potential to revert to their original phenotype."
921401,12.0,"Our study has not taken into consideration factors related to the invertebrate host (e.g., species and sites and number of “infectious” bites) that could affect BTV pathogenesis. The insect host certainly plays a role in modulating the interaction between virus and the mammalian host, as some studies are beginning to suggest (86). It is possible that transmission of BTV by different species of Culicoides, in different geographical areas, could influence the pathogenesis of bluetongue in different ways. This is an exceedingly important area of research that will need to be addressed in the coming years."
2299383,0.0,"Sheep and cattle in the selected herds in this study were commonly exposed to BTV, concurring with previous studies in Zimbabwe. Musuka and Kelly (2000) recorded sero-prevalences of antibodies against BTV in sheep and goats of up to 100% in the north-east of Zimbabwe while Jorgensen et al. (1989) reported an overall sero-prevalence of 71% in indigenous goats at 25 farms across Zimbabwe. Prevalence rates of BTV antibodies, ranging from 64% to 84%, have been reported in sheep and goat populations in South Africa (Gerdes 2004). In North Africa, BTV sero-prevalences in Algeria were found to be only 14% in sheep and 29% in cattle (Madani et al. 2011). In 455 calves sampled in Kenya, BTV antibodies were found in 94% of these animals (Toye et al. 2013). Cattle rarely show clinical signs of BT, however, and are, therefore, not vaccinated in Zimbabwe (Musuka & Kelly 2000). It should be noted, however, that the strain of BTV serotype 8, which recently emerged in Europe, appears more pathogenic in cattle than is usually the case for BTV (MacLachlan & Guthrie 2010) so the situation in Zimbabwe may change. Consequently, animal health officials in Zimbabwe should continue to monitor the exposure rates of cattle to BTV and consider the possibility of emerging BTV strains with increased pathogenicity."
2299383,1.0,"Cattle sentinel herds recorded high sero-prevalences and sero-incidences of antibodies against EHDV in the selected Highveld populations in Zimbabwe. Antibodies against EHDV were detected in 64% of calves tested in Kenya (Toye et al. 2013) and in 38% of cattle and sheep sampled on Reunion Island (Desvars et al. 2015). The low sero-prevalence and sero-incidence rates against this disease recorded in sheep suggest that cattle, which are likely to be the preferred hosts of Culicoides vector species, as in other countries, may be a more natural host of this virus in Zimbabwe, compared to sheep. Whereas EHDV can cause a fulminant haemorrhagic disease syndrome in white-tailed deer, similar infections of ruminant livestock are usually subclinical or clinically inapparent. Nonetheless, it is now widely accepted that EHDV infections in some circumstances have resulted in disease in cattle (Maclachlan et al. 2015). Maclachlan et al. (2015) reported strains of EHDV serotypes 2, 6 and 7 as the apparent recent cause of a BT-like disease syndrome of cattle in northern and southern Africa, Reunion Island, North America and the Mediterranean Basin, including Algeria, Israel, Jordan, Morocco, Tunisia and Turkey. Affected cattle in North Africa and Israel exhibited BT-like disease signs when infected with EHDV serotype 7 (MacLachlan & Guthrie 2010). Ibaraki disease, an acute febrile disease in cattle caused by a variant of EHDV serotype 2, was first reported in the central and western parts of Japan in 1959 (Inaba 1975). The host and virus factors that may lead to the expression of EHD in cattle remain, however, undetermined (Maclachlan et al. 2015)."
2299383,2.0,"In Zimbabwe, the sero-prevalence of antibodies against EHDV was, however, lower than the sero-prevalence of BTV antibodies in both cattle and sheep, suggesting that domestic ruminants in this country are less susceptible to EHDV than BTV, or that the vector species of Culicoides, which transmit EHDV, are different to those that transmit BTV in Zimbabwe. Venter (2015) classified C. imicola as the most important vector of BTV to livestock in South Africa. In the Sudan, Mellor, Osborne and Jennings (1984) isolated BTV only from C. imicola while EHDV isolates were made from Culicoides schultzei group midges. Lee (1979) also reported the isolation of EHD group viruses from C. schultzei collected in Nigeria. Gordon et al. (2015) recorded C. imicola as the most abundant species trapped in Zimbabwe and only recorded very low numbers of Schultzei group midges."
2299383,3.0,"The highest prevalence of both BTV and EHDV in cattle was detected during the second rainy season (2000/2001). This was a season which recorded above-average rainfall (Department of Meteorological Services, Belvedere, Harare, Zimbabwe). During each rainy season, most seroconversions to the BTV and EHDV occurred towards the end of the season. Mellor (1990) states that this delay reflects the increase in vector numbers and the viraemia, which develops in the infected ruminants, providing a source of infection for further vectors. Large populations of Culicoides vectors have been reported previously in Zimbabwe in areas with high average annual rainfall and summer temperatures (Gordon et al. 2015; Musuka et al. 2001). Large catches of Culicoides have also been recorded in the warm, summer rainfall areas of South Africa (Venter 2015; Venter, Nevill & Van der Linde 1997). Coetzee et al. (2012) state that BT is commonly reported in South Africa in late summer in areas with high rainfall and after good rains. Rainfall data would need to be utilised in future studies to demonstrate the association between rainfall, Culicoides numbers and the sero-incidence of BTV and EHDV in Zimbabwe."
2299383,4.0,"In this study, the only BTV serotype identified was serotype 3. While this serotype has not been previously reported in Zimbabwe, Coetzee et al. (2012) reported that serotypes 1–6 are commonly encountered in neighbouring South Africa. Sghaier et al. (2017) recently identified serotype 3 in Tunisia while Desvars et al. (2015) attributed BTV-3 as the cause for a clinical outbreak in Merino sheep on Reunion Island. Previous studies have identified BTV serotypes 1, 2, 8, 10, 11, 12, 15, 16 and 18 from Culicoides vectors in Zimbabwe, although these studies were also restricted to samples from the Highveld region (Blackburn et al. 1985; Gordon et al. 2015). As yet, no data exist on the distribution of BTV or EHDV serotypes in the Middleveld or Lowveld plateaus of Zimbabwe."
2549955,0.0,"Several studies have modeled the effect of vaccination on BT spread and simulations have demonstrated that vaccination can be a highly effective means to control BT epizootics when a high level (>80%) of vaccine coverage is achieved [14], [15], [17], [18]. However, field studies investigating the effect of vaccination against BTV are rare. To our knowledge, our study is the first to have quantified vaccine induced reductions in the velocity of BTV spread using data from a real epidemic. After accounting for environmental factors known to influence the velocity of BT spread, vaccination divided by a factor 2 the mean velocity at which BTV-1 spread across the study area. Vaccination thus helped to slow down disease progression by decreasing the number of infectious hosts and vectors, and consequently the probability that infected vectors bite susceptible hosts in a non-contaminated area. Despite regulations on farm animal movements, BT has rapidly spread in Europe during the 2006–2008 BTV-1 and BTV-8 epizootics. Vaccination was the only efficient method that could stop BTV-8 and BTV-1 spread, and decrease the number of BT foci until apparent full eradication of BT among European livestock."
2549955,1.0,"Since previous studies have estimated the velocity of BTV-8 spread in France and the effects of environmental covariates [23], [24], a comparison of BTV-1 and BTV-8 epizootics is possible. In both cases, restrictions were imposed on animal movements. Regarding the velocity of BT spread, the mean value of velocity of spread was similar for both serotypes (5.4 and 5.6 km/day for BTV-1 and BTV-8, respectively). The first and ninth deciles of the estimated velocities were 1.9 and 10.4 km/day for BTV-1 and 3.7 and 7.8 km/day for BTV-8, thus the distribution of estimated velocities appeared narrower for BTV-8 than for BTV-1. Moreover, the lower values of velocities that were observed for BTV-1 than for BTV-8 may be related to the effect of vaccination. Indeed, contrary to what was observed for BTV-1, there was no large area with high vaccine coverage for BTV-8. Regarding the influence of environmental factors, variables related to the ecology of Culicoides vectors (weather and elevation) were the main factors influencing the velocity of BT spread for both serotypes. Weather at a two month-lag plausibly could affect Culicoides abundance through direct effects on demographic life cycle parameters e.g. larvae and pupae require moist habitats, adults are prone to desiccation [3], and temperature is known to influence survival and duration of all stages of life cycle [45]. Weather at a one-month lag is most likely to influence Culicoides activity [46], [47]. The strong negative effect of temperature and the positive effect of rainfall, both at a two-month lag, suggest that in late summer (most clinical cases occurred in August and September) Culicoides dynamics in south-western France become damped when high temperatures exacerbate low level of moisture availability, a combination of factor which is known to induce low survival rates [48], [49]. However, at a one-month lag, monthly averages of maximum daily temperature around 24°C were associated with slightly increased velocities. These apparently contrasting results could reflect that incidence rates were greatest following several months of more or less exponential growth in both vector and virus populations and immediately prior to a desiccation induced crash in vector abundance effectively damping the velocity of further spread. Velocity of BTV-1 spread was also influenced by elevation, the highest velocity being observed for an elevation range between 280 and 454 m. The influence of elevation on velocity of BT spread, which was also observed for BTV-8 [23], was probably related to abundance, species composition and vector competence of the Culicoides vector populations. Indeed, Culicoides populations from the Obsoletus Complex have been found in Europe along a broad altitudinal cline [50], but their abundance changed with elevation. Moreover, Carpenter et al. [51] observed in the United Kingdom a variation of Culicoides susceptibility to BTV infection according to geographic areas within and between species and populations. Similar variation of Culicoides susceptibility and competence may partly explain the effect of elevation on velocity of BT spread. The positive effect of beef cattle density on BTV-1 spread contrasted with the negative effect of dairy cattle density on the BTV-8 spread. This might be related with differences in cattle management practices. Indeed, dairy cattle are kept close to farms, thus creating localized clusters of hosts and a relatively discontinuous pattern of host availability, which might be less favorable to BT spread. By contrast, beef cattle herds tend to be scattered throughout the landscape, a spatial arrangement that facilitates BTV progression [23]. High densities of small ruminants were negatively associated with the velocity of BTV-1 spread, a result that was also observed for BTV-8. With 1.3 million reproductive animals in 2008, dairy sheep farming was more important than meat sheep farming (926,000 reproductive animals) or goat farming (145,000 reproductive animals) in south-western France. Furthermore, according to our 2008 small ruminant count data, dairy sheep flocks are larger than meat sheep flocks, with a mean value of 153 versus 49 animals. Consequently, the negative association between high small ruminant densities and velocity may be due to dairy sheep management practices, which are similar to the dairy cattle management practices mentioned above. Another hypothesis would be that small ruminants were less competent hosts for BTV-1 than cattle, which may cause a dilution effect, and ultimately a negative association between high density of small ruminants and velocity of BTV-1 spread. One landscape-related covariate was significantly linked with velocity: the edge density between arable land and forests. This finding is consistent with previous results as edge density between arable land and forests was identified as a BTV-8 seropositivity risk factor for cattle in France [4]. It was also related to velocity of BTV-8 spread [23]. Arable land may serve as feeding areas for wildlife and forests provide breeding [52] and resting sites [50] for Obsoletus Complex midges. Edges between these habitats may facilitate contacts between BT vectors and wild hosts, then influencing BT dynamics."
2549955,2.0,"Finally, two potential weaknesses of our study need to be considered. First, we used clinical cases to describe BT spread, and they may suffer from biases because of asymptomatic animals and, to a lesser degree, under-reporting of diseased animals. Consequently, the 1,595 municipalities included in the study might not represent an exhaustive sample of contaminated municipalities. Regarding asymptomatic animals, the severity of BT infection is influenced by various factors including host species, breed, age, individual susceptibility, environmental factors and BTV serotype [53]. Little information is available on BTV-1 clinical signs: the most common clinical signs observed in small ruminants are fever, depression, lethargy, facial edema and salivation [54]. However, a recent experimental study showed that BTV-1 infection induced more marked clinical signs in sheep than BTV-8 infection [55]. Moreover, as farmers received monetary compensation for BT diseased animals, under-reporting was probably rare. We could thus expect limited biases of BTV-1 clinical cases. Furthermore, even if the real BT clinical incidence was underestimated, it did not preclude an unbiased estimate of the spatial trend [56]. A second weakness of our study is that we did not account for wind-mediated vector movements on BT spread [57], [58]. However, our main purposes were to assess the effect of vaccination on BTV-1 spread velocity, and to compare the effect of environmental features on this velocity with previous results obtained for BTV-8. The effect of wind was beyond our scope."
341358,0.0,"Our study provides evidence for the serological prevalence of circulating antibodies against BTV and RNA of the BTV-1, − 2, − 3, − 15, and − 16 serotypes in the dairy cattle population in South Korea. The present study demonstrated that BTV infection was prevalent in the dairy cattle populations analyzed, in which approximately one in five dairy cattle herds and one in six dairy cattle were infected (Table 1). Further studies might include virological and serological investigations of BTV-4 and -7 circulating in South Korea because neutralizing antibodies against BTV-4 and -7 have been detected in dairy cattle serum samples. However, none of the blood samples in the present study showed virological evidence of BTV-4 and -7. To understand the results of the present study, it is important to know that South Korea has no vaccination program for bluetongue. Thus, the high seroprevalence of BTV infection in South Korea can be assumed to reflect a natural infection of the dairy cattle evaluated."
341358,1.0,"Briefly, with respect to Asian countries and the Middle East, the overall seroprevalence obtained was lower than that previously reported in this species in Taiwan, Nepal, India, and Japan [31–34], but higher than that previously reported in central and south-east Iran [35, 36]. The seroprevalence of BTV infection described in our study (18.2%) is comparable to that described among sentinel ruminants, such as cattle, buffalo, and goats, in China (17.1% seroprevalence) [37] and Indonesian ruminants (2–23% seroprevalence) [38]. The seroprevalence determined in this study is lower than that described among ruminants in Taiwan (32.7% in cattle and 8.2% in goats) [31], sentinel cattle in Japan (5–71%) [13], and sheep and cattle in Israel (16.7% in sheep and 63.2% cattle) [39] but higher than that reported in domestic yaks in China (2–5%) [23]."
341358,2.0,"It was noted that 81.1% (28/34) of the seropositive dairy cattle herds clustered at either < 30% or > 81%, suggesting a “bimodal” distribution (Fig. ​(Fig.2),2), similar to that described by Taiwanese and Dutch research groups [31, 40]. The total dairy cattle population of South Korea is mostly composed of the Holstein breed. In 2015, the dairy cattle population in South Korea was composed of 402,405 bovines, which were kept on 5,407 holdings (approximately 74 dairy cattle per holding) [41]. The highest density of dairy cattle was in Gyeonggi Province, with 15.7 dairy cattle per km2 (Fig. ​(Fig.2).2). The lack of clinical signs of BTV might also be because the development of bluetongue in sheep has attracted little attention in South Korea because of the very small number of domestic sheep. Consequently, most BTV episodes throughout the world may be completely silent. Therefore, the results of the seroepidemiological surveillance performed in this study suggest that subclinical or mild BTV infection of ruminants in South Korea is prevalent almost every year and that there are repeated and recurrent infections among domestic cattle and other ruminants in the affected regions."
341358,3.0,"Despite the serological evidence of BTV infection and the serotype distribution in dairy cattle, there is no clinical report of bluetongue in any species in South Korea. BTV is apparently still infecting dairy cattle but causing subclinical disease. The factors related to the lack of a routine monitoring system, difficulties in making a clinical diagnosis, and misdiagnoses of similar viral diseases should also not to be ignored. For example, Aino virus, Akabane virus, Chuzan virus, and Ibaraki virus present the greatest difficulty for differential diagnosis because these viral infections cause similar symptoms and have high seropositive rates in sentinel cattle in South Korea, 33.2% for Aino virus, 40.2% for Akabene virus, 29.1% for Chuzan virus, and 7.5% for Ibaraki virus [42]. In the present study, an intra-class correlation coefficient of 0.21 was found, indicating that the correlation between two animals within a herd with respect to the BTV result was relatively high. The intra-class correlation coefficient of livestock infectious disease is usually < 0.2 and ranges from 0.04 to 0.42 [43]. This finding indicates that in any particular dairy cattle herd, it is likely that either most dairy cattle in that herd will be serologically positive or most will be negative. Our results revealed a significant association between age and BTV seroprevalence in South Korea, and a similar association was previously reported by several groups studying other cattle populations [44–46]. This association between age class and BTV seropositivity can be explained by a longer time of exposure for adults [46, 47]. The proportion of seropositive animals increased with age, probably resulting more from prolonged exposure of the adults to the vector than to any resistant status of juveniles. BTV infection significantly decreased with the increase in the number of dairy cattle inside a farm. The virus pressure in the vectors and ruminants would decrease due to the dilution effect caused by the increase in the density of susceptible dairy cattle on the farm. Nevertheless, this interesting finding requires further investigation. The results obtained from this study indicate that northern and eastern regions are areas of relatively low BTV seroprevalence. Relative to the factors that affect the likelihood of infection at an individual animal level, living in southern and western regions in South Korea increased the risk of being BTV positive compared with living in northern and eastern areas. This result could partially be explained by the spatial distribution of the Culicoides group, but the association between the BTV epidemiology and Culicoides vector distribution in South Korea is not fully understood."
822569,0.0,"Here, we summarized the reported cases of melioidosis in South Korea. A total of 11 cases of melioidosis were reported during the past 12 years, and the overall fatality rate was 36.4%. All the patients had visited Southeast Asia, where melioidosis is endemic. Most of the cases stayed in the endemic area for months to years, and developed symptoms before returning to South Korea or within a few months after the return. However, some cases stayed in the endemic region for as little as a few days, and one case developed symptoms 3 years after returning to South Korea. Pneumonia was the most frequent clinical manifestation, but the patients showed a wide spectrum of clinical features, including a mycotic aneurysm of the aorta and coinfection with tuberculosis. An early diagnosis and initiation of the appropriate antibiotics can reduce mortality. Consequently, increased awareness of the risk factors and clinical features of melioidosis is required."
822569,1.0,"Most melioidosis cases result from recent exposure, as the majority of melioidosis cases in the endemic region occur during the rainy season. The incubation period of melioidosis ranges from days to months [10]. A previous study calculated an incubation period of 1–21 days between the inoculating event and symptom onset [11]. However, B. pseudomallei can reactivate after an initial asymptomatic infection with extremely prolonged latency. There are reports of melioidosis becoming symptomatic more than 20 years after the suspected exposure 12, 13. In this report, incubation periods could not be determined clearly, as most of the patients failed to recall the actual event of inhalation or inoculation. Therefore, the time period between returning to South Korea and the onset of symptoms was recorded as an indirect measure of the minimum incubation period. More than half of the cases developed symptoms during their stay in the endemic region or within 1 month after returning to South Korea. However, the initial symptoms developed more than 1 month after returning to South Korea in five cases, and the interval between return and symptom manifestation was as long as 3 years in one case [4]. Although poorly understood, the prolonged latency of B. pseudomallei is thought to be associated with its facultative anaerobic nature, ability to persist without nutrients, and ability to endure antibiotic pressure by assuming filamentous forms [14]. It is important that physicians should not rule out melioidosis simply because the duration from the suspected exposure to symptom onset is longer than a month."
822569,2.0,"Risk factors for melioidosis include diabetes mellitus, heavy alcohol consumption, chronic lung disease, chronic renal disease, malignancy, and immunosuppressive treatment [3]. Patients with these risk factors are predisposed to melioidosis, and previous studies showed that 80–87% of melioidosis patients had at least one risk factor 3, 15, 16. Of the risk factors, diabetes mellitus is most important. Diabetes has been reported in 38–60% of melioidosis patients 17, 18, 19, 20, and the calculated risk of melioidosis in diabetics is as high as 21.2 (95% confidence interval 17.1–26.3) times the risk in nondiabetics [16]. The presence of risk factors is associated not only with susceptibility to the disease, but also with its severity and clinical manifestations. The chronic form of melioidosis was less common in diabetic patients and more commonly observed in patients without risk factors. In addition, patients without risk factors tended to have lower rates of bacteremia, septic shock, and mortality [3]. In the present series, seven (64%) patients had at least one risk factor, all of whom were diabetic. Of the four patients without risk factors, chronic infection was observed in three (75%) and two (50%) had bacteremia. By contrast, none of the seven patients with at least one risk factor developed a chronic infection and all of them were bacteremic."
822569,3.0,"Previous reports showed that male sex is an independent risk factor for melioidosis 16, 21. High occupational exposure to soil and water is a significant risk factor for melioidosis [22], and the association between male sex and melioidosis is suggested to reflect an increased exposure to B. pseudomallei [16]. In the present series, all the cases were male, and this may also be associated with high environmental exposure. Cases with higher occupational exposure to soil and a prolonged duration of stay in an endemic area have a higher chance of exposure to B. pseudomallei. Under the current social circumstances of South Korea, men are more likely to work in foreign countries for a long period, and this may explain the reason that all the patients are male. In fact, only three cases in the present series were tourists, and the remaining eight cases were those who stayed in Southeast Asia for occupational purpose for months to years."
822569,4.0,"The fatality rate of melioidosis ranges from 14% to 49% 3, 17, 18 and was reported to be as high as 65% in bacteremic patients [19]. Old age (≥50 years), the presence of risk factors (diabetes, heavy alcohol use, and chronic lung or renal disease), and markers of organ dysfunction (leukopenia; raised liver enzyme, urea, and creatinine levels; and acidosis) on admission are thought to be associated with mortality 3, 23, 24. In the present series, four patients, three with diabetes and one elderly patient, died from melioidosis. Two patients had septic shock with evidence of organ dysfunction on admission. The overall fatality rate was 36.4%. Despite its considerable fatality rate, a recent study suggested that survival is improving. In the Darwin study, the fatality rate from melioidosis decreased from 30% to 9% over the past 20 years, and this was attributed to an early diagnosis, early treatment with the appropriate antibiotics (ceftazidime or meropenem), and access to intensive care management [3]. Similarly, the mortality of melioidosis in South Korea may be reduced by an early diagnosis and initiation of the proper antibiotics. Empirical antibiotic treatment often does not cover B. pseudomallei, so an early suspicion of melioidosis is essential."
822569,5.0,"Pneumonia is the most common presenting feature of melioidosis, followed by genitourinary infection and soft tissue infection 1, 3. Similarly, in this study, seven (63.6%) patients presented with pneumonia, three (27.3%) patients presented with genitourinary infection, and one (9.1%) patient presented with soft tissue infection. Abscesses were found in the prostate, spleen, and liver, which are well-recognized sites of abscess formation [3]. A mycotic aneurysm of the aorta was observed in Case 5. Although very uncommon, mycotic aneurysms due to melioidosis have been reported, and are thought to be associated with high mortality and relapse rates [25]. Therefore, melioidosis should be suspected when patients with risk factors for melioidosis present with a mycotic aneurysm. Moreover, although pain is seen in the majority of patients, the presenting symptoms of a mycotic aneurysm are often nonspecific [25]. The patient in Case 5 denied any pain on admission, and the mycotic aneurysm was detected incidentally in the process of evaluation of the fever focus. Therefore, a mycotic aneurysm should be included in the sites for further evaluation in melioidosis patients whose primary infection focus is obscure."
822569,6.0,"Case 1 had a coinfection with pulmonary tuberculosis and melioidosis. The patient was initially diagnosed with pulmonary tuberculosis, but showed a poor response to antitubercular treatment and was finally diagnosed with melioidosis. Melioidosis might manifest as a chronic pulmonary infection that mimics pulmonary tuberculosis and could be misdiagnosed as tuberculosis, especially in areas endemic for tuberculosis, but not for melioidosis [26]. Moreover, as in this case, a coinfection of melioidosis and pulmonary tuberculosis can occur [27], leading to considerable confusion in the treatment process. Although not well established, previous case reports suggested that mycobacterial infection is a risk factor for melioidosis, reflecting host susceptibility to intracellular pathogens [1]. The patient of Case 1 might have had underlying pulmonary tuberculosis, and this might have acted as a risk factor for melioidosis. Increased suspicion for melioidosis is required when patients with a history of visiting endemic regions present with chronic pneumonia."
822569,7.0,"In summary, we presented the detailed epidemiological and clinical features of the reported cases of melioidosis from 2003 to 2014."
435540,0.0,"Few models have been used to predict the incidence of melioidosis on either a national or global scale [14, 15, 43]. We applied population dynamics, seasonal movement, and the impact of diabetes to study melioidosis epidemiology in Thailand. Other approaches such as decision tree or Markov model can also be used to study melioidosis epidemiology given that the rate of transmission is constant and the system is linear. Our model fits a dynamic model for a non-transmissible disease to data on notifications only, which should allow reasonable predictions to be made as to the future course of the epidemic. However, drawing strong inferences regarding parameter values that pertain to transitions through infection/disease states after the point of infection is less safe, such that particular caution should be exercised in regards to parameters, especially those discussed here. Our findings have some similarities and differences compared with previously published work. Limmathurotsakul and colleagues used a negative binomial model to derive estimates of 7,572 (3,396–17,685) cases for global melioidosis incidence in the year 2015 [15]. This figure was similar to the incidence of melioidosis estimated in our study, which was 7,569 cases (4,834–8,701) for 2015. Both studies reached similar estimates of approximately 50% for case reporting and 40–45% for mortality/death rate for melioidosis (see S2 Table A), while the risk factors identified for melioidosis were also in agreement, i.e. being male, aged more than 44 years old, and having diabetes [13]. Two previous studies by both Thai and Australian researchers consistently showed that type 2 diabetes increased the risk of melioidosis by more than 10 times when compared with those non-diabetes [13, 44], this figure is similar to our model estimates. Buckee and colleagues pointed out that seasonal disease incidence could be driven by the mobility and aggregation of human populations, which spark outbreaks and sustain transmission [30]. Northeast and transient males aged more than 45 years old were also predicted by our model to be a highest risk groups for melioidosis. Apart from reporting and mortality/death rates for melioidosis, the model also gave the estimates for some natural history of disease parameters which would be hard to measure i.e. proportion and duration of asymptomatic infections, duration of untreated symptomatic infections, and host susceptibility to melioidosis [45]. With regard to asymptomatic infections, a few studies have tried to characterize and estimate the number of these hidden infections [44–46]. Our model suggested that there could be a significant proportion of asymptomatic infections (54%). Although the parameter values used in the fitting and prediction process are based on annual incidence data only, the variation in the parameter values is included to reflect the uncertainty in the predictions, and the posterior distributions represent sets of collinear parameters that reproduce the observed data. Further studies and more appropriate data are required to refine these parameters."
435540,1.0,"Our model can provide many benefits for health policy planning. First, the model, in common with previous studies, estimated that only about half of all melioidosis cases were being reported. Under-reporting results in melioidosis being neglected, even more than other neglected diseases such as dengue and leptospirosis [16]. Previous study suggested that melioidosis was the third most frequent cause of death from infectious diseases in northeast Thailand, after HIV/AIDS and tuberculosis [13]. By regarding melioidosis as being less important disease has made it being further under-recognized by healthcare professionals, low health budgets to invest in intensive prevention and control, poor disease knowledge and practices among the population at risk, and finally a lack of research that would enable the development of concrete strategies to improve standards of care. Second, the model can be used to guide the design of targeted interventions i.e. predicting and identifying populations at high risk for morbidity and mortality. In line with the model’s predictions, targeted intervention strategies could be concentrated among the male population of working age who live in the Northeast, as well as the transient population. These strategies could include providing health education to increase protective practices while engaging in agricultural activities, washing after work, and seeking appropriate health advice when feeling sick. To prevent deaths due to infections in older age groups, i.e. 45 years or older (65%) (see S2 Table A), national strategies could focus on early diagnosis and appropriate treatment, as well as improving diabetes screening programmes, since elderly people with diabetes may be prone to severe melioidosis."
435540,2.0,"Our model has some limitations. For simplicity we assumed that diabetes influenced the likelihood of melioidosis infection alone and therefore once the person is infected with melioidosis, the diagnosis and disease progression are independent of diabetes status. Although diabetes has been shown to play roles in increasing severity and/or that medication of diabetes may also affect susceptibility and presentation of melioidosis [47, 48]. We also assumed that mortality/death rates for melioidosis, incidence rates of diabetes, and seasonal movement rates were constant over time, although they varied by age. It has been suggested that mortality/death rate for melioidosis due to diabetes have decreased over time because of improved access to hospitals [49], and lifestyle changes might also have affected incidence rates [50]. In this model we classified the population into those living in the Northeast and non-Northeast, which meant that the model was unable to predict the incidence of melioidosis in locations more specifically than non-Northeast. It is important to keep in mind that melioidosis is probably prevalent in all regions of Thailand, the lack of knowledge, disease awareness and diagnosis tools led to heavily report of cases by the Northeast region only [16]. We assumed that the estimates of reporting among both males and females were the same. The annual epidemiological surveillance reports of melioidosis data used in the model included cases from all provinces around Thailand, except for those cases seen in private hospitals, which account for about 30% of hospital provision, although there is no information on the relative likelihood of melioidosis being diagnosed in different sectors. Melioidosis diagnoses reported annually by the BoE are made using an indirect hemagglutination (IHA) technique to test for antibodies to B. pseudomallei, which has been found to have low sensitivity and specificity. It could therefore potentially under-predict the number of cases."
435540,3.0,"Conclusion. Population dynamics, seasonal movement, melioidosis infection rates, and under-reporting are important components of melioidosis incidence patterns. The increases seen in melioidosis cases are partly attributable to demographic changes as working, transient, and aging population groups are more prone to develop melioidosis. The key findings from our study are firstly, the increasing trend of melioidosis incidence, especially among males aged 45–59 years old, is predicted to continue; and secondly, the male, Northeast, and transient populations aged 45–59 years old were at the highest risk of melioidosis infection."
435540,4.0,"We anticipate that the modelling methods described here could be used in similar settings, especially those with reliable census data, to estimate the future melioidosis burden, as well as the potential effects of under-reporting. In addition, this modelling approach could be adapted to study other infectious diseases, behavioral changes, and seasonal movements, where demographic factors are important drivers of a population’s disease burden."
2515873,0.0,"There were 4632 genes differentially expressed in melioidosis and 5045 genes in tuberculosis, thus approximately 20% of the human genome is differentially regulated in each disease. The most prominent pathway in melioidosis was interferon (IFN)-γ and the same was true of tuberculosis. There were no pathways differentially regulated in melioidosis that were not also differentially regulated in tuberculosis, and there was no signature which reliably distinguished melioidosis and tuberculosis."
2515873,1.0,"Berry et al. identified an 86-gene signature as being specific for tuberculosis after eliminating differentially regulated genes common to Streptococcus pyogenes and Staphylococcus aureus infections, and to two auto-inflammatory diseases (systemic lupus erythematosus and Still’s disease). This signature was also present in melioidosis, which is surprising given that all melioidosis patients recruited had acute rather than chronic melioidosis, which is clinically distinct from tuberculosis."
2515873,2.0,"Interferon-mediated Responses. The IFN-γ pathway was reported as the most prominent pathway identified in gene expression studies of a mouse model of melioidosis [27], and blocking IFN-γ dramatically increases host susceptibility to melioidosis [3]. In human studies, plasma IFN-γ concentrations were high in melioidosis [3], and IFN-γ-mediated responses were also the most prominent feature in a gene expression study of melioidosis in another human cohort [28]. The finding here that this feature is shared with tuberculosis is unsurprising, because IFN-γ responses are crucial for the host response against intracellular pathogens such as B. pseudomallei and M. tuberculosis. IFN-γ treatment has a role in the management of multidrug-resistant tuberculosis, and adjunctive therapy with IFN-γ is beneficial in a mouse model of melioidosis [29], although its role in clinical melioidosis remains undefined [30]."
2515873,3.0,"In their original report on this tuberculosis cohort, Berry et al. noted that type 2 IFN-γ responses were prominent, but noted that type 1 IFN-αβ responses were present also [7]. We found that type 1 interferon-αβ responses were just as prominent in melioidosis, but the clinical relevance of this remains to be defined."
2515873,4.0,"Type 1 interferons can be produced by almost any cell type (leukocytes, fibroblasts and endothelial cells) and are induced by a range of bacterial pathogens, whereupon they proceed to modulate the host response in a manner that is as yet incompletely understood [31]. The signalling pathways initiated by type 1 interferons are best described in terms of their activation of signal transducer and activator of transcription (STAT) family members (STAT1 to STAT6) [32], the best studied of which are STAT1 and STAT3. STAT1 activation is dependent on both type 1 and type 2 interferons and results in a pro-inflammatory response, with recruitment of inflammatory cells and the enhancement of antigen presentation [31]. On the other hand, STAT3 activation is a key mediator of IL-10 signalling, and results in inhibition of inflammatory responses and directly inhibits STAT1 activation [31]. The role of STAT4 is less well described, but STAT4 activation may play a role in T helper 1 lymphocyte differentiation, which is an essential part of the host response to intracellular pathogens. Type 1 interferons are also necessary for the production of inducible nitric oxide synthase [33], which is in turn necessary for the clearance of intracellular bacteria. Interestingly, type 1 interferons are able to inhibit IL-1β production and inflammasome assembly by two separate mechanisms: the first is via inhibition of NLRP1 and NLRP3 inflammasomes in a STAT1-dependent manner; the second, is a reduction in pro-IL-1 levels via a STAT3-dependent pathway [34]. It has previously be shown that host response to B. pseudomallei is inflammasome-dependent [35]."
2515873,5.0,"The role of type 1 interferons in tuberculosis is unclear, since mice deficient in the production of type 1 interferons are better able to control M. tuberculosis infections [36], but type 1 interferons also play a non-redundant protective role in the absence of type 2 interferon signaling [37]. The role of type 1 interferons in the pathogenesis of melioidosis remains to be studied."
2515873,6.0,"PAMP-specific Responses. TLR4 and CD14 are upregulated in both melioidosis and tuberculosis. The classical ligand for TLR4 [38] and for CD14 [39] is lipopolysaccharide (LPS), which would explain this finding for B. pseudomallei. TLR4 will recognize heparin-binding haemagglutinin [40], and CD14 will bind lipoarabinomannan [41], both of which are expressed by M. tuberculosis."
2515873,7.0,"The pattern recognition receptors TLR5 [25] and NLRC4 [26] both recognize flagellin. No alternative ligand has yet been described for TLR5, so it is more difficult to explain why tuberculosis should apparently induce a flagellin-response. One explanation is that upregulation of pattern recognition receptors is not driven by their ligands. TLR5 expression is induced as part of the type 1 interferon response [42], while NLRC4 is upregulated as part of the TNF-α response [43]. Both pathways are prominent in the host response to melioidosis. In support of this hypothesis, the TLRs are upregulated as a group in both melioidosis (TLR1, TLR2, TLR4, TLR5, TLR6, TLR8 and TLR10) and tuberculosis (TLR2, TLR4, TLR5, TLR6, TLR7, TLR8)."
2515873,8.0,"Limitations and Future Research. Tuberculosis is strongly associated with HIV infection, but melioidosis is not. HIV targets primarily CD4-positive T-lymphocytes and lymphocyte depletion is a feature of all sepsis. Lymphocytes were depleted in both the melioidosis and the TB cohorts, so lymphocyte-related pathways and modules are missing from the whole blood gene expression data of both cohorts, making it difficult to make any comment about the relative role of CD4-positive cells in melioidosis compared to tuberculosis. The whole blood signature was dominated by neutrophils which may also have obscured any lymphocyte signature. Future studies that use purified lymphocytes harvested from melioidosis patients may shed light on this issue."
2515873,9.0,"Microarrays generate large amounts of data that are useful for the development of hypotheses. Our analysis has identified a number of other pathways that are differentially regulated in melioidosis, but which are unstudied to date. Notably, the TRAIL pathway is differentially regulated in melioidosis, but its role remains undefined at present. The glypicans (cell surface proteoglycans) contribute to cell proliferation and growth, both essential processes in the host response to infection. To date, investigations into the role of glypicans have been confined primarily to cancer biology, although glypican-deficient mice are more susceptibility to respiratory infections [44]. In tuberculosis, the glypican network appears to have greater prominence than even the interferon-mediated responses."
2515873,10.0,"Conclusions. Host responses to melioidosis and TB are dominated by interferon-signalling events, despite the fact that the organisms are unrelated and present completely different cell-surface PAMPs to the host. This is likely because they both stimulate host responses common to intracellular pathogens, and because the expression of pattern recognition receptors is not driven by their ligands, but by cytokine responses (primarily IFN-γ and TNF-α). The 86-gene signature identified by Berry et al. clusters melioidosis patients just as effectively as it clusters tuberculosis. It therefore seems likely that whole blood gene signatures will not be able to diagnose tuberculosis in areas where melioidosis and TB are co-endemic, but may find utility when interpreted in combination with clinical features. Further studies using direct comparisons will be required to confirm this finding."
1477397,0.0,"Our data suggest that melioidosis is the third most common cause of death from an infectious disease in our region after HIV/AIDS and tuberculosis. Deaths from melioidosis and tuberculosis were comparable in 2006, and melioidosis may become established as the second most common cause of death if the current trend of increasing melioidosis incidence is sustained. The population mortality rate for melioidosis is more than from malaria and diarrheal illness combined in this setting, diseases that are usually considered to be of high priority by funding agencies and global health organizations. We propose that the number of cases and deaths from melioidosis reported here represents a minimum estimate, because we have not accounted for patients presenting to community hospitals in the province who were not referred to Sappasithiprasong Hospital. There are several reasons for the lack of referral, including patients with mild disease and those who were seriously ill and died shortly after admission to a community hospital or during inter-hospital transfer. Diagnostic microbiology is not available in these hospitals, and it is also possible that the diagnosis could be missed. Melioidosis can present with a wide range of clinical manifestations, and our study shows that active use of diagnostic microbiology can improve case detection. It is also likely that melioidosis is underreported as a cause of death in the National Statistics; laboratory isolation and identification of B. pseudomallei may take up to 1 week, and one-half of deaths because of melioidosis occur in the first 48 hours after presentation to hospital when the true diagnosis may be unknown and not listed on the death certificate."
1477397,1.0,"The death rate from melioidosis showed a modest decline during the study period and is currently around 40%. This improvement was not associated with changes in recommended antimicrobial regimens over the study period. Ceftazidime has been the recommended first-line parenteral therapy at Sappasithiprasong Hospital for suspected or proven melioidosis after a study published in 1989 showed ceftazidime to be associated with a 50% overall reduction in mortality compared with a combination of chloramphenicol, doxycycline, and trimethoprim-sulphamethoxazole.17 Improved outcome from melioidosis over the study period may reflect improvements in the standard of medical care provided in provincial Thailand. However, the death rate of 40% is double that reported for patients with melioidosis at the Royal Darwin Hospital in Northern Australia.7 The major cause of death in our melioidosis patients is severe sepsis and the associated organ failure. Most cases are not treated in an intensive-care setting because of limited resources, and we propose that improvements in outcome would require investment in early sepsis management and critical-care facilities. Evidence for the efficacy of improved intensive-care unit management for patients with melioidosis comes from the experience described at the Royal Darwin Hospital.9 There is also anecdotal evidence from Khon Kaen University Hospital in neighboring Khon Kaen province where patients with severe melioidosis are treated in an intensive-care unit wherever possible and the mortality rate is around 20% (P. Chetchotisakd, personal communication)."
1477397,2.0,"There are several possible explanations for the increasing rate of melioidosis over time. Our method of case ascertainment did not change during the study, but most patients admitted to Sappasithiprasong Hospital were referred from community hospitals situated throughout the province; it is possible that referral patterns have changed over this period. A rise in incidence could also relate to rising life expectancy and a concomitant increase in people with pre-disposing conditions. Data available for life expectancy in Thailand between 1998 and 2004 show a modest increase from 68.9 years in 1998 to 70.3 years in 2004.16 The strongest risk factor for melioidosis is diabetes mellitus, which was shown in this study to put individuals at vastly increased risk of B. pseudomallei infection. The prevalence of diabetes in people in Thailand seems to be rising over time, and the reported rates of diagnosed plus undiagnosed diabetes are 2.3% for 1991, 4.6% for 1996, and 6.7% for 2004.15,16 The adjusted RR of melioidosis for diabetes of 12.4 in this study is consistent with that reported from northern Australia.6 We are not aware of any factors relating to changes in social behavior that would lead to an increase in the rate or duration of exposure to B. pseudomallei in the environment, and no association was found with annual rainfall. Understanding the reasons behind the increasing incidence in melioidosis is important for targeted prevention, and it is the basis of on-going studies."
1477397,3.0,"A limitation of this study is that the incidence of melioidosis and population mortality rates were based on active surveillance at a single hospital. This provides a minimum estimate for a single province and may not be accurate for the rest of northeast Thailand. To address this, we contacted 19 provincial hospital laboratories in northeast Thailand to obtain data on the number of culture-confirmed cases of melioidosis in 2007. A total of 1,865 culture cases were defined without active surveillance, including 387 patients defined from Ubon Ratchathani provincial hospital laboratory. This is equivalent to an annual incidence rate for northeast Thailand of 8.7 per 100,000 people (95% CI = 8.3–9.1 per 100,000 people), which is a minimum estimate that is significantly higher than the rate reported previously.2 Melioidosis is known to occur in the areas of Laos and Cambodia that are immediately adjacent to northeast Thailand,18–20 but incidence rates are poorly defined and may be grossly underestimated because of a lack of diagnostic microbiology facilities. We predict that melioidosis will become recognized as a major pathogen throughout this region in the wake of laboratory strengthening and use of standard guidelines for the investigation of suspected melioidosis."
1477242,0.0,"Our enhanced population-based surveillance provides updated estimates of the incidence and in-hospital mortality rate of bacteremic melioidosis among hospitalized patients in Nakhon Phanom, Thailand, during 2009–2013. The estimated average annual incidence of bacteremic melioidosis hospitalizations was 14.9 per 100,000 persons, very similar to our previous estimates from 2006 to 2008,9 and slightly higher than a hospital-based study of melioidosis in northeastern Thailand from 1997 to 2006 (12.7 per 100,000 populations).10 Overall incidence showed variability between years from 13 to 17 per 100,000 persons per year with a nonsignificant decrease in in-hospital mortality rate from 2009 to 2013."
1477242,1.0,"During 2009–2013, our incidence rate is still higher than country reported melioidosis morbidity rate from Thailand’s national surveillance system.28 During our investigation period, the national melioidosis morbidity rate ranged from 2.13 to 6.13 per 100,000 population with mortality rate less than 1%. This likely substantially underestimates the number of melioidosis cases and deaths in Thailand given the passive nature of the national surveillance system. Our investigation’s report showed the advantage of the integration of laboratory information and clinical information from hospitalized pneumonia surveillance to monitor melioidosis in this province. The ideal epidemiological system for monitoring infectious diseases morbidity and mortality would connect laboratory information, clinical information, and death registry data in the integrated reporting mechanism. The high observed incidence among patients aged 55–64 years may be due to this age group likely having higher prevalence of comorbid conditions known to be risk factors for melioidosis17 and having greater occupational exposures (e.g., rice farming).29 As noted in the recent melioidosis global burden of disease estimates, the true burden of this disease is likely still underestimated even in endemic areas.30 There was a marked temporal clustering of hospitalized bacteremic melioidosis patients during the rainy season from July through October similar to what has been reported elsewhere in Thailand and other endemic areas."
1477242,2.0,"The slight observed decrease in in-hospital mortality rate from 2009 to 2013 could have resulted from detection of more (and possibly less severe) cases through improved blood culturing practices or from improved recognition and awareness of the clinician for treating suspected melioidosis patients, especially during the rainy season since 2006. The in-hospital death of bacteremic melioidosis after installing the automated blood culture system decreased from 29% in 2006 to 21% in 20089 and continued to decrease to 13% in 2013. Previously, the number of in-hospital deaths of hospitalized melioidosis cases in Nakhon Phanom was reported as high as 56% in 2003 and 34% in 2005.32 Given the observed frequency of patients being discharged in poor clinical condition noted mortality could range from a low of 14% when limited to in-hospital deaths to 46% if all patients with poor clinical condition at the time of discharge subsequently died. Unfortunately, our database did not obtain the Thai identification number for tracking out-of-hospital deaths through the national death registry database. Although limited by a low number of patients per month, the in-hospital mortality rate was noted to be highest in November during the cool season, which could suggest lower clinician awareness compared with the rainy season and may indicate a need for continued sensitization of clinicians during non-rainy periods of the year when incidence is lower. Clinicians in endemic regions should have a high index of suspicion for B. pseudomallei infection throughout the year and empirical septicemia treatment guidelines should include antibiotics active against B. pseudomallei.12,17 This is particularly true for patients in the highest risk age-groups (patients aged more than 50 years), those with previously identified risk factors for B. pseudomallei infection such as diabetes mellitus and renal insufficiency, and those groups with likely intense environmental exposure, such as rice farmers and other agricultural workers. During the 5-year analysis period, in-hospital mortality rate was higher among melioidosis patients with pneumonia, which is consistent with recent studies in other locations.2,10 Although limited by the small number of cases annually, the observed decline in in-hospital mortality rates among patients with melioidosis pneumonia deserves further investigation and could relate to greater clinician awareness of melioidosis among severely ill patients with pneumonia."
1477242,3.0,"There were several limitations to our analyses. First, our investigation underestimated the overall incidence and burden of melioidosis in the province. We did not have information on outpatients or non-bacteremic melioidosis patients, and previous studies have shown that only around 50–60% of melioidosis patients in northeastern Thailand are bacteremic.1 In addition, the sensitivity of blood culture for diagnosis may be as low as 60%,33 so additional testing modalities would likely have identified more case patients. Second, we did not have information on outcome after hospital discharge, and more than a third of surviving patients had their discharge status listed as poor. Not including out-of-hospital outcomes likely led to an underestimation of overall mortality rate. Third, more than 40% of bacteremic melioidosis cases lacked details on clinical characteristics and antibiotic treatment, limiting the ability to assess overall disease severity and the impact of treatment on outcome among hospitalized bacteremic melioidosis patients."
1477242,4.0,"The burden of bacteremic melioidosis in Nakhon Phanom remains high despite the reduction in in-hospital case fatality seen since the introduction of automated blood culture capacity in the province in 2005. The continued high incidence of bacteremic melioidosis, pneumonia, and associated deaths in an endemic area is concerning given increasing numbers of persons with high-risk conditions such as diabetes, the incidence of which increased from 658 to 894 per 100,000 persons from 2009 to 2013 in Nakhon Phanom province.34 Therefore, maintaining sensitive BSI surveillance, which provides a consistent and comparable method to monitor and track trends in serious invasive infections, is vital to guide the development of affordable strategies to reduce morbidity and mortality. Such surveillance activities are also important for rapidly detecting, responding to, and controlling this public health problem, and thereby contributing to global health security. We encourage clinicians working in B. pseudomallei endemic areas to investigate potential melioidosis patients using standardized, validated microbiological techniques together with CXR, especially for patients presenting with sepsis or acute respiratory symptom in the rainy season. Consistent application of this approach will optimize the opportunity for early diagnosis and targeted treatment to improve outcomes. Increased availability of tests with faster turnaround times than conventional blood culture including sensitive, point-of-care tests or use of advanced molecular methods, may allow for earlier provision of life-saving treatment. Promising options, including PCR, latex agglutination, and lateral flow assays have been recently developed, and evaluations of these tests in the clinical setting are underway.35 In the absence of an approved human vaccine, the identification of alternative prevention measures for melioidosis are urgently needed in high-burden areas, especially for known high-risk groups such as patients with diabetes, renal failure patients, and older adults."
429616,0.0,"Gene expression profile of cytokines. Our gene expression analysis of melioidosis patient samples did not reveal any statistically significant differential gene expression pattern in comparison with healthy controls (Table 2, Fig 1). The cytokine cascade events following B.pseudomallei infection has been studied in several animal models and show an up regulated mRNA expression of inflammatory cytokines such as IL6, IL12, IL10, IFNγ, TNFα and IL1β within 72 hours of infection[10, 11, 31, 32]. Studies have also shown an elevated level of expression of IL8, IL6, IL12, IL18, IL15, and IFNγ in plasma of melioidosis patients[12, 33]. Our findings are contrary to a published study showing an increased mRNA expression of inflammatory response genes such as IL6, IL15, IL10, IL4, IFNγ, TNFα and IL1β in melioidosis patients compared to healthy controls [9]. Majority of our samples in the melioidosis cohort, consisted of patients with greater than 10 days of fever/clinical symptoms and antibiotic treatment, in comparison to other studies which have sampled melioidosis patients in early acute phase, within 3 days of antibiotics treatment. This could have been the main reason for the contradicting result. We did not have adequate number of samples in early acute phase of melioidosis (n = 4) to see a statistically significant differential expression compared to healthy controls. Melioidosis patients display differential expression of several immune response genes compared to cases of sepsis resulting from other infections, indicating that the differential expression of these genes can be used as diagnostic marker for melioidosis."
429616,1.0,"IL4 was up regulated in the melioidosis patients, including the diabetic cohort, compared to other sepsis cases (Tables ​(Tables22 and ​and5,5, Figs ​Figs11 and ​and3).3). IL4 plays a major role in B-cell activation and T-cell proliferation, thus acting as a key regulator of humoral and adaptive immunity. Its role as an anti-inflammatory cytokine participating in decreasing the production of Th1 cells and pro-inflammatory cytokines is suggestive of down regulation of inflammatory responses in melioidosis. Up regulated IL4 expression has been reported in melioidosis patients and acute melioidosis models [9, 13]. Thus, further investigations are currently being carried out on gene expression of IL4 and closely related anti-inflammatory cytokine IL13 in melioidosis patients."
429616,2.0,"IL8 was significantly down regulated in septicaemic melioidosis patients when compared to other sepsis cases, suggesting that it could be a marker of disease severity (Table 3, Fig 1). IL8 down regulation in early acute melioidosis cases (less than 15 days of fever/clinical symptoms and antibiotic treatment) was also seen compared to other sepsis cases (Table 3, Fig 2). A study using a human lung epithelial cell line showed that IL8 production upon B. pseudomallei infection was lower than cells infected with other gram negative bacteria which is in agreement with our findings [34]. Studies have also shown that B. pseudomallei can activate NF-κB and induce IL8 production without involving TLRs[35]. Increased level of plasma IL8, IL6 concentration being associated with mortality have also been reported[33]. Type 2 diabetes (T2D) has been reported to be a significant comorbidity associated to melioidosis, particularly septicaemic cases [36]. A study by Morris et al, showed significantly elevated levels of IL8 in plasma of diabetic cohorts compared to non-diabetics 3.5 hours after B. pseudomallei stimulation, suggesting a dysregulated immune response in T2D as underlying factor for susceptibility to melioidosis[37]. Thus IL8, a key mediator of innate immunity associated with inflammation, being down-regulated in early acute stages of melioidosis and in septicaemic cases needs to be investigated further with a larger sample pool, to get a better understanding of IL8’s role in disease severity."
429616,3.0,"HMGB1, classified as a late onset mediator of sepsis which may function as a pro-inflammatory and anti-bacterial factor [38, 39], was consistently down regulated in melioidosis patients irrespective of other confounding factors like comorbidities (risk factors) and duration of clinical symptoms and treatment, compared to other sepsis infection cases (Tables ​(Tables22–5, Figs ​Figs11–3). Our findings also reveal a significantly up regulated expression of HMGB1 in other sepsis cases compared to healthy controls. HMGBI has been reported to show high levels of expression in plasma of melioidosis patients, particularly septicaemic melioidosis cases compared to other sepsis infections and has been associated with clinical severity and mortality[39]. Some studies have also shown that HMGB1, could induce Th2 type response, leading to increased production of anti-inflammatory cytokines like IL4, IL5 etc, while lowering the Th1 type response [40, 41]. HMGB1, plays a key role as an immune modulating factor and its potential role in cell mediated immune dysfunctions ought to be investigated further."
429616,4.0,"Gene expression profile of Toll-like receptors. TLR2 and TLR4 did not exhibit a differential expression in melioidosis patients compared to healthy controls which is contrary to published studies showing up regulated expression[9, 16]. This contradiction, we believe is also due to variation in duration of clinical symptoms and antibiotic treatment in the studies. However, TLR4 is significantly up regulated in the sepsis cohort, compared to healthy controls (Table 2, Fig 1). As such bothTLR2 and TLR4 are down regulated in the septicaemic melioidosis cases compared to other sepsis infections (Table 2, Fig 1), suggesting poor pathogen recognition. Studies have indicated that LPS of B. pseudomallei signals via TLR2 as opposed to TLR4 being the main receptor for other gram-negative bacteria and TLR-mediated dysregulation of immune response plays a major role in disease pathogenesis [13, 16, 18]. HMGB1 shows a TLR4 dependent activity, with CD14 playing a key role in activation of TLR4 dependent signaling by HMGB1 has also been reported [42, 43].These correspond with our findings of a low level of expression of HMGB1, which plays a major role in activation of TLR4 mediated immune responses, hence down regulation of inflammatory and defense responses upon B. pseudomallei infection."
429616,5.0,"Gene expression profile of genes associated with the antigen presentation pathway and cell mediated immunity. MICB, PSMB2, PSMB8 and PSME2 showed consistently low level of expression in melioidosis cohort irrespective of other factors like comorbidities (risk factors), duration of clinical symptoms and antibiotic treatment, compared to other sepsis controls (Tables ​(Tables22–5, Figs ​Figs11–3). Low level of expression of these immune response genes which play a major role in antigen presentation and thereby cell mediated immunity, suggests altered defense responses during melioidosis or diminished proteasomal activity at the time of sample collection during later stages of infection. Our findings are contradicting with a similar study that showed differential gene expression pattern of proteasomes and other genes involved in MHC class I & II antigen presentation pathway, where genes PSMB2, PSME2, PSMB8, PSMA5 and HLADMB were over expressed in meliodosis patients compared to other sepsis cases [8].Variations in duration of clinical symptoms, administration of antibiotics at the time of sampling and associated comorbidities could have played a role in this contradicting result, while taking into consideration that the Pankla et al, 2009 study included samples mostly collected within 48 hours of hospitalization. It is also to be noted that the results of this 2009 study has not been independently verified."
429616,6.0,"Gene expression profile of epigenetic modification factors. There have been extensive studies on the role of epigenetic factors and their association with several communicable and non-communicable diseases [19–21, 25]. DNA methylation, histone deacetylation and methylation are epigenetic modifications associated with a repressed chromatin state, which contributes to repression of gene transcription[44]. Studies have shown that DNMT3B, exhibits an inverse correlation between gene expression levels and DNA methylation levels[19]. A study by Bonsch et al, 2006 showed that genomic DNA hypermethylation was associated with lower mRNA levels of DNMT3B in patients with alcohol dependence [45]. Study by Zong et al 2015, showed that reduction of HDAC activity and expression, was associated with disease severity in smokers with chronic obstructive pulmonary disease (COPD)[20]. Epigenetic modifications are heavily influenced by practices such as smoking, alcohol dependence[19, 20], diseases such as diabetes, cardiovascular and kidney diseases [22, 23, 25, 46] which in turn could play a major role in disease pathogenesis and susceptibility."
429616,7.0,"A recent study on epigenetic changes in human host DNA following B. pseudomallei infection, revealed significant changes in DNA methylation in the vincity of genes involved in inflammatory responses, intracellular signaling, apoptosis and pathogen induced signaling, suggesting that DNA methylation changes could be altering gene transcription thus affecting key immune pathways [47]. High throughput gene expression analysis in melioidosis have not revealed strong association of epigenetic factors with B. pseudomallei infection [8]. However, extensive study in this area is required. While this study on whole-genome transcriptional profiles of septicaemic melioidosis and sepsis caused by other infections has revealed a transcriptional signature [8], those findings were never validated in an independent study. Therefore, we found it necessary to follow up on those studies, as well as investigate a unique set of target genes involved in epigenetic regulation to evaluate whether the expression of the epigenetics markers were deferentially regulated in melioidosis compared to sepsis caused by other infections. While those genes were not found to be deferentially expressed in whole-genome studies [8], given the relapse and reactivation of infection in susceptible groups, it was of interest to further investigate the transcription profiles of this set of genes involved in epigenetic modifications."
429616,8.0,"Our study investigated the differential expression of several epigenetic regulators in melioidosis. DNMT3B, responsible for DNA methylation, HDAC1 and HDAC2, responsible for histone deacetylation were significantly down regulated in melioidosis patients compared to other sepsis cases irrespective of confounding factors like duration of fever/clinical symptoms, antibiotic treatment and associated comorbidities (Tables ​(Tables22–5, Figs ​Figs11–3). Thus, correlation between mRNA levels of DNMT’s and levels of methylation ought to be analyzed further. Our comparison of melioidosis patients with regular alcohol consumption habit (n = 8) and sepsis controls (n = 10) showed a similar expression pattern to the entire melioidosis cohort (n = 30), in addition to down regulated expression of DNMT3A and IL8 (Table 5, Fig 3). We could observe a significantly lower level of expression of the epigenetic factors in melioidosis cases compared to sepsis controls, while assessing alcohol consumption as a risk factor, indicating its confounding effect."
429616,9.0,"From our studies we find that epigenetic factors HDAC1, HDAC2 and DNMT3B show consistent differential expression compared to other sepsis cases, suggesting a role in disease susceptibility and pathogenesis. However, further studies such as DNA methylation arrays with a larger sample pool and further analysis of confounding comorbidities, is required to fully understand the role of epigenetic mechanisms in relation to pathogenesis of melioidosis."
429616,10.0,"Limitation of study. The main limitation of our study was that the melioidosis patient samples were collected well after start of antibiotic treatment which may affect immunocompetant cells, which in turn affects the immune response genes investigated. Studies have shown that antibiotics like meropenem exert an immunomodulatory effect, affecting the production of some cytokines in PBMC’s [48]. This may have been the main reason, as to why we could not see any significant differential expression of some key inflammatory response cytokines such as IFNγ, especially between the melioidosis and healthy cohorts, while similar studies have sampled within 3 days of antibiotic treatment. Duration of clinical symptoms ranged from >10 days to >90 days and duration of antibiotics treatment ranged from 1 day to >30 days at the time of blood collection for all the melioidosis samples. Since our sample collection was nationwide, duration between patient identification/ disease confirmation and sampling was substantial due to logistical issues. Thus, due to wide range of duration of infection and limited number of samples with ≤15 days of fever (n = 4) we could not evaluate statistically significant differential expression of inflammatory genes during the early stages of infection. Melioidosis is severely under-reported and under-diagnosed in Sri Lanka and most patients who get hospitalized and diagnosed of the disease, are already in a later stage of infection. This makes it very difficult to carryout investigations with patient samples within an early stage of infection and or anti-microbial therapy. While its important to gather information on expression levels of host factors during early acute phase of infection, it is also imperative to have some understanding on expression levels of important host genes during later stages of infection like in our study, which may be useful to monitor antibiotic treatment regimes. As we see diabetes as a major comorbidity in our experimental cohort, we analyzed our data to see if there was any significant differential expression between diabetic melioidosis cases and non-diabetic melioidosis cases. The gene expression between these two groups were comparable and we could not find any statistically significant differential expression due to diabetes (S3 Table). Therefore, our results show a consistent differential expression of HMGB1, MICB, PSMB8, PSMB2, PSME2, HDAC1, HDAC2 and DNMT3B when compared to other sepsis cases, irrespective of comorbidities (risk factors), duration of fever/clinical symptoms and antibiotic treatment, primarily due to melioidosis infection. We also note that gene expression analyses of blood leukocytes only provide insight in immune pathways regulated at mRNA level in circulating cells, hence we look to expand our future study on a proteomic level as well. We also take note of the limitation of IHA test used in this study for diagnosis of melioidosis. Though the sensitivity and specificity of IHA test is limited, it has been used worldwide as a laboratory method for melioidosis diagnosis. The bacterial loads of patient samples are not always high for culture positive results, with general antibiotic treatment playing an intervening role. Hence IHA test results were included, as they serve as a useful reference for melioidosis diagnosis."
429616,11.0,"Conclusion. Our findings did not show significant differential expression among the immune response genes and the epigenetic modifiers in melioidosis cases and the healthy controls. However, our study indicates differential expression in inflammatory responses, defense responses and epigenetic factors during melioidosis compared to other cases of sepsis, thus differential gene expression among the genes under investigation that can distinguish melioidosis cases from other sepsis infections. These findings suggest that the differentially expressed genes during melioidosis should be validated during different stages of infection for their potential as markers of disease diagnosis and for therapeutic intervention. Thus, our future studies shall be aimed at studying gene expression profiles in early and late acute phases of melioidosis and also looking into susceptible groups for further study on disease severity. Studies would also be broadened into the four areas showing differential gene expression pattern- key cytokines, antigen presentation pathways, toll-like receptor signaling pathways and epigenetic chromatin modifying enzymes. Further investigations to confirm these findings in a larger cohort is needed, which may validate the potential of these differentially expressed genes to serve as disease biomarkers that could pave the way for novel diagnostic and therapeutic approaches for melioidosis intervention."
1807783,0.0,Whether meropenem can reduce mortality in acute melioidosis remains unclear. The only published trial comparing a carbapenem with ceftazidime in severe cases found a modest and non-significant reduction in mortality.4 The outcomes of a clinical trial comparing ceftazidime and meropenem in severe melioidosis are not yet available (NCT00579956).
1807783,1.0,"In addition to the scanty evidence are a number of other limitations. First, we assume general sepsis guidelines would be effective in classification of severity. If this is wrong, some non-severe cases could receive the more costly treatment unnecessarily, while more severe cases would be denied the potential advantage offered by meropenem. Second, we assume both treatments are equally effective in empirically treated non-melioidosis cases. Third, due to methodological challenges we did not include the societal cost of carbapenem-resistant organisms following increased use of meropenem."
1807783,2.0,"Costs and other key parameter estimates are specific to the Thai context, therefore our recommendations may not be generalizable to other settings. However, the model could be adopted and re-analyzed with other sites' specific parameters to identify the cost-effective treatment strategies for acute melioidosis."
1807783,3.0,"Conclusions. Notwithstanding the limited evidence, we conclude that use of meropenem in patients with suspected severe melioidosis is likely to be cost-effective in northeast Thailand, assuming even a modest reduction in mortality. This strategy also resembles the standard guidelines of melioidosis treatment in Australia.5 Empirical meropenem treatment for all acute cases, however, is less likely to be cost-effective unless the reduction in mortality is much higher than indicated by a previous clinical trial.4 This analysis should be repeated once better evidence become available."
428872,0.0,"In the current study, we investigated derangements of VWF in the host defense against septic melioidosis. Thrombocytopenia has been observed incidentally in previous cases of melioidosis [32, 33] and so we first sought evidence of thrombocytopenia in melioidosis, since platelet counts are routinely available as part of the initial assessment of all sepsis patients. We found that thrombocytopenia is a feature of sepsis caused by B. pseudomallei and is correlated with mortality."
428872,1.0,"The concept that platelets are the chief cellular effector of hemostasis is well established [34], however in very recent years we and others have showed in two preclinical studies that platelets also function as key effector players in the host response against bacterial infections [35, 36]. Of note, mice treated with the platelet depleting antibody (α-GpIbα) had a strongly impaired host response when intranasally challenged with Gram-negative bacteria leading to increased bacterial growth and a decreased survival [35]. Additionally, a recent clinical study which included a heterogeneous group of 913 consecutive patients with sepsis, blood microarray analysis revealed a distinct gene expression pattern in sepsis patients, low platelet counts, showing reduced signaling in leukocyte adhesion and diapedesis and increased complement signaling [37]. Platelets interact with the innate immune system through different mechanisms; direct bacterial killing, immunothrombosis, recruitment of neutrophils, and by potentiating effects such as neutrophil extracellular traps (NETs) production [5, 38]. NETs form a central role in the host response against invading pathogens, and entrap and kill bacteria. We previously showed that melioidosis patients had increased levels of NET-related components and that NETs have antibacterial activity against B. pseudomallei [39]. However, NET formation did not protect against bacterial dissemination or inflammation in a murine B. pseudomallei-induced sepsis model [39]."
428872,2.0,"In septic patients, the development of thrombocytopenia is secondary to various mechanism [40]; platelets are activated and bound to endothelium, resulting in sequestration and destruction [41]. It remains unclear whether reduced platelet counts lead directly to adverse clinical outcome in sepsis, or whether they are simply a biomarker for disease severity at presentation [42]. We show that thrombocytopenia is related with progression to death in melioidosis which is probably associated with a more disturbed host response [37]. However, further clinical and animal studies on the exact role of platelets and platelet neutrophil interactions in the host response against B. pseudomallei infection are needed."
428872,3.0,"In our study, VWF antigen levels were higher in melioidosis patients compared to controls, however not correlated with mortality. We showed that excess circulating VWF in melioidosis is correlated with excess secretion and reduced ADAMST13. Our finding that ADAMST13 activity is decreased in melioidosis is consistent with the wider sepsis literature which finds that ADAMST13 deficiency is a common feature of severe sepsis both in adults [30] and in children [20]. This decline of ADAMST13 may be explained by the presence of proinflammatory cytokines in septic patients [22], which suppress mRNA transcription encoding ADAMTS13 [43]. ADAMST13 was decreased in non-surviving patients potentially reflecting the decrease of proteolytic activity of this enzyme in sepsis [31, 44]. However, literature contains conflicting data on the utility of VWF and ADAMTS13 as predictors of outcome in sepsis [16]. In a single-center observational study of 40 patients conducted as part of a larger randomized-controlled trial of C1-inhitbitor supplementation in all-cause sepsis they show that ADAMST13 levels were lower and VWF antigen levels were higher in severe sepsis or septic shock [31]. Neither ADAMST13 nor VWF parameters correlated with outcome. In our study, the range of ADAMST13 values felt within the normal range. We therefore, can conclude that ADAMTS13 is of no prognostic value in melioidosis. Moreover, as the link between ADAMST13 levels and VWF antigen is not as strong and the correlation with mortality is weak, novel strategies for the adjunctive treatment of sepsis such as ADAMTS13 supplementation that have been proposed for the management of all-cause sepsis [44] are also less likely to be beneficial in melioidosis."
428872,4.0,"Interestingly, there was no correlation between VWF and thrombocytopenia, suggesting that excess VWF is not the primary driver of thrombocytopenia in melioidosis. We note, incidentally, that other additional mechanisms may contribute to the accumulation of VWF during sepsis: for example, oxidative modification of VWF in sepsis has been shown to prevent cleavage of VWF by ADAMST13 [45, 46]. Another potential driver of thrombocytopenia includes diffuse intravascular coagulation (DIC) [47]. Activation of the coagulation pathway may be deleterious when the triggered blood coagulation is insufficiently controlled. This could lead to DIC and microvascular thrombosis. Reduced level of ADAMTS13, as we showed in this study, has been reported in DIC due to severe sepsis [30]. However, the median DIC score from this melioidosis cohort, as calculated using the International Society on Thrombosis and Haemostasis (ISTH) standardized method was 3 (2–4), indicating that DIC is not overt [9, 48]. More studies focusing on DIC in melioidosis patients are needed."
428872,5.0,"Additionally, hemophagocytosis, a life-threatening condition of excessive immune activation, has been postulated to drive sepsis-related thrombocytopenia. Proliferation of macrophages, leading to uncontrolled phagocytosis of platelets, erythrocytes and lymphocytes characterizes the macrophage activation syndrome (MAS) [49, 50]. It could very well be that a significant portion of our patients have MAS, which can be defined as having five of the following eight features: fever, splenomegaly, peripheral blood cytopenia, hypertriglyceridemia and/or hyperfibrinogenemia, hemophagocytosis in bone marrow, spleen, lymph node of liver, diminished NK cell activity, high ferritin levels and elevated soluble CD35 [51]. Unfortunately, most of these markers are not available for our patients. This is an interesting area to further explore. Patients with sepsis associated MAS might benefit from interleukin-1 receptor blockade [50]. In this respect, it is of interest that both the administration of anti-IL-1ra as well as anti-IL-1b protects against experimental melioidosis [52, 53]."
428872,6.0,"To the best of our knowledge, this work is the first that studies the impact of VWF, ADAMST13 and platelets in patients with melioidosis. However, study limitations need to be considered. First, we only included patients with diabetes in this study. We made this decision, because diabetes itself is associated with abnormalities of coagulation, anticoagulation and fibrinolysis [54]. In addition, the majority of patients with melioidosis have diabetes and we were not interested in the effect of diabetes on coagulation during melioidosis, because this has been investigated extensively elsewhere [9]. Second, the number of patients analyzed was restricted due to financial constraints for the assays. Third, although to the best of our knowledge no intersex differences in VWF and ADAMTS13 levels have been described, it should be mentioned that the male/female ratio of controls differs from patients."
428872,7.0,"Conclusions. In conclusion, thrombocytopenia is a feature of sepsis caused by B. pseudomallei and is correlated with mortality. Excess VWF is a feature of acute melioidosis and is likely driven by both by increased secretion of VWF propeptide in endothelium and by reduced clearance by ADAMTS13. However, the thrombocytopenia of melioidosis is likely not driven by excess VWF: other possible drivers include diffuse intravascular coagulation (DIC) and hemophagocytosis. In the past years, tremendous progress has been made toward our understandings of the protective roll of platelets in sepsis and the possibility in the use of thrombocytopenia as biomarker [35, 37]. More animal and human studies are necessary to understand the reason of thrombocytopenia in septic melioidosis patients and to translate this to clinical practice."
432177,0.0,"To our knowledge, this is the most extensive systematic review of individual participant data of case reports and case series on the CNS melioidosis. However, the critical issue is that some data were not reported by the author, and we do not know if such variables were absent or not evaluated. This missing data issue makes the data percentage look higher than actual values so the results should be cautiously interpreted."
432177,1.0,"Like all melioidosis, most CNS melioidosis also occurred in the endemic regions especially northern Australia and Thailand. Some cases developed in the non-endemic region, however, they all had a history of physical presence in the endemic area where they could contact the soil or water before. Because B. pseudomallei infection can be latent for an extended period, it can then reactivate the disease later like CNS tuberculosis. For example, Beck et al. reported a case of melioidosis meningitis from the US who served during the Vietnam War 13 years ago [5]. This pattern of disease occurrence leads melioidosis to be known as the “Vietnam time bomb”. Some occupations, such as rice farmer, have a potential risk for percutaneous inoculation of the organism via exposure to soils or water."
432177,2.0,"The pathogenesis of CNS melioidosis appears to be different among the cases. Firstly, hematogenous spreading of the microorganism to the CNS plays an essential role in the disease mechanism. It is supported by the study’s result that blood is the most common specimen that isolates B. pseudomallei. The circulating bacteria can cross the cellular barriers, which include the blood-brain barrier and blood-CSF barrier, using three methods: transcellular, paracellular or Trojan horse method [6]. The latter mode occurs when the bacteria can survive within the circulating monocytes which can cross the cerebral endothelium via L-selectin (CD62L) mediated migration [7]. Secondly, as evidence from the animal studies, direct brainstem invasion through the nasal passage and cranial nerves is believed to be another distinguish mechanism of CNS melioidosis especially encephalomyelitis type [8]. This mechanism is supported by the results from Darwin study [1] which demonstrated low bacteremic rate for encephalomyelitis in comparison to brain abscess (21% vs. 100%). However, our review has found that the bacteremic rates between cases of encephalomyelitis and brain abscess were quite similar (54% vs. 58%). Finally, percutaneous inoculation over the scalp and subsequent direct extension to the skull and brain may be responsible for the pathophysiology of particular cases. This is supported by the observation that 25 percent of CNS melioidosis patients also had concurrent skull osteomyelitis or scalp abscess, and some indeed reported the history of preceding cranial injury."
432177,3.0,"All age groups and both sexes could have CNS melioidosis, although most patients were adult males. However, the proportion of children to adult is much higher for the CNS melioidosis (23%) compared to all melioidosis (5%) from the prospective study in northern Australia [9]. CNS melioidosis is a heterogeneous disease that can present as encephalomyelitis, brain abscess, isolated meningitis or isolated extra-axial collection. Most cases of encephalomyelitis were reported from northern Australia, while regional genotypic differences between B. pseudomallei strains may explain this geographical restriction [10]. Encephalitic syndrome without myelitis became the dominant presentation of encephalomyelitis type. Fever is the most common manifestation, although it also occurs in various infectious diseases. Focal neurological deficits especially cranial nerve palsies are also prominent features of this CNS melioidosis type, in which lesions are mainly located in the brainstem (48%). Sixth, Seventh, Ninth and Tenth cranial nerves are most commonly involved. These cranial nerve deficits can be found in the brainstem encephalitis of other causes as well including Listeria monocytogenes, herpes simplex virus type 1, and Mycobacterium tuberculosis [11,12,13]. Despite its common brainstem attack, alteration of consciousness develops in only half of cases. Other subtypes of encephalomyelitis include combined encephalitis with myelitis and isolated myelitis. Patients with pure myelitis usually present with classic spinal cord syndrome including paraparesis or quadriparesis accompanying by sensory level and bowel-bladder dysfunction. For brain abscess type, focal neurological deficit, i.e., unilateral weakness is the most common feature; however, cranial nerve palsies occur infrequently. Unlike brain abscess from other causes [14], presentation with fever is more prevalent in melioidosis (74%). Headache is still a common symptom, and most patients have conscious changes. Neck stiffness appears to be associated with occipital lobe abscess. Clinical features of melioidosis meningitis remain similar to other meningitis syndromes which encompass fever, headache, and neck stiffness. Isolated extra-axial collection of melioidosis has not been reported in children so far. Fever remains the most common presentation while headache, focal neurological deficits or seizures is not a reliable indicator of melioidosis subdural empyema or epidural abscess."
432177,4.0,"Interestingly, CNS melioidosis patients can initially present with cranial swelling, which is associated with infection of extracranial structures such as scalp abscess, parotid abscess or orbital abscess. This manifestation may be the result of direct extension from the intracranial pathology or led by the organism inoculation into structures themselves. Although many organisms other than B. pseudomallei can cause pneumonia and skin infection, these two organs involved in the CNS melioidosis were not uncommon. Some cases have the splenic abscess, a rare disease which is generally caused by a handful of pathogens including Klebsiella pneumoniae, Escherichia coli, Salmonella spp., and Staphylococcus aureus [15]. Moreover, prostatitis or prostatic abscess, which is typically caused by Enterobacteriaceae, was also observed in some cases of CNS melioidosis."
432177,5.0,"The CSF profile of CNS melioidosis, which commonly shows pleocytosis with mononuclear cells predominance, is comparable to tuberculous or viral meningitis. Nevertheless, tuberculous meningitis typically shows a lower CSF glucose level and higher CSF protein concentration. The CSF polymorphonuclear cells predominance can be demonstrated in one-third of CNS melioidosis patients. This variable characteristic of cells dominance is also shown on the Listeria meningoencephalitis [11]."
432177,6.0,"Contrast-enhanced MRI is the neuroimaging study of choice for CNS melioidosis because of its high sensitivity. Brain CT may appear normal if it is done early. Therefore, MRI should be further undergone in case CNS melioidosis is still in the differential diagnosis. Brainstem lesions are seen to be prominent in the encephalomyelitis type. Some authors suggested that CNS melioidosis shows a propensity to involve and spread along the white matter tracts across the commissural or longitudinal fibers [16,17]. This may explain the study’s result that a reasonable proportion of encephalomyelitis patients had lesions involving both supra- and infratentorial structures as well as the spinal cord. However, isolated supratentorial lesions are found to be the most common findings of all CNS melioidosis and even all encephalomyelitis types. Interestingly, some parenchymal lesions also involve adjacent subdural or epidural spaces, skull and extracranial structures. This invasive behavior is similar to tuberculosis, invasive fungal infection and malignant neoplasms."
432177,7.0,"Culture from the CNS specimen is the best way to establish a definite diagnosis of CNS melioidosis. These sources encompass 1) brain tissue 2) pus from the brain or subdural collection or epidural abscess and 3) CSF. However, it is hard to obtain the specimen from some pathogenic location such as the brainstem. As a result, the diagnosis can be made by the presentation of the compatible CNS diseases that is supported by positive cultures from other specimens. The hemoculture yields a positive result in the highest sensitivity with 52 percent. Other samples including sputum, skin pus, osteomyelitic skull, and scalp abscess would be helpful for the diagnosis. Although we included 9 CNS melioidosis cases who were diagnosed by only serology in our review, the indirect hemagglutinin assay (IHA) alone is not reliable enough for diagnosis in clinical practice because of its problematic false positive and false negative [18,19]."
432177,8.0,"Once the diagnosis of CNS melioidosis was made, most cases were treated with appropriate drugs both in the intensive- and eradication-phase therapies. According to the 2010 consensus recommendations, ceftazidime and meropenem are the drugs of choice for intensive-phase therapy, while trimethoprim/sulfamethoxazole is the first-line drug for eradication-phase therapy [20]. Some medications including chloramphenicol and doxycycline were used in intensive-phase therapy in some CNS melioidosis cases; however, they are not effective thus should be avoided. The treatment duration is also a key to success that eight weeks and six months are the minimal duration for intensive- and eradication-phase therapy, respectively [21]. However, we found that most non-death cases were treated inadequately. Although most cases of CNS melioidosis completely or partially recovered following treatment, one-fifth finally succumbed to the disease."
432177,9.0,"There are several limitations to our study besides the missing data issue. We identified only the CNS melioidosis cases that were presented in the literature. However, we believe that the CNS melioidosis patients were still underreported especially in the endemic regions of melioidosis. The precise diagnosis of CNS melioidosis types which differentiates encephalomyelitis from brain abscess was not definite. Some large rim-enhancing lesions diagnosed with brain abscess might be progressed from prior microabscesses, which is recognized as encephalomyelitis type. On the other hand, in encephalomyelitis, abnormal imaging findings other than rim enhancement may be cerebritis, early stage of brain abscess. Moreover, the contrast-enhanced imaging characteristics including nodular, irregular and linear enhancement were not explicitly defined on the reports."
432177,10.0,"Conclusions. The CNS melioidosis mostly occurs in adults, and diabetes mellitus is the most common risk factor. Encephalomyelitis and brain abscess are the major types of the disease which fever, unilateral weakness, and cranial nerve deficits are the most prominent presenting features. The CSF profile usually shows mononuclear pleocytosis with high protein and normal glucose level. Contrast-enhanced MRI is the most sensitive imaging study for detecting brain lesions. The rim-enhancing pattern is the most frequent finding, and the brainstem is the most affected location. One-fifth of the patients is fatal. The interpretation from this study may be biased as there were several missing data from the case reports and case series."
1974798,0.0,"A recent disease burden study suggested that there are around 165,000 cases and 89,000 deaths due to melioidosis every year and that the disease is endemic in at least some parts of 83 geographies. Our impact and cost-effectiveness modelling indicates that a melioidosis vaccine targeted at high-risk populations living in environmentally suitable areas for melioidosis transmission could reduce the burden of this disease in these populations and be cost-effective. To our knowledge, this is the first study evaluating the impact and economic consequences of melioidosis vaccination from a global perspective. In addition, this is the first study systematically quantifying the relative risk of melioidosis in different groups by pooling evidence from a review of observational studies."
1974798,1.0,"Only one cost-effectiveness study of melioidosis vaccination has been published; this was restricted to north-eastern Thailand but also found vaccination to be potentially cost-effective [7]. Our analysis extends this by considering all geographies with endemic melioidosis, a wide range of risk factors based on a synthesis of studies in the literature, and a range of vaccine strategies. By targeting the vaccine to the population at greatest risk, we can ensure that vaccination is cost-effective even when its duration of protection is short (5 years)."
1974798,2.0,"In addition, with the trend of increasing prevalence of chronic renal disease and diabetes in some of the endemic countries, the size of the high-risk population could increase considerably in the near future, making the vaccine more cost-effective [25, 26]. The long-term prognosis for these chronic diseases is improving in many countries, again making the protection from a melioidosis vaccine potentially more valuable in the future. On the other hand, melioidosis patients may have better survival prospects due to the better quality and accessibility of treatment [34]."
1974798,3.0,"The potential target areas for vaccination were determined from a study that determined environmentally suitable areas for melioidosis with very high spatial resolution (5 × 5 km2). However, the environmentally suitable areas generally do not correspond to administrative boundaries between or within countries, especially in large geographies such as Australia, China and India. In practice, it may be easier to target vaccination strategies by administrative units such as provinces or states based on average risks within those units. Improving surveillance systems at both the local and global levels would also strengthen the robustness of data informing such decisions."
1974798,4.0,"This study has several limitations. Firstly, we considered three risk factors for melioidosis in our evaluation: age over 45 years, diabetes and chronic renal disease. These have been consistently identified as some of the most important risk factors for melioidosis in studies. However, other attributes have also been identified as potential risk factors, although they are less consistently reported and/or have smaller reported relative risks. They include male gender, chronic lung disease, thalassaemia, excessive alcohol consumption and being an indigenous Australian [2, 12]. Further data about melioidosis relative risks and prevalence of these risk factors at the country level may help more precisely targeted vaccination strategies that could increase vaccine impact and cost-effectiveness further."
1974798,5.0,"Secondly, many parameters around treatment protocols and costs were estimated for broad categories of geographies stratified by income level. Furthermore, some parameters had to be established through expert elicitation due to limitations in available data. Economic studies at the country/territory level could help establish more reliable estimates of the economic burden of melioidosis."
1974798,6.0,"Moreover, in the absence of explicit cost-effectiveness threshold in most of the evaluated countries, we adopted one GDP per capita to be the threshold which has been widely used in low- and middle-income settings [30]. However, recent modelling evidence suggests that GDP per capita thresholds are much higher than the actual heath opportunity costs in several countries [35, 36]. Hence, our findings may require careful interpretation and ideally supplemented by country-level analyses using local thresholds and deliberative processes."
1974798,7.0,"Lastly, as no melioidosis vaccines have entered human trials to date, we made broad assumptions about the potential cost, protective efficacy and duration of protection afforded by a vaccine. Our assumptions have been fairly conservative: we assumed short duration (5 years), imperfect efficacy (50–80%) and vaccine cost assumptions based on one of the most commercially successful vaccines to date (human papillomavirus vaccine). By doing so, our analysis establishes the minimal characteristics of a vaccine that can be successful in commercial and public health terms for vaccine developers to aim for. Even with 50% protective efficacy assumption in sensitivity analysis, a cost-effective vaccine strategy still exists in 51 out of 83 melioidosis endemic countries. If a melioidosis vaccine is brought to market with longer duration, better efficacy or a lower price, then it will be even more cost-effective than we report."
639480,0.0,"We examined the learning and retention of a visuomotor rotation using error-based and reinforcement feedback, and whether these mechanisms depend on the integrity of cerebellar function. Reinforcement schedules produced better retention compared with error-based learning. Moreover, using a closed-loop reinforcement schedule, where the reward was contingent on prior performance, produced rapid learning. Cerebellar patients could learn under the closed-loop reinforcement schedule and retained much more of the learned reaching pattern compared to when they performed error-based learning. However, cerebellar patients varied in their learning ability in the reinforcement condition, with some showing only partial learning of the rotation. We developed a computational model of the reinforcement condition and found that learning was dependent on the balance between motor noise and exploration variability, with the patient group having greater motor noise and hence learning less. Our results suggest that cerebellar damage may indirectly impair reinforcement learning by increasing motor noise, but does not interfere with the reinforcement mechanism itself."
639480,1.0,"We based the open-loop task on prior work showing binary reinforcement could drive visuomotor learning in controls (Izawa and Shadmehr, 2011). However, in open-loop paradigms, subjects sometimes lag behind the perturbation to such an extent that they end up receiving no reward and no longer adjust their reaches or explore sufficiently to reacquire the target zone. We, therefore, included a closed-loop condition to mitigate this problem and to ensure that the reward schedule was set as close to 50% in the early period of learning. This was done by rewarding any reach that exceeded the average of the last 10 reaches in the desired direction (i.e. countering the rotation). This closed-loop paradigm led to more rapid learning than the open-loop reinforcement paradigm, with similar final levels and retention across our subjects."
639480,2.0,"While we designed our study to assess reinforcement and error-based learning in isolation, in general these mechanisms will work together in real-world situations. In the reinforcement task participants clearly do not have access to error information, whereas in the error-based task they have both error and task success (reinforcement) information. Nevertheless, our results show clear differences between the two paradigms, which suggests that we are largely studying distinct processes�one that is driven primarily by error that is not well retained and another clearly driven by reinforcement that is well retained. This is consistent with previous studies examining interactions between the two learning mechanisms, showing enhanced retention of an error-based learning process when reinforcement is also provided (Shmuelofet al., 2012;Galeaet al., 2015). In addition, combining error-based and reinforcement learning has been shown to speed up learning (Nikooyan and Ahmed 2015)."
639480,3.0,"Cerebellar patients have known deficits in error-based learning(Martinet al., 1996;Maschkeet al., 2004;Richteret al., 2004;Morton and Bastian, 2006). Yet, the patients in our study were able to follow the gradual perturbation in the error-based condition. Analysis of their reach trajectories suggests that this ability relied on the patients using online visual feedback of the cursor to steer their reaching movements toward the target. As reach angles were calculated using the endpoint of each movement (this was necessary to compare the error-based task to the reinforcement tasks where reaches were rewarded based on movement endpoint), this feedback-dependent compensation made them appear to be learning. However, when visual feedback was removed to assess retention, the ability to correct the movement online was immediately lost (i.e. no retention). Thus, cerebellar patients did not truly adapt to the visuomotor rotation in the error-based task. Consistent with this, is that cerebellar patients cannot follow a gradual rotation when only endpoint cursor feedback is provided (Schlerfet al., 2013)."
639480,4.0,"Reinforcement learning has been posited as a spared mechanism of motor learning following cerebellar damage. Consistent with this,Izawaet al.(2012)showed that cerebellar subjects could counter a gradual visuomotor rotation and generalize the new reach pattern to other targets when they learned with concurrent online visual cursor feedback and binary reinforcement feedback. They suggested that the cerebellar patients were relying on a reinforcement mechanism because they did not change the perceived location of their hand in a proprioceptive recalibration test, whereas control subjects did. Such proprioceptive recalibration is thought to be a hallmark of error-based adaptation (Synofziket al., 2008). Here we specifically show that cerebellar patients can use binary reinforcement feedback alone to alter their reaching movements, although some patients were able to learn more than others. All patients, regardless of how much they learned, showed almost complete retention in the reinforcement task. This is in in stark contrast to the same patients showing a complete lack of retention in the error-based paradigm."
639480,5.0,"The failure of some cerebellar patients to learn in the reinforcement task could be due to either deficits in the reinforcement learning mechanism itself, or deficits in other mechanisms which might limit the usefulness of reinforcement learning (or a combination of the two). Cerebellar damage causes reaching ataxia, indicated by curved and variable reaching movements (Bastianet al., 1996). Patients with cerebellar damage also have proprioceptive impairments during active movements that are consistent with disrupted movement prediction (Bhanpuri et al., 2013). Together these studies suggest that cerebellar damage increases motor noise and/or reduces the sensitivity with which they can locate their own arm. In other words, this leads to variability in reaching that cannot be estimated by the brain. We suspected that such variability (which we consider a form of motor noise) might interfere with reinforcement learning."
639480,6.0,"To test this hypothesis we used a simple model of the reinforcement task in which each subject was characterized by two sources of variability�one that they were unaware of, which we call motor noise, and one that they were aware of, which we term exploration variability. We chose to use a simpler model than the one used byIzawa and Shadmehr (2011), in which reward learning was based on a temporal difference learning algorithm. This algorithm requires specification of a range of parameters a priori (e.g. motor noise, discount rates, motor costs). In the temporal difference rule the current reward is compared to the expected reward to drive learning. However, due to the closed-loop nature of our paradigm, which set the perturbation so that the expected reward rate was always close to 50%, getting a reward was in general better than expected and failing to get a reward worse than expected. Therefore, we simplified the rule to update the motor command only for success and maintained the motor command for failure. This allowed us to avoid setting any parameters a priori and we fit two parameters to characterize subjects� learning. Moreover, rather than fit squared error we were able to use a full probabilistic model using maximum likelihood, which allowed us to test whether our model was adequate to explain the subjects� data. The model provided a good fit for the vast majority of subjects and showed that the patients� had increased motor noise, but similar exploration variability compared to the matched controls. In other words, the majority of variability contributing to cerebellar patients� behaviour could not be used by the motor system for learning. When reinforcement was received on successful trials, updating of internal estimates of the correct action (i.e. reach angle) was impaired�estimates could only be updated based on a small proportion of the movement variability (corresponding to the exploration component) resulting in less learning. The younger group had similar motor noise to the older control group but had higher exploration variability, which led to improved learning."
639480,7.0,"Previous work has noted that increased motor variability in the rewarded dimension of a reinforcement learning task is associated with more successful performance (Wuet al., 2014). These results suggested that behavioural variability might be a deliberate output of the motor system that is necessary during learning to explore the task space, find the optimal response and yield maximal reward. Our results are in general agreement with these findings as in a reinforcement learning task, exploration variability is essential. In general, for reinforcement learning the optimal amount of exploration variability will depend on the level of motor noise. Therefore, for a fixed level of motor noise subjects should ideally learn to set their exploration variability so as to have maximum adaptation. Although we have phrased the model as exploration variability and motor noise (Fig. 7C), a mathematically similar way of expressing this is that there is a total variability in a participant�s reach direction, but he or she is only aware of a proportion of this variability or only corrects for a proportion when rewarded (Fig. 7D). Under this interpretation of the model, the cerebellar subjects have higher overall variability and correct for less of this variability when they are successful."
639480,8.0," In summary, we have shown that cerebellar patients are able to use binary reinforcement feedback alone to learn and retain a visuomotor reaching task. However, their motor noise interferes with this process. Future work is needed to determine if motor noise can be reduced or if increasing exploration variability can benefit these patients."
2436073,0.0,"Our study shows that exposure to stressors before testing has a negative effect on learning performance, mainly under conditions of positive reinforcement. In addition, learning performance appears to be differentially related to personality according to the type of reinforcement and the presence of extrinsic stress. In the absence of stressors unrelated to the task, the most fearful horses were the best performers when they learned with negative reinforcement but the worst when they learned with positive reinforcement. When stressors unrelated to the task were applied, the most fearful horses were consistently the worst performers, particularly with negative reinforcement learning."
2436073,1.0,"Stressors unrelated to the task impair learning performance according to the type of reinforcement \n  In the absence of stressors unrelated to the task, we did not observe any effect of reinforcement type on learning performance and behavioural or physiological parameters. However, in the presence of stressors unrelated to the task, learning performances decreased or tended to decrease in both groups (NR+ES and PR+ES). Impairment of attention induced by the stressors could be involved [1]. Horses exposed to stressors unrelated to the task displayed more startled reactions and alert behaviours oriented outside of the learning task and directed at the audience horse than horses not exposed to stressors unrelated to the task, suggesting a decrease in attention toward the learning task itself, as observed previously [13]. Interestingly, this learning deficit appeared to be smaller in horses learning with negative reinforcement than in horses learning with positive reinforcement. This result could be explained by the fact that negative reinforcement in itself could be considered a stressor directly related to the cognitive task, known to alleviate learning performance by focusing attention toward the learning task ([1,3], e.g. [25]). This focus of attention toward the task could have counterbalanced the impact of the stressors unrelated to the task in the NR group. In addition, acute stress is known to decrease food motivation [26�28]. An adaptative response to stress consists for an organism to prepare itself to react to this threat, generally based on a fight or flight response. Escaping becomes then the priority other eating. Within a behaviour analytic framework, fear could serve as a potential establishing operation [29], increasing the effectiveness of escaping as a reinforcer. In contrast, that fear could serve as an abolishing operation for food as a reinforcer. Therefore, an impaired motivation for food rewards may also explain why learning performance was less impaired with negative than with positive reinforcement (i.e. food reward). \n  Different patterns of behavioural and physiological responses between PR+ES and NR+ES horses were observed but did not provide an explanation for which group was more stressed. Elevated salivary cortisol levels indicate that the stress level might have been higher in horses learning with positive reinforcement, since PR+ES horses showed increased cortisol levels. However, NR+ES horses but not PR+ES horses showed more ears pointing backward and blowing, which might both indicate discomfort or stress [30�32]. One reason that may explain those differences is the history with escaping a stressor that changes according to the reinforcement type. Indeed, during each learning session, NR+ES experienced several times the avoidance of a negative outcome (i.e. tactile stimulation) by going actively into a safe compartment (i.e. pointed compartment). Experiencing this active escaping repeatedly may help reduce cortisol, whereas PR+ES horses did not experience such active process to cope with stress. However, even though the present results suggest that different patterns of stress response may emerge, they do not allow establishing clearly which reinforcement type was more stressful, and further investigations are needed to improve our knowledge of discomfort and stress measurements in horses."
2436073,2.0," Importance of personality in relationship between stress and learning performance \n  Our study reveals the existence of relationships between personality and learning performance. However, because personality is only one factor among others of predisposition of the learning performances, these relationships are tenuous. That explains why, at a statistical level of 5% and with only 15 animals per group, only a few variables of personality are significantly correlated with learning performance. However, what is interesting is that that these relationships vary with the presence of stressors unrelated to the task and type of reinforcement, especially for the dimension of fearfulness. In the absence of stressors unrelated to the task, the most fearful horses appear to be the best performers with negative reinforcement learning (NR group) but the worst when they had to learn with positive reinforcement learning (PR group). The fact that fearfulness might be advantageous when animals learn with negative reinforcement but disadvantageous when they learn with positive reinforcement is in agreement with previous experiments (active avoidance in guppies [33], instrumental task in horses [22,34,35], instrumental task of discrimination in ravens [36], instrumental cooperative task in rooks [37]). It is possible that the positive effect of negative reinforcement (considered as stressor) on performance, through the focus of attention toward the task, might have been accentuated in fearful individuals and could explain their improved performance in the absence of stressors unrelated to the task. By contrast, there were no stressors related to the learning task in the PR group, and the fearful horses might have been more easily distracted by the external environment. When stressors unrelated to the task were added, learning performance of the most fearful horses appeared to be consistently impaired, independent from the type of reinforcement (PR+ES and NR+ES groups). The disruption of cognitive and attentional processes caused by the stressors unrelated to the task is likely to be more pronounced in fearful individuals who react more strongly to various forms of stressors (e.g. [37], synthesis: [38,39]). \n  Finally, our study indicates that other dimensions of personality may also influence learning performance. Locomotor activity was positively related to performance in negative reinforcement learning, independent from exposure to stressors unrelated to the task. We hypothesize that locomotor activity may broadly reflect a tendency to initiate actions, since we previously observed such a positive effect with learning tasks requiring a displacement of body position similar to the present task [22], but also when requiring different types of movement (touching an object with the nose: [13]). In addition, reactivity to humans was negatively correlated with learning performance in the NR+ES group only. The horses less frightened by humans might have been less affected by the stressors unrelated to the task because they were more attentive to human cues. \n  We have to mention again that with 15 animals per group, only a few significant correlations between variables of personality and learning performance were revealed. To reveal the whole influence of personality on learning abilities, it would be ideal to perform this kind of studies on a larger amount of subjects, maybe several hundreds. Unfortunately, it is quite impossible to test this number of subject, within the equine species. However, we have to underline the fact that, taken together, all the experiments carried out on horses reveal consistent links between personality and learning abilities [10,13,14,22,40]. It would mean that these correlations are not obtained by chance, but more likely reflect a real influence of personality."
2436073,3.0," Conclusion \n  The present study contributes to a better understanding of the influence of stress on learning performance by showing the importance of the nature of the stress (related or unrelated to the task) and personality. Our results also provide important clues to more personalized training according to each animal�s needs. Indeed, the present study shows that positive and negative reinforcement may lead to equivalent learning performance in the absence of stressors unrelated to the task. Considering that previous studies highlighted long-lasting promising effects of positive reinforcement on training, welfare and relationships with humans [32,41�46], our results provide additional evidence for promoting the use of positive reinforcement in horse training, while the use of negative reinforcement still dominates traditional training methods [47,48]. However, the present results also show that the use of food reward as reinforcement may not be adequate in stressful conditions. Indeed, the loss of food motivation induced by stress could render the reinforcement inefficient and constitute an additional source of stress. In addition, our analyses including personality suggest that stress is a key factor in understanding how animals differ in learning performance according to their personality. Predicting how fearful animals react when they face a learning challenge therefore not only requires an evaluation of stress level but also of the nature of the stress (related or unrelated to the task) and their possible interactions. Future investigations are needed to define more precisely when the switch from favourable to unfavourable conditions for learning occurs according to the type and the intensity of a stressor and how it relates to personality."
1649970,0.0,"There has been a discrepancy in the results of studies on neurotransmitters mediating reinforcement signals in crickets and fruit-flies, and the purpose in this study was to fully resolve the discrepancy. Results of studies using transgenic fruit-flies have suggested that different types of dopamine neurons mediate both appetitive and aversive reinforcement signals via the Dop1 receptor12,13,14,15,16,34,35. On the other hand, results of our pharmacological studies have suggested that octopamine and dopamine mediate appetitive and aversive reinforcement signals in crickets3,4,5,6,7,8,36. In accordance with this, moreover, our recent study using Dop1-knockout crickets showed that they are defective in aversive learning but not in appetitive learning19. The results of that study, however, were not conclusive, since the aversive learning defects may be due to defects in the development of brain circuitry."
1649970,1.0," In this study, we observed that OA1, Dop1 and Dop2 RNAi crickets exhibited ca. 70�75% reduction in expression levels of these genes. These results suggest that silencing of the expression of these genes is successful, though we could not determine amounts of these receptor proteins in the brains of dsRNA-injected crickets because antibodies against these receptors were not available. We observed that Dop1-silenced crickets exhibited impairment of aversive learning but not appetitive learning, whereas OA1-silenced crickets exhibited impairment of appetitive learning but not aversive learning. The impairments in RNAi crickets were not due to defects of odor (CS) or water (US) perception or motor function necessary for performance of MER, because the Dop1-silenced crickets exhibited normal appetitive MER conditioning and the OA1-silenced crickets exhibited normal aversive conditioning. It can be argued that different experimental procedures for appetitive and aversive conditioning (absolute or differential conditioning and the use of different odors) might be the reason for the different effects of Dop1 and OA1 gene silencing. However, this is unlikely because we previously showed that effects of dopamine and octopamine receptor antagonists are conserved among experiments with an absolute or differential conditioning procedure and among different kinds of odors used as the CS (see discussion in ref. 8). The consistency between the results of the present gene silencing study with those of the pharmacological and Dop1 gene knockout study using CRISPR/Cas9 (cited above) strongly suggests that dopamine mediates aversive reinforcement via Dop1 receptors and octopamine mediates appetite reinforcement via OA1 receptors in associative learning in crickets."
1649970,2.0," Our observation that silencing of OA1 but not that of Dop1 or Dop2 impairs appetitive learning suggests that impairment of appetitive learning by administration of octopamine receptor antagonists observed in our previous studies3,4,5,6 is due to their effect on OA1 receptor. We observed that epinastine and mianserin impair appetitive learning but not aversive learning and, since these drugs are known as potent antagonists of insect octopamine receptors37,38, we suggested that octopamine mediates appetitive reinforcement but not aversive reinforcement. A recent study in honey bees, however, suggested that these drugs antagonize not only OA1 but also Dop239, which raised the possibility that the impairment might be mediated by blockade of the Dop2 receptor, instead of or in addition to the OA1 receptor. The finding in the present study that silencing of Dop2 does not impair appetitive learning refutes this possibility."
1649970,3.0," Our suggestion that OA1 but not Dop1 mediates appetitive reinforcement with water reward in crickets fundamentally differs from the suggestion based on results of studies using transgenic fruit-flies that Dop1 mediates appetitive reinforcement with sucrose or water reward in addition to aversive reinforcement12,13,14,15,16,34,35. The different conclusions regarding neurotransmitters mediating appetitive reinforcement obtained in studies on crickets and fruit-flies cannot be ascribed to a slight difference in conditioning and testing procedures (see discussion in ref. 8). We thus conclude that neurotransmitters mediating appetitive reinforcement indeed differ in crickets and fruit flies, whereas those for aversive reinforcement are the same."
1649970,4.0," The results of our studies in crickets raise a question about the diversity and evolution of reinforcement systems for associative learning among animals. There is evidence suggesting that dopamine neurons mediate appetitive reinforcement in mammals1,2, mollusks40 and fruit-flies14,15, whereas octopamine neurons have been suggested to mediate appetitive reinforcement in honey bees9,10. The diversity of neurotransmitters mediating appetitive reinforcement among invertebrates and vertebrates should emerge as a fascinating research subject."
1649970,5.0," The fact that appetitively reinforcing neurons in mammals and crickets use different neurotransmitters may indicate that they mediate different information for reinforcement. Results of our recent study, however, suggested that the signals that these neurons mediate are conserved between mammals and crickets. In mammals, it has been shown that whether appetitive learning occurs is determined by the discrepancy, or error, between the predicted reward and the actual reward41 and that certain classes of midbrain dopamine neurons mediate reward prediction errors1,2. In crickets, our behavioral and pharmacological studies suggest that octopamine neurons mediate reward prediction error33. We are currently investigating whether dopamine neurons mediate punishment prediction error in aversive learning in crickets."
1649970,6.0," We found no impairment in appetitive or aversive learning in Dop2 RNAi crickets. Since we did not evaluate to what extent the level of Dop2 protein is reduced in Dop2 RNAi crickets and since Dop2 is known to be expressed in some Kenyon cells in crickets24, more studies are needed to examine the possibility that Dop2 plays some roles in olfactory learning and memory. Insects possess several types of octopamine and dopamine receptors other than Dop1, Dop2 and OA125,26,27. Expression of these genes in the cricket brain and the possible participation of these receptors in learning and memory remain as subjects of our future study."
1649970,7.0," There is dense expression of Dop1 mRNA in Kenyon cells of crickets24. A high expression level of OA1 mRNA in Kenyon cells of the mushroom body has been reported in the fruit-fly31 and honey bee32, and there is an urgent need to confirm this in crickets. The mushroom body is known to play critical roles in olfactory learning in fruit-flies29, honey bees30, cockroaches42 and crickets43. Anatomical and physiological characterization of dopaminergic and octopaminergic neurons that make synapses with Kenyon cells is the next step for elucidation of the neural mechanisms of appetitive and aversive learning in crickets."
1649970,8.0," We conclude that the neurotransmitter and the receptor mediating appetitive reinforcement signals differ in crickets and fruit-flies. This urges us to examine neurotransmitter mechanisms for associative learning in different species of insects to evaluate the diversity, and evolution, of reinforcing mechanisms in insects. Moreover, consideration of the ubiquity and diversity of reinforcing mechanisms for associative learning in vertebrates and invertebrates should become an important research subject in neuroscience."
1962556,0.0,"We examined whether perturbing neurologically healthy individuals by adding noise to their reach endpoints would impair reinforcement learning in a manner similar to what has been observed in individuals with cerebellar damage (Therrien et al., 2016). Adding a low level of noise, to increase participants� baseline variability by 50%, did not impair learning relative to a control condition where no noise was added. However, adding a high level of noise (to increase baseline variability by 150%) significantly impaired learning. Increasing variability affects the mapping of hand location to reward. That is, in the presence of noise it is possible for the hand to be within the reward zone yet not be rewarded or, conversely, be rewarded when outside it. To assess whether reinforcing errors could account for impaired learning with high noise, we performed an additional experiment in which we artificially reduced (clamped) the reinforcement rate to match the reinforcement corresponding to reaches where both the hand and noisy locations were in the reward zone. In contrast to the noise conditions, in this additional task participants were never rewarded when the hand location was outside the reward zone. Reducing reward yielded learning similar to that in a control condition. Together, these results suggest that the reduced learning in the high-noise condition was driven by the reinforcement of incorrect behavior, rather than not reinforcing correct behavior. Finally, comparing performance in the high-noise condition to that of a group of patients with cerebellar damage showed similar total learning between the groups, but faster early learning in the high-noise condition."
1962556,1.0," Similar to previous work (Pekny et al., 2015), we found a larger change in reach angle following unrewarded trials, suggesting that participants tend to explore more following errors than after successful movements. However, when noise was added, this exploratory behavior was modulated by whether outcome feedback matched the true hand position. That is, participants showed greater change following unrewarded trials when the hand reach angle was outside the reward zone (i.e., an appropriate withholding of reward) compared with when the hand reach angle was actually correct (i.e., a false withholding of reward). In experiment 2, the change in reach angle was also greater for the unrewarded versus rewarded trials. However, there was no difference between control and clamp conditions. This is in contrast to the results of Pekny et al. (2015), who found that clamping reinforcement at a lower level increased variability following unrewarded trials. This discrepancy may have been the result of methodological differences between the two tasks. In their study, Pekny et al. (2015) clamped the reinforcement rate during a prolonged period where no rotation perturbation was applied. Thus, many subjects would have reached a plateau in performance before experiencing the clamp. A sudden reduction in the reward rate under these conditions may have prompted subjects to change their behavior to search for a new solution to the task. In our study, however, the clamp was applied during the rotation phase. Here, subjects would naturally experience changes in the reinforcement rate as the task solution changed with each rotation. As a result, subjects in our study may have been less likely to change their behavior, relative to the control condition, on the introduction of clamp."
1962556,2.0," Adding a high level of noise to reaches of healthy participants matched the total learning of a group of patients with cerebellar damage. However, healthy participants still showed a faster early learning rate than the patient group. To describe how variability from noise influenced learning in our task, we expanded a model developed in our previous work (Therrien et al. (2016). The simple mechanistic model assumes that trial-to-trial variability in subjects� reach angles stems from two broad sources termed �exploration variability� and �motor noise.� The important distinction between these sources of variability is that the sensorimotor system has access to the amount of exploration on any trial, but it does not have access to the motor noise on that trial. Although the model is framed in terms of motor noise and exploration variability, it is equally valid to view the motor noise as proprioceptive noise (or a combination of both motor and sensory noise), so that this noise limits the ability to localize the limb. As a result, when a reach is reinforced, the motor system can only learn from the magnitude of exploration that contributed to it. Thus, high motor noise may decrease the efficiency of learning by altering the mapping of the reach angle to the reinforcement signal. Here, we allowed exploration to vary depending on whether the previous trial was rewarded or not. Fitting the model to an individual participant�s task performance revealed that added noise increased the fitted motor noise in healthy participants to match that found in patients, but there were group differences in exploration variability. While patients with cerebellar damage showed similar exploration following rewarded trials compared with healthy control subjects with and without added noise, their exploration following unrewarded trials was reduced. This suggests that the patient group was less able to modify their behavior following errors than healthy participants, even when the level of noise was matched between groups."
1962556,3.0," A discrepancy in error sensitivity between our high-noise condition and the patients with cerebellar damage could have arisen for a number of reasons. Studies of visuomotor adaptation have shown that healthy individuals are able to detect false or variable feedback and explicitly alter their behavior so as to learn normally (Bond and Taylor, 2017; Morehead et al., 2017). Added noise in the present study was akin to providing participants with false feedback. Given that they had normal proprioceptive precision, it is possible they were aware of a discrepancy between the movements performed and the feedback received, which may have reduced their sense of agency over feedback about performance errors (Parvin et al., 2018). Furthermore, healthy participants may have been able to use an estimate of the discrepancy to adjust their response to achieve more rewarding feedback. In contrast, pathological motor variability from cerebellar damage is considered to be the product of faulty predictions of limb states (Miall et al., 2007; Miall and King, 2008), which result in poor compensation for limb dynamics and interjoint interaction torques during movement (Bastian et al., 1996; Bhanpuri et al., 2014). Therefore, in patients with cerebellar damage, noise may increase uncertainty about the movement performed�that is, decrease proprioceptive precision (Bhanpuri et al., 2013; Weeks et al., 2017a,b). While the feedback resulting from such movements can also be viewed as false, patients with cerebellar damage are likely to be less able to detect and estimate the discrepancy, making it difficult to detect the source of errors."
1962556,4.0," Previous work has addressed how motor noise can alter learning in a variety of motor tasks. There are several studies of error-based learning that have artificially added noise into various sensorimotor tasks. These have shown that, although performance degrades, participants change their behavior so as to be close to optimal in performance given the noise (Baddeley et al., 2003; Trommershäuser et al., 2005; Chu et al., 2013). Our finding that motor noise can impair motor learning is in agreement with a recent study of reinforcement learning by Chen et al. (2017). The purpose of that study was to understand the similarities between motor reinforcement and decision-making using tasks that were designed to have similar structures. They found that the decision-making task was learned faster and suspected that this was due to the motor noise present in the motor reinforcement task. In a separate experiment, they measured the level of motor noise outside of the reinforcement learning task and showed that the level of noise was inversely related to learning. That is, participants with more noise learned slower. However, they were able to equilibrate performance by artificially adding noise into the decision-making task. This suggested, as in our experiment, that variability from noise limits the ability to learn from reinforcement feedback."
1962556,5.0," In conclusion, we have shown that adding external noise to the movements of neurologically healthy individuals alters reinforcement learning in a motor task. Our findings suggest that high levels of noise primarily impair learning through the attribution of reinforcement to incorrect behavior. Not reinforcing correct behavior did not impair learning in our task, suggesting that it is less detrimental to the motor system. Additionally, adding noise to healthy individuals� reaches reduced total learning to a level similar to that of a group of patients with cerebellar damage. However, healthy participants showed a faster initial learning rate. We suggest that this may result from a discrepancy between the form of noise in the present study and the source of noise in the patients with cerebellar damage. That is, the added noise in our experiment did not disrupt participants� estimate of their actual behavior. This left a sufficient proportion variability accessible to the sensorimotor system, which may have supported a faster learning rate."
440047,0.0,"So far, we have shown that a simple reservoir-based network model may acquire task structures. The more interesting question is that why the network is capable of doing so and how this network model may help us to understand the functions of the OFC."
440047,1.0,"Encoding of the task space \n  We place a reservoir network as the centerpiece of our model. Reservoir networks are large, distributed, nonlinear dynamical recurrent neural networks with fixed weights. Because of recurrent networks� complicated dynamics, they are especially useful in modeling temporal sequences including languages [42, 43]. Neurons in reservoir networks exhibit mixed selectivity that maps inputs into a high dimensional space. Such selectivity has been shown to be crucial in complex cognitive tasks, and experimental works have provided evidence that neurons in the prefrontal cortex exhibit mixed selectivity [44�46]. In our model, the reservoir network encodes the combinations of inputs that constitute the task state space. States are encoded by the activities of the reservoir neurons, and the learned action values are represented by the weights of the readout connections. \n  There are several reasons why we choose reservoir networks to construct our model. First reason is that we would like to pair our network model with reinforcement learning. Reservoir networks have fixed internal connections; the training occurs only at the readout. The number of parameters for training is thus much smaller, which could be important for efficient reinforcement learning. Generality is another benefit offered by reservoir networks. Because the internal connections are fixed, we may use the same network to solve a different problem by just training a different readout. The reservoir can serve as a general-purpose task state representation network layer. Lastly, our results as well as several other studies show that neurons in reservoir networks�even with untrained connections weights�show properties similar to that observed in the real brain [24, 25, 47], suggesting training within the network for specific tasks may not play a role as important as previously thought. \n  The fact that the internal connections are fixed in a reservoir network means that the selectivity of the reservoir neurons is also fixed. This may seem at odds with the experimental findings of many OFC neurons shifting their encodings rapidly during reversals [48]. However, these observations may be interpreted differently when we take into account rewards. The neurons that were found to have different responses during reversals might in fact encode a combination of sensory events and rewards. On the other hand, there is evidence that OFC neurons with inflexible encodings during reversals might be more important for flexible behavior [49]. \n  The choice of a reservoir network as the center piece of task event encoding may appear questionable to some. We do not train the network to learn task event sequences. Instead, we use the dynamic patterns elicited by task event sequences as bases for learning. This approach has obvious weaknesses. One is that the chaotic nature of network dynamics limits how well the task states can be encoded in the network. We have illustrated the network works well for relatively simple tasks. However, when we consider tasks that have many stages or many events, the combination of possible states grows quickly and may exceed the capacity of the network. The fact that we do not train the internal network connections does not help in this regard. However, the purpose of our network model is not to solve very complicated tasks. Instead, we would like to argue this is a more biologically-realistic model than many other recurrent networks. First, it does not depend on supervised learning to learn task event sequences [47, 50]. Second, although the network performance may appear to be limited by task complexity, the real brain, however, also has limited capacity in learning multi-stage tasks [37]. Lastly, we show that a reservoir network can describe OFC neuronal responses during value-based decision making. Several other studies have also shown that reservoir networks may be a useful model of the prefrontal cortex [24, 25]."
440047,2.0," Reward input to the reservoir \n  One key observation is that reward events must also be provided as inputs to the reservoir layer for the network model to perform well. Including reward events allows the network to establish associations between sensory stimuli and rewards, thus facilitates task structure acquisition. Although reward modulates neural activities almost everywhere in the cortex, the OFC plays a central role in establishing the association between sensory stimuli and rewards [9, 48, 51, 52]. Anatomically, The OFC receives visual sensory inputs from inferior temporal and perirhinal cortex, as well as reward information from the brain areas in the reward circuitry, including the amygdala and ventral striatum, allowing it to have the information for establishing the association between visual information and reward [30�32]. Removing the reward input to the reservoir mimics the situation when animals cannot rely on such an association to learn tasks. In this case, the reservoir is still perfectly functional in terms of encoding task events other than rewards. This is similar to the situation when animals have to depend on their other memory structures in the brain�such as hippocampus or other medial temporal lobe structures�for learning. Consistent with this idea, it has been shown both the OFC and the ventral striatum are important for model-based RL [53]. The importance of the reward input to the reservoir explains the key role that the OFC plays in RL. \n  Several recent studies reported that selective lesions in the OFC did not reproduce the behavior deficits in reversal learning previously seen if the fibers passing through or near the OFC were spared [29]. Since these fibers probably carry the reward information from the midbrain areas, these results do not undermine the importance of reward inputs. Presumably, when the lesion is limited to the OFC, the projections that carrying the reward information are still available to or might even be redirected to other neighboring prefrontal structures, including ventromedial prefrontal cortex, which might take over the role of the OFC and contribute to the learning in animals with selective OFC lesions."
440047,3.0," Model-based reinforcement learning \n  The acquisition of task structure is a prerequisite for model-based learning. Therefore, it is interesting to ask whether our network model is able to achieve model-based learning. The two-stage task that we model has been used in human literature to study model-based learning [5, 6, 33�36]. Our model, although exhibiting behavior similar to human subjects, can be categorized as the Reward-as-cue agent that was described and categorized as a form of model-free reinforcement learning agent by Akam et al. [37]. Yet, with reward incorporated as part of the task state space, goal-directed behavior can be achieved by searching in the state space for a task event sequence that ends with the desired goal and associating the sequence with appropriate actions. Thus, our network could in theory support model-based learning by providing the task structure to the downstream network layers."
440047,4.0," Extending the network \n  The performance of our network depends on several factors. First, it is important that reservoir should be able to distinguish between different task states. The number of possible task states may be only 4 or 8 as in our examples, or may be impossibly large even if the number of inputs increases only modestly. The latter is due to the infamous combinatorial explosion problem. One may alleviate the problem by introducing learning in the reservoir to enhance the representation of relevant stimulus combinations and weed out irrelevant ones. A recent study showed that the selectivity pattern in the prefrontal neurons may be better explained by a random network with Hebbian learning [54]. Second, the dynamics of the reservoir should allow information to be maintained long enough in a decipherable form until the decision is made. The recent developed gated recurrent neural networks may provide a solution with units that may maintain information for long periods [55]. Third, the model exhibits substantial variability between runs, suggesting the initialization may impact its performance. Further investigation is needed to make the model more robust. Last, we show that a reinforcement learning algorithm is capable of solving the relatively simple tasks in this study. However, it has been shown that reinforcement learning is in general not very efficient for extracting information from reservoir networks. Especially, when the task demands the information to be held for an extended period, for example, across different trials, the current learning algorithm fails to extract such relevant information from the reservoir. A possible solution is to introduce additional layers to help with the readout [25]."
440047,5.0," Testable predictions \n  Our model makes several testable predictions. First, because of the reservoir structure, the inputs from the same source should be represented evenly in the network. For example, in a visual task, different visual stimuli should be represented at roughly the same strength in the OFC, even if their task relevance may be drastically different. Second, we should be able to find neurons encoding all relevant task parameters in the network, even when a particular combination of task parameters is never experienced by the brain. Third, reducing the number of inputs may make the network to be more efficient in certain tasks. This may seem counter-intuitive. But removing inputs reduces the number of states that the network has to encode, thus improves learning efficiency for tasks that do not require those additional states. For example, if we remove the reward input to the SEL, which is essential for learning tasks with volatile rewards, the network should however be more efficient at learning tasks in a more stable environment. Indeed, animals with OFC lesions were found to perform better than control animals when reward history was not important [56]."
440047,6.0," Summary \n  Our network does not intend to be a complete model of how the OFC works. Instead of creating a complete neural network solution of reinforcement learning or the OFC, which is improbable at the moment, we are aiming at the modest goal of providing a proof of concept that approaches the critical problem of how the brain acquires the task structure with a biologically realistic neural network model. By demonstrating the network�s similarity to the experimental findings in the OFC, our study opens up new possibilities in future investigation."
2347850,0.0,"As expected, we found that psychosis patients had deficits in ED set shifting. This deficit has been previously documented in chronic schizophrenia19,28�30 and in first-episode psychosis,23 although some \n studies in first-episode psychosis suggest that there may either be no deficit in this domain31 or that the deficit may be slight.22,32 Given that a number of previous studies have investigated ED set shifting in schizophrenia and early psychosis, here we focus our \n interpretation on the results concerning simple reinforcement learning and reversal learning."
2347850,1.0," Elliott et al18 and Pantelis et al28 previously demonstrated reversal learning deficits in chronic schizophrenia. Both these groups extracted reversal learning performance from a version of the ID/ED test: the same approach that we employ in this study. Waltz and Gold20 showed profound reversal deficits in 34 patients with chronic schizophrenia using a different method; they employed a probabilistic reversal task adapted from Robbins and colleagues.33,34 Our results demonstrate that reversal learning deficits are also present in many patients near the time of initial presentation to psychiatric services. We note that these deficits were not universal however, and many patients performed at comparable levels to controls (see figure 4)."
2347850,2.0," In contrast to Waltz and Gold,20 who argued that patients performed adequately on rule acquisition, we were able to detect subtle abnormalities in simple reinforcement learning (SD learning), possibly because of our large sample size. Our study thus provides further support to long-held contentions that there are reinforcement learning abnormalities in psychosis. Discrimination learning has been shown to be impaired by caudate tail lesions35; previous data supports caudate dysfunction in psychosis.13,36,37 Specifically, the tail of the caudate is itself connected to the medial temporal lobe, an area that is strongly implicated in the pathogenesis of psychotic illness38 as well as playing a role in discrimination learning. Research in rhesus monkeys has shown that lesions to the medial temporal lobe rhinal cortex, and to the inferior temporal cortex, result in mild and severe deficits, respectively, in discrimination learning, possibly through an inferior temporal-frontal-thalamic network.39�43 Thus, the SD deficit we note is also consistent with previous evidence for disruptedfrontotemporal connectivity in psychosis.44,45"
2347850,3.0," Patients with affective and nonaffective psychosis did not differ significantly in reversal learning errors (or indeed in EDS errors). Previous research has identified deficits in reversal learning to be present in bipolar mania,46 consistent with other recent research implicating orbitofrontal cortex dysfunction in mania (including manic psychosis), such as the presence of impairment on the Iowa Gambling Test.47 Interestingly, reversal learning is intact in euthymic bipolar disorder without a history of psychosis, suggesting a state-dependent deficit in nonpsychotic bipolar disorder.48"
2347850,4.0," Lesion studies in rodents and nonhuman primates have demonstrated a key role for the orbitofrontal cotrex and ventral striatum in reversal learning.49�53 Moreover, this evidence is corroborated from human functional imaging studies33,54,55 and from studies of human patients with orbitofrontal lesions.56,57 These regions are critical for motivational and goal-directed processing10; thus, the present study suggests that there is dysfunction of orbitofrontal/ventral striatal circuitry in psychosis. This contention is consistent with the findings of our correlational analysis in patients, which demonstrates that the greater the reversal impairment, the more severe the negative symptoms (ie, the greater the impairment in motivational and goal-directed behavior). We note that the specificity of this correlation should be viewed with caution because the magnitude of the significant correlation coefficient between negative symptoms and reversal errors (? = 0.3) differed only slightly from the nonsignificant correlation between positive symptoms and reversal errors (? = 0.2)."
2347850,5.0," Interestingly, we found that the patient group made few errors at the compound discrimination stage, which is in contrast with recent results reported by Jazbec et al.29 They studied 34 patients with chronic schizophrenia and found pronounced deficits in compound discrimination. It is possible that this process may deteriorate with disease progression, though longitudinal research will be required to examine this conjecture."
2347850,6.0," Our study does have a number of limitations. Although we found deficits in SD learning, the ID/ED test is not solely or primarily a test of this cognitive domain. Given that the test starts with SD learning, it is conceivable that some psychosis patients might have had trouble adjusting to the task environment in general, leading to an apparent specific deficit in this domain. In addition, there was only  small range in scores in SD learning, which limits the power of correlation and regression analyses to detect associations with clinical variables. For this reason, the failure to detect association between SD errors and clinical variables should not be overinterpreted. SD learning, and its association with clinical variables, merits further investigation in early psychosis in other cognitive paradigms that focus on SD learning in more detail."
2347850,7.0," Another limitation of the current study is that the majority of patients were taking second-generation antipsychotic medications. Such medications act on dopaminergic and serotonergic systems, and ascending serotonin and dopamine neurotransmitter systems are known to play a modulatory role in reinforcement learning processes.51,52,58 There are, however, a number of reasons why our current results are unlikely to be secondary to medication effects. First, we note that in a recent functional magnetic resonance imaging study in healthy volunteers, a low dose of the dopamine D2/D3 receptor ntagonist, sulpiride, did not modulate brain activations during reversal learning or impair behavioral reversal performance.59 Secondly, we observed a correlation between the level of negative symptoms and reversal errors, consistent with the theory that both these measures are secondary to one underlying pathological process. Finally, we have, in recent studies, demonstrated behavioral and physiological abnormalities during tests of reinforcement learning and motivational modulations in unmedicated first-episode psychosis patients.13,15 Future studies should examine reversal learning in unmedicated patients with psychosis, its relation to symptoms, and the extent to which reinforcement and reversal learning deficits can be modulated by pharmacological interventions. The relationship between reinforcement learning and reversal deficits and functional impairments also merits investigation."
1816166,0.0,"Analysis of time complexity and space complexity \n In this algorithm, for one sequence, many iterations of training are required to get its lowest energy. Therefore, the time complexity of the algorithm is determined by the length of the amino acid sequence (N) and the number of training iterations (I), that is, the time complexity is O(N?×?I). The time complexity of the ant colony algorithm for solving HP two-dimensional structure prediction is O(N?×?(N???1)?×?M?×?I/2), where N is the sequence length, I is the number of iterations, and M is the number of ants. The time complexity of particle swarm optimization is O(N?×?I?×?M), where N is the sequence length, I is the number of iterations, and M is the number of particles. Obviously, the time complexity of the method in this paper is the smallest of the three methods, and the larger the sequence length, the more prominent the time advantage. \n  The space complexity is composed of state-transfer function matrix and state-action value matrix. The rows of both matrices represent states, and the columns all represent actions. The number of rows in new state-transfer function matrix is 3N?1?12 and the number of columns is 3. The number of rows in state-action value matrix is 3N?12 and the number of columns is 3. So the space complexity is O3N?1?12×3+3N?12×3."
1816166,1.0," Case study \n  Sequence 12 is a zinc finger protein 528 (fragment), which is a transcription factor with a finger-like domain and plays an important role in gene regulation. Taking sequence 12 as an example, a series of optimized structures with the lowest energy obtained by the method of this paper under rigid criterion are given, as shown in Fig.�2a-c. The results of the last 100 samples of the method and the greedy algorithm and reinforcement learning with partial states in the training exploration process are given, as shown in Fig.�3a-c. The greedy algorithm itself cannot converge, and the convergence of reinforcement learning with full and partial states in the test process is shown in Fig.�4a, b. \n  For reinforcement learning with full states, the agent can be trained to select the better action to obtain a lower energy structure after training for several million times, and then guarantee that the structure obtained after convergence is the optimal structure, and it can be considered that the training effect of reinforcement learning with full states is stable. However, the greedy algorithm is not ideal for training. Only several structures with the lowest energy are trained occasionally, and the accuracy of the lowest energy structure cannot be guaranteed. As a whole, reinforcement learning with full states is better than the greedy algorithm. This is because, for reinforcement learning, the agent can choose better actions based on the previous interaction with the environment during the exploration process. Therefore, as the number of training increases, the agent can select the optimal action more quickly and accurately. Also, because of the setting of the reward function, the agent is more concerned about the overall situation without being trapped in a local optimum. The calculation of each plot in the greedy algorithm is independent, and the previous experience does not help the development of the current plot. As a result, the calculation amount becomes larger and the correct structure cannot be stably obtained. \n  From the testing process, it can be found that reinforcement learning with full states can maintain the lowest energy and achieve stable convergence after reaching the minimum energy. In contrast, reinforcement learning with partial states has fluctuations, cannot be stably maintained, and cannot reach the convergence state. This is because each state in the full state space is uniquely determined and can only be transferred by a unique state-action pair, and the process has Markov properties. However, the state in the partial state space can be transferred by different state-action pairs, which has partial uncertainty."
1816166,2.0," Full state space compares to partial state space \n  The full state space and the partial state space are two different descriptions of the state space in the 2D-HP model under reinforcement learning framework. The same point of the full and partial state spaces is that different states corresponding to each amino acid are set in advance, but they differ in the rules of the state setting. For the full state space, the number of states of subsequent amino acids is always three times the number of previous amino acid states. The state of the subsequent amino acid is obtained by a specific action of the previous amino acid in a specific state. That is to say, each state is transferred by a certain state-action pair, and the whole process has Markov properties. For the partial state space, the number of states for each amino acid except the first amino acid is four. The four states of the subsequent amino acid can be transferred from the four states of the previous amino acid through four different actions, and the whole process does not have Markov properties. The advantage of the full state space is that it can accurately find the lowest energy of the sequence and stabilize the convergence. The disadvantage is that the state space dimension is too high and the memory requirement is high, and the sequence with long length cannot be calculated. The advantage of partial state space is that the required state space is small, and it is possible to calculate a sequence with a long length. The disadvantage is that it cannot converge and cannot find the lowest energy of the sequence. \n  Function approximation is especially suitable for solving problems with large state space. The method described above for pre-setting the state-action value matrix and updating the state-action value matrix during the training process takes up a large amount of memory. Function approximation can be used to map the state-action value matrix to an approximation function (such as a parameter approximation function). Updating the parameter values with experimental data during the training process is equivalent to updating the state-action value, and finally a suitable approximation function is obtained. It can save memory space and solve the problem of sequence length limitation."
1786347,0.0,"We find that a reinforcement learning model with separate parameters after a win or a loss (WL) gives a significantly better description of human behavior in a two-alternative probabilistic learning task with rule reversals than the other models tested. Our VOL model, implementing the model of Behrens et al. (2007), is flexible and able to adapt to changes in the level of expected uncertainty, or feedback validity (FV), and volatility. However, the WL model is a better fit than the others although it has constant parameters throughout all trials. Behrens et al. (2007), in a broadly similar decision making task, found their model to be a better fit to human behavior than reinforcement learning with constant parameters for each participant or separate parameters for volatile and stable blocks. The difference between the fit of our WL model and standard reinforcement learning (RL) applies even when they are compared using a method that penalizes the WL model. In particular, the advantage of the WL model was observed when the BIC of the models was compared, a method which strongly penalizes models with higher numbers of parameters such as our WL model (Lewandowsky and Farrell, 2011)."
1786347,1.0," Comparing the performance of ideal agents on our task, we find no significant difference between the HMM and WL models. Ideal agents have parameters which are chosen to maximize rewards given the model and always choose the option given by the model as the most favorable. Bayesian models are constructed to make optimal decisions, providing that the assumptions underlying the models are correct. Although the assumptions of the Bayesian models are based on the experimental structure used to generate rewards, the HMM or VOL models do not outperform the WL model on our task although the WL model does not adjust its learning rate to accommodate different levels of unexpected uncertainty and volatility. There is a small but significant improvement in performance of the WL model compared to the RL model when using ideal agents. All of the models, when used by ideal agents, far outperform human behavior."
1786347,2.0," In our WL model, we find that for both the learning rate and temperature parameters, there is a significant difference between the fit parameter values following a win and a loss. These differences are consistent with psychological studies reporting behavioral differences in response to wins and losses and with existing neuroscientific knowledge indicating the existence of different neural pathways linked to the processing of wins and losses (see e.g., Kravitz et al., 2012; Yechiam and Hochman, 2013b)."
1786347,3.0," In psychology, the concept of loss-aversion, which suggests that behavior changes more in response to losses than to gains of similar magnitude (Kahneman and Tversky, 1984) has prompted much investigation. As an alternative mechanism to loss-aversion, Yechiam and Hochman (2013b) proposed a loss-attention mechanism in which losses cause participants to attend more closely to a task and so losses decrease the amount of randomization."
1786347,4.0," These ideas of loss-aversion are often tested in studies of response to risk, that is where participants choose between alternatives with known outcome probabilities. An example (from Kahneman and Tversky, 1984) is a choice between a safe or risky option, where the risky option has an 85% chance of winning $1000 and a 15% chance of winning nothing and the safe option pays out $800 with certainty. People tend to prefer the safe option. In their examination of the loss-attention hypothesis, Yechiam and Hochman (2013a), used several tasks which involved repeated selections between a safe and a risky option where the probabilities had to be learnt from experience. They tested their loss-attention model by fitting a choice sensitivity parameter, the inverse of our temperature parameter, for each task. They found less randomization of responses in tasks in which losses were possible compared to tasks without losses. In our task, unlike that of Yechiam and Hochman (2013a), the participants could not avoid losses as there was no way to predict the outcome on individual trials. We find a higher temperature after individual losses, implying that participants are less likely to follow the underlying belief after a loss. This does not necessarily conflict with the idea of loss-attention, as adding randomness to a response after a loss may be a mechanism for testing an underlying belief without making a large adjustment to that belief."
1786347,5.0," In neuroscience, dopamine is related to reward and punishment and separate D1 and D2 dopamine receptors in the basal ganglia have been found to respond to reward and punishment, respectively (see e.g., Gerfen, 1992). This inspired the use of separate pathways to respond to reward and punishment in the computational neural models of reinforcement learning by Frank and colleagues (see e.g., Frank, 2005; Samson et al., 2010). Testing this, Kravitz et al. (2012) found different pathways in the striatum of mice to be involved in processing reward and punishments. Rather than indicating reward and punishment directly, Schultz and colleagues suggested that dopamine signals the difference between an expected reward and that actually received (see e.g., Schultz, 1998). This difference forms the prediction error which is calculated in reinforcement learning."
1786347,6.0," Following their neural models with separate pathways for learning after a win and a loss, (see e.g., Frank, 2005; Samson et al., 2010). Frank et al. (2007) use separate learning rate parameters following a win and a loss when using a reinforcement learning model to analyze human behavior in a probabilistic task. Like us, they find that the mean learning rate following a win is higher than that after a loss. They use just one temperature parameter and, as they are looking at associations between genetics and reinforcement learning parameters, they do not compare alternative models of behavior."
1786347,7.0," We are aware of only a few studies that have considered separate effects of reward and punishment when comparing alternative computational models of learning from experience, none of which compare Bayesian models which make assumptions about the nature of the environment. These studies are based on different learning tasks to ours, and fit different reward values following a win or a loss (e.g., Ito and Doya, 2009; Guitart-Masip et al., 2012). Guitart-Masip et al. (2012) had four fractal images which signalled whether participants should respond or not to gain rewards or avoid punishments, these associations had to be learnt from experience and there was no switch in associations. Guitart-Masip et al. (2012) fit a number of different reinforcement learning models to behavior, the best fit model did not scale rewards and punishments differently. Analyzing the decisions of rats in two-stage probabilistic decisions, Ito and Doya (2009) found that a reinforcement learning model with different reward values after a win and a loss was a better fit to the rats' behavior than reinforcement learning without differentiation between wins and losses. To maintain the symmetry of the task in which exactly one response is correct on each trial, we have taken a different approach and fit a separate learning rate, rather than reward value, following wins and losses."
1786347,8.0," As our Bayesian and reinforcement learning based models make different assumptions about the environment, comparing the fit of different models to human behavior can give insights into the assumptions people make about the environment. Our HMM, as with that of Hampton et al. (2006), as well as assuming that the two outcomes are coupled, assumes that there will be rule switches within probabilistic feedback. Our VOL model not only assumes that there will be rule switches, but also that the frequency of switches depends on the level of volatility in the environment. Hampton et al. (2006) compared a hidden Markov model to a reinforcement learning model that made no assumptions about the structure of the environment. They concluded that participants make assumptions about the structure of the environment. We also found that our hidden Markov model (HMM) was a better fit to behavior than a reinforcement learning model which did not assume that the outcomes were coupled. This uncoupled reinforcement learning model, however, was not as good a fit as our RL and WL models. From this we conclude that participants made some assumptions about the environment but have no evidence that they adjusted their rate of learning to the structure of the environment."
1786347,9.0," Bayesian models can optimize the number of rewards received when the assumed structure for the Bayesian inference exactly matches the underlying structure of the task. We examined the performance of our models when, rather than being fit to human behavior, the model parameters were selected to maximize the total reward achieved, we refer to this as an ideal agent using the model. In our task, the rewards obtained by an ideal agent using the WL model were not significantly different to those of the ideal HMM. Our ideal HMM has parameters to closely resemble the generative structure, but assumes that there is a small constant probability of a rule switch. In the experimental data, rule switches only occurred at the ends of blocks of 30 or 120 trials. The HMM also assumes that for each environmental state, there is a constant probability of each outcome. However, the experimental data was generated using two levels of FV, with outcomes randomized to give the correct proportion within a block. We do not believe that these differences between the generative process and the assumptions of the HMM significantly hamper the performance of the HMM. We believe that the ideal agent using the WL model is approaching an optimal level of response in this task. The ideal WL model had a small but significant advantage over the ideal RL model in our task. Our ideal HMM also performed significantly better than our VOL model, our implementation of the model of Behrens et al. (2007). The VOL model is more flexible in the situations in which it can learn."
1786347,10.0," The parameters for the ideal WL model, those which gave the best performance in the task, were learning rates of 0.48 after a win and 0.24 after a loss. Our participants also had significantly higher learning rates after a win than a loss, although generally higher than the ideal parameters, with means of 0.76 and 0.52 after a win and a loss, respectively."
1786347,11.0," Learning under expected uncertainty with volatility is not simple as indicated by the range of participants' responses. However, our task, having coupled outcomes in which one or the other response is correct, does not require any exploration, or trying the different alternatives to see if things have changed. We expected the participants to know that if the button press was incorrect, then the other button would have been correct. Exploration is an important feature of learning from experience (see e.g., Cohen et al., 2007). Tasks which have more than two options automatically require exploration, as negative feedback does not show what would have been the correct response. It will accordingly be more difficult to learn when there are more alternatives. It has been acknowledged that standard reinforcement algorithms are not suitable in complex situations in which there may be many possible states or actions (e.g., Botvinick et al., 2009). Wilson and Niv (2012) compared optimal performance between a Bayesian and non-Bayesian model in their probabilistic learning task and the Bayesian model clearly had superior performance. Our finding that our ideal WL reinforcement learning model performs as well as our HMM may be restricted to the case of coupled two alternative tasks. Additionally, the level of feedback validity might affect the relative performance of the different styles of responding. These issues remain to be investigated."
1786347,12.0," We assumed that whatever decision making processes the participants use to make their responses, these remain constant for the whole task. We have assumed that the task instructions give participants enough information to form a model. Some studies have found that a Bayesian model is a better fit to human behavior only in conditions when participants have been told to expect changes in rule (Payzan-LeNestour and Bossaerts, 2011; Wilson and Niv, 2012). In our study, participants were not given such information. The instructions given to participants can affect behavior in other ways, Taylor et al. (2012) found that probability matching was influenced by whether participants had an explanation for the probabilistic outcomes."
1786347,13.0," In summary, we conclude that, with distinctions between learning from a win and a loss, reinforcement learning provides a very good description of the participant responses to repeated trials under expected uncertainty and volatility. It is able to account for individual differences with parameters that remain constant throughout all trials although the feedback validity and volatility varied. Future research should explore whether the differential treatment of a win and a loss would lead to a similar robust performance in other experimental situations."
2183937,0.0,"These results establish that the AcbC contributes to learning of actions when the outcome is delayed. Lesions of the AcbC did not impair instrumental learning when the reinforcer was delivered immediately, but substantially impaired learning with delayed reinforcement, indicating that the AcbC 'bridges' action-outcome delays during learning. Lesions made after learning also impaired performance of the instrumental response in a delay-dependent fashion, indicating that the AcbC also contributes to the performance of actions for delayed reinforcement. Finally, the lesions did not impair the perception of relative reward magnitude as assessed by responding on identical concurrent interval schedules for reinforcers of different magnitude, suggesting that the impulsive choice previously exhibited by AcbC-lesioned rats [22] is attributable to deficits in dealing with delays to reinforcement."
2183937,1.0,"Effect of delays on instrumental learning in normal animals \n  Delays have long been known to retard instrumental learning [1,37]. Despite this, normal rats have been shown to acquire free-operant responding with programmed response-reinforcer delays of up to 32 s, or even 64 s if the subjects are pre-exposed to the learning environment [1]. Delays do reduce the asymptotic level of responding [1], though the reason for this phenomenon is not clear. It may be that when subjects learn a response with a substantial response-reinforcer delay, they never succeed in representing the instrumental action-outcome contingency fully. Alternatively, they may value the delayed reinforcer slightly less; finally, the delay may also retard the acquisition of a procedural stimulus-response habit and this might account for the decrease in asymptotic responding. It is not presently known to what degree responses acquired with a response-reinforcer delay are governed by declarative processes (the action-outcome contingency plus a representation of the instrumental incentive value of the outcome) or procedural mechanisms (stimulus-response habits), both of which are known to influence instrumental responding [38,39]; it is similarly not known whether the balance of these two controlling mechanisms differs from that governing responses learned without such a delay. \n  Effect of AcbC lesions on instrumental learning and performance with or without delays \n  In the absence of response-reinforcer delays, AcbC-lesioned rats acquired an instrumental response normally, responding even more than sham-operated controls. In contrast, blockade of N-methyl-D-aspartate (NMDA) glutamate receptors in the AcbC has been shown to retard instrumental learning for food under a variable-ratio-2 (VR-2) schedule [in which P(reinforcer | response) ? 0.5] [40], as has inhibition or over-stimulation of cyclic-adenosine-monophosphate-dependent protein kinase (protein kinase A; PKA) within the Acb [41]. Concurrent blockade of NMDA and DA D1 receptors in the AcbC synergistically prevents learning of a VR-2 schedule [42]. Once the response has been learned, subsequent performance on this schedule is not impaired by NMDA receptor blockade within the AcbC [40]. Furthermore, infusion of a PKA inhibitor [41] or a protein synthesis inhibitor [43] into the AcbC after instrumental training sessions impairs subsequent performance, implying that PKA activity and protein synthesis in the AcbC contribute to the consolidation of instrumental behaviour. Thus, manipulation of Acb neurotransmission can affect instrumental learning. However, it is also clear that excitotoxic destruction of the AcbC or even the entire Acb does not impair simple instrumental conditioning to any substantial degree. Rats with Acb or AcbC lesions acquire lever-press responses on sequences of random ratio schedules [in which P(reinforcer | response) typically declines from around 1 to 0.05 over training] at near-normal levels [44,45]. In such ratio schedules, where several responses are required to obtain reinforcement, there is no delay between the final response and reinforcement, but there are delays between earlier responses and eventual reinforcement. It is therefore of interest that when differences between AcbC-lesioned rats and shams have been observed, AcbC-lesioned animals have been found to respond somewhat less than shams on such schedules late in training, when the ratio requirement is high [44,45], consistent with our present results. However, lesioned rats are fully sensitive to changes in the instrumental contingency [27,44,45]. Our present results indicate that when AcbC-lesioned rats are exposed to a FR-1 schedule for food [P(reinforcer | response) = 1] in the absence of response-reinforcer delays, they acquire the response at normal rates. \n  In contrast, when a delay was imposed between responding and reinforcement, AcbC-lesioned rats were impaired relative to sham-operated controls, in a systematic and delay-dependent fashion. The observation that learning was not affected at zero delay rules out a number of explanations of this effect. For example, it cannot be that AcbC-lesioned rats are in some way less motivated for the food per se, since they responded normally (in fact, more than shams) when the food was not delayed. Thus although the Acb and its dopaminergic innervation are clearly very important in motivating behaviour e.g. [23,46-48], this is not on its own a sufficient explanation for the present results. An explanation in terms of a rate-dependent impairment is also not tenable, since the AcbC-lesioned rats were capable (in the zero-delay condition) of responding at a level greater than they exhibited in the non-zero-delay conditions. Depletion of Acb DA also impairs rats' ability to work on high-effort schedules, where many, or very forceful, responses are required to obtain a given amount of food [47,48]. However, in the present experiments the ratio requirement (one response per reinforcer) and the force required per press were both held constant across delays, so this effect cannot explain the present results. Similarly, although AcbC lesions are known to impair the control over behaviour by Pavlovian conditioned stimuli e.g. [23,29,49-52], there was no Pavlovian stimulus that was differentially associated with delayed as opposed to immediate reinforcement in this task, so this cannot explain the present results. \n  Our results also indicated that when there were programmed delays to reinforcement, AcbC-lesioned animals experienced longer response-reinforcer collection delays, partly due to their failure to collect the reinforcer as promptly as shams. These additional experienced delays probably retarded learning. However, in addition to this effect, there was a further deficit exhibited by AcbC-lesioned rats: even allowing for the longer response-collection delays that they experienced, their instrumental learning was impaired more by delays than that of sham-operated controls. Deficits in learning with delayed reinforcement may account for some of the variability in the effect of AcbC lesions or local pharmacological manipulations on instrumental learning across different schedules. \n  The fact that pre-exposure to the context improves instrumental learning in normal rats [1] suggests one possible mechanism by which AcbC lesions might retard learning when delays are present. When a reinforcer arrives, it may be associated either with a preceding response, or with the context. Therefore, in normal animals, pre-exposure to the context may retard the formation of context-reinforcer associations by latent inhibition, or it might serve to retard the formation of associations between irrelevant behaviours and reinforcement. Similarly, non-reinforced exposure to the context forces the subjects to experience a zero-response, zero-reinforcer situation, i.e. P(outcome | no action) = 0. When they are then exposed to the instrumental contingency, such that P(outcome | action) > 0, this prior experience may enhance their ability to detect the instrumental contingency ?P = P(outcome | action) - P(outcome | no action). In one aversive Pavlovian conditioning procedure in which a conditioned stimulus (CS) was paired with electric shock, AcbC lesions have been shown to impair conditioning to discrete CSs, but simultaneously to enhance conditioning to contextual (background) CSs [53], though not all behavioural paradigms show this effect [54,55]. It is therefore possible that enhanced formation of context-reinforcer associations may explain the retardation of response-reinforcer learning in AcbC-lesioned rats in the presence of delays. \n  The instrumental task used requires animals either to associate their response with the delayed food outcome (an action-outcome association that can be used for goal-directed behaviour), or to strengthen a stimulus-response association (habit) when the reinforcer eventually arrives [38,39]. Both mechanisms require the animal to maintain a representation of their past action so it can be reinforced (as a habit) or associated with food when the food finally arrives. This mnemonic requirement is not obviated even if the animal learns to predict the arrival of food using discriminative stimuli, and uses these stimuli to reinforce its responding (conditioned reinforcement): in either case, since the action precedes reinforcement, some trace of past actions or stimuli must persist to be affected by the eventual delivery of food. \n  A delay-dependent impairment was also seen when AcbC lesions were made after training. This indicates that the AcbC does not only contribute to the learning of a response when there is an action-outcome delay: it also contributes to the performance of a previously-learned response. Again, AcbC-lesioned rats were only impaired when that previously-learned response was for delayed (and not immediate) reinforcement. Of course, learning of an instrumental response depends upon the animal being able to perform that response; preventing an animal from pressing a lever (a performance deficit) would clearly impair its ability to learn an instrumental response on that lever to obtain food. In the present set of experiments, it is clear that AcbC-lesioned rats were just as able to perform the response itself (to press the active lever and to discriminate it physically from the inactive lever) as controls, as shown by their normal performance in the zero-delay condition, so it is not clear whether the delay-dependent impairments in learning and performance can be attributed to the same process. Again, since responding was unaffected in the zero-delay condition, many alternative interpretations (such as a lack of motivation to work for the food) are ruled out. It may be that AcbC-lesioned rats are impaired at representing a declarative instrumental action-outcome contingency when the outcome is delayed, or in forming or executing a procedural stimulus-response habit when the reinforcing event does not follow the response immediately. It may also be that they represent the action-outcome contingency normally but value the food less because it is delayed, and that this affects responding in a free-operant situation even though there is no alternative reinforcer available. \n  Excitotoxic lesions of the whole Acb do not prevent rats from detecting changes in reward value (induced either by altering the concentration of a sucrose reward or by changing the deprivational state of the subject) [27]. Such lesions also do not impair rats' ability to respond faster when environmental cues predict the availability of larger rewards [28], and nor does inactivation of the Acb with local anaesthetic or blockade of AMPA glutamate receptors in the Acb [56]; the effects of intra-Acb NMDA receptor antagonists have varied [57,58]. AcbC-lesioned rats can still discriminate large from small rewards [24,25]. Similarly, DA depletion of the Acb does not affect the ability to discriminate large from small reinforcers [59-61], and systemic DA antagonists do not affect the perceived quantity of food as assessed in a psychophysical procedure [62]. Our study extends these findings by demonstrating that excitotoxic AcbC lesions do not impair rats' ability to allocate their responses across two schedules in proportion to the experienced reinforcement rate, even when the two schedules are identical except in the magnitude of the reinforcements they provide, thus demonstrating their sensitivity to reinforcer magnitude is quantitatively no worse than shams'. In this experiment, there was substantial undermatching, but this is common [33,63] see also [64,65]; differential cues signalling the two rewards might have improved matching but were not used in the present experiments since it is known that AcbC lesions can themselves affect rats' sensitivity to cues signalling reinforcement [23,29,49-52]. Given that AcbC-lesioned subjects showed a reduced probability of switching between two identical RI schedules, it may be the case that an enhanced sensitivity to the COD accounts for the better matching exhibited by the AcbC-lesioned rats [34]. Alternatively, the lesion may have enhanced reinforcer magnitude discrimination or improved the process by which behaviour allocation is matched to environmental contingencies. In summary, the present results suggest that AcbC damage leads to pathological impulsive choice (preferring a small, immediate reinforcer to a large, delayed reinforcer) [22] not through any relative lack of value of large reinforcers, but through a specific deficit in responding for delayed reinforcement."
2183937,2.0," Contribution of the AcbC to reinforcement learning \n  The term 'reinforcement learning' simply means learning to act on the basis of reinforcement received; it is a term used in artificial intelligence research [66] that does not specify the mechanism of such learning [67,68]. Our present results indicate that the AcbC is a reinforcement learning structure that is critical for instrumental conditioning when outcomes are delayed, consistent with electrophysiological and functional neuroimaging evidence indicating that the ventral striatum responds to recent past actions [10,15] and to predicted future rewards [8-15], and with computational models suggesting a role for the striatum in predicting future primary reinforcement [20,21]. However, when reward is certain and delivered immediately, the AcbC is not necessary for the acquisition of instrumental responding. The delay-dependent role of the AcbC indicates that it plays a role in allowing actions to be reinforced by bridging action-outcome delays through a representation of past acts or future rewards. Acb lesions have also produced delay-dependent impairments in a delayed-matching-to-position task [69,70]; their effects on the delayed-matching-to-sample paradigm have also been studied, but a more profound and delay-independent deficit was observed, likely due to differences in the specific task used [71]. Finally, the AcbC is not alone in containing neurons that respond to past actions and future rewards. The dorsal striatum is another such structure [10,15,72,73]; expression of stimulus-response habits requires the dorsal striatum [74,75], and the rate at which rats learn an arbitrary response that delivers electrical stimulation to the substantia nigra is correlated with the degree of potentiation of synapses made by cortical afferents onto striatal neurons, a potentiation that requires DA receptors [76,77]. The prelimbic area of rat prefrontal cortex is important for the detection of instrumental contingencies and contributes to goal-directed, rather than habitual, action [78,79]. Similarly, the orbitofrontal cortex and basolateral amygdala encode reinforcement information and project to the AcbC, and lesions of these structures can produce impulsive choice see [24,80-82]. It is not yet known whether lesions of these structures also impair learning with delayed reinforcement."
908754,0.0,"We applied microstimulation in SN of patients undergoing DBS for the treatment of PD as they performed a reinforcement learning task. We found that microstimulation applied during the 500-ms post-reward interval impaired learning. These results demonstrate a causal relation between post-reward SN firing and human reinforcement learning as microstimulation is known to acutely enhance local neural firing (Histed et al., 2009). We hypothesized that the effect of SN microstimulation on learning would vary based on their relative proximity to dopaminergic (DA) neurons that guide reinforcement learning (Glimcher, 2011) or GABAergic neurons that exert inhibitory control on DA neurons (Damier et al., 1999a; Lobb et al., 2011; Ramayya et al., 2014b). As hypothesized, we observed the largest stimulation-related impairments in learning when the electrode was positioned near neurons with relatively high firing rates and narrow waveforms, properties characteristic of GABA neurons (Joshua et al., 2009; Matsumoto and Hikosaka, 2009; Ungless and Grace, 2012). Thus, our results suggest that microstimulation near GABA neurons impairs reinforcement learning."
908754,1.0," This finding provides direct evidence relating phasic SN neural firing to human reinforcement learning. It goes beyond animal electrophysiology studies that may not generalize to human learning because they typically involve long periods of intense training. It also goes beyond prior human studies of reinforcement learning; functional neuroimaging studies cannot test a causal role for SN neural activity (Montgomery et al., 2009), and pharmacological manipulations of DA in patients with PD (Frank et al., 2004; Rutledge et al., 2009) cannot distinguish phasic neural activity from tonic changes in DA throughout the brain (Niv et al., 2007). Ramayya et al. (2014a) also showed a stimulation-related decrease in performance. However, because rewards in that study were contingent on stimuli, but independent of actions, the observed stimulation-related decrease in performance could either be attributed to an impairment of learning or a selective strengthening of action-reward associations that competed with stimulus-reward learning. Our current study overcame this limitation by using an experimental design with consistent stimulus-response mapping, such that stimulus-reward and action-reward associations were always correlated. Thus, our finding of a stimulation-related impairment in performance suggests decreased learning."
908754,2.0," In Ramayya et al. (2014a), stimulation-related decreases in performance were correlated with an increased propensity to repeat the same action following reward, particularly when the electrode was positioned near putative DA neurons, suggesting that microstimulation near SN DA neurons enhanced action-reward learning. The current finding that stimulation near putative GABA neurons produced impairments in reinforcement suggests opposing roles of DA and GABA neurons during reinforcement learning. Specifically, if phasic bursts of SN DA neurons encode reward prediction errors that result in subsequent learning (Glimcher, 2011), and SN GABA neurons provide inhibitory inputs to local DA neurons (Tepper et al., 1995; Luscher and Ungless, 2006; Lobb et al., 2011; Henny et al., 2012; Pan et al., 2013), then one would observe enhanced learning when stimulating DA neurons (Ramayya et al., 2014a), but impaired reinforcement learning following microstimulation of SN GABA neurons. This explanation is also supported by our observation of opposing post-reward firing responses from putative DA and GABA neurons in the human (Ramayya et al., 2014b)."
908754,3.0," It is difficult to interpret whether the observed changes reinforcement learning were related to changes in stimulus-reward and/or action-reward learning because these forms of learning were perfectly correlated in the current experimental design. That we did not observe robust stimulation-related changes in learning near putative DA sites is difficult to interpret when considering our previous finding that microstimulation near putative DA neurons enhances action-reward learning (de Berker and Rutledge, 2014; Ramayya et al., 2014a). It is possible that we did not sample from a functional population of DA neurons in this study, as suggested by the absence of phasic post-reward bursts in activity from putative DA neurons in this study, unlike our previous studies (Ramayya et al., 2014a,b). Alternatively, it is possible that stimulation near SN DA neurons has a specific effect on action-reward learning that was not evident in this study because it was masked by simultaneous stimulus-reward learning."
908754,4.0," An alternative explanation for how microstimulation of SN GABA neurons might have resulted in impaired learning is that stimulation may have caused a behavioral change during the post-reward interval that impaired subjects' learning during those trials. Several studies have linked the firing of SN GABA neurons in the pars reticulata subregion (that contains the majority of SN GABA neurons; Nair-Roberts et al., 2008) to regulation of downstream movement and saccade-generating structures (e.g., superior colliculus; Carpenter et al., 1976; DeLong et al., 1983; Hikosaka and Wurtz, 1983). If microstimulation of SN GABA neurons suppressed orienting saccades that likely occurred in response to the presentation of salient reward stimuli (in this case, a silver dollar and the sound of cash register; Hikosaka and Wurtz, 1983), then reward stimuli presented during stimulation trials might be associated with diminished salience and result in reduced learning. However, this is unlikely to be the case because non-human primate studies have shown that SN microstimulation has a limited influence on visually-guided saccades (Mahamed et al., 2011)."
908754,5.0," We note several limitations to our study. First, we are unable to provide direct histochemical evidence that electrophysiological parameters (spike rate and waveform duration) indicate distinct neuronal populations, however, a large body of evidence from animal studies suggest that these electrophysiological criteria may be used to identify distinct midbrain neuronal populations (Ungless and Grace, 2012). Second, we did not observe stimulation-related changes in learning near putative DA neurons in this study, whereas we observed such changes in our previous microstimulation study (Ramayya et al., 2014a). This likely reflects reduced sampling of DA neurons during this experiment, which is consistent with the fact that we did not observe post-reward bursts of activity in this study (a marker of DA activity), in contrast to Ramayya et al. (2014a). Finally, the population we studied�patients undergoing DBS surgery for PD�is known to have degeneration of DA neurons in SN. Even though this poses the challenge of interpreting findings concerning the functional role of SN neurons in patients who have degenerative disease, histological studies in PD patients (Damier et al., 1999b), and electrophysiological studies in rat models of PD (Hollerman and Grace, 1990; Zigmond et al., 1990), and humans (Zaghloul et al., 2009; Ramayya et al., 2014b) indicate that a significant population of viable neurons remain in the parkinsonian SN. Taken together with the clear evidence of learning that subjects demonstrated during the task, we suggest that the neural processes we describe reflect the subpopulation of healthy neurons that remain in the SN."
1904820,1.0,"RDTs are generally useful for diagnosing malaria infection [3 5]. However, the sensitivity of RDTs depends upon the species of malaria parasite. The highest sensitivity was observed for P. falciparum malaria (78.8 99.1%) [5, 11, 15 18], followed by P. vivax malaria (77.6 96%), P. ovale malaria (18.4 80.0%), and P. malariae malaria (21.4 47.0%) [3 5, 19, 20]. For non-falciparum malaria, RDT sensitivity was particularly low for P. ovale and P. malariae, a finding that may be attributable to the low parasitemia [3 5], the difference in targeted antigens [6, 9, 10], or the genetic variability between the infected parasites [21, 22]."
1904820,2.0,"Low parasitemia was associated with false-negative results from the RDTs regardless of malaria species [4, 5]. Moreover, since reinfection and semi-immune status generally cause low parasitemia [23], those influences need to be eliminated. Travellers who visiting families and relatives and expatriates living in malaria endemic areas were excluded from our study because they were at a higher risk of reinfection. The parasitemia in P. ovale malaria was consequently as low as that in immune patients and was significantly lower than that in P. vivax malaria. This finding, along with those in previous reports [23], suggests that P. ovale malaria presents with low parasitemia even in non-immune travellers, which may result in false-negative results from the RDTs. As mentioned above, P. malariae and P. ovale malaria may be difficult to diagnose by RDTs due to the low parasitemia [7, 8, 11]. Recently, Houz  et al. [24] reported that the new RDT, Clearview, has improved sensitivity for P. malariae malaria but that the sensitivity for P. ovale malaria was not improved. Moreover, even a case of P. ovale malaria with relatively high parasitemia (21,150 parasites/?L) resulted in a false-negative RDT result in their study, implying some reason for false-negative results other than low parasitemia in P. ovale malaria. The difference in target antigens from parasites, such as HRP2 and PfLDH for P. falciparum or pLDH and aldolase for pan-malaria species, can also influence the sensitivity of RDTs [6]. Detecting P. ovale and P. vivax, two RDTs (BN for aldolase detection and SDMA for pLDH detection) were used in our study. SDMA showed relatively good sensitivity for both P. vivax and P. ovale as compared to BN, probably because pLDH-based RDTs generally perform better than aldolase-based RDTs [20]. Bigaillon et al. [6] also reported the ineffectiveness of BN in detecting P. ovale malaria, which they suggested was due to low aldolase production by P. ovale malaria [10]. Aldolase-based RDTs generally show low sensitivity not only for P. ovale malaria but also for other types of malaria [9], but low sensitivity was also observed for pLDH-based RDTs [10 12, 16, 17, 24]. Although the genetic variability of HRP2 was related to the low sensitivity of RDTs for P. falciparum malaria [21], it was reported that the genetic variability of pLDH [22] and aldolase [25, 26] did not explain the relatively poor performance of RDTs for the detection of P. falciparum, P. vivax, and P. malariae. Talman et al. [22] reported that P. ovale exhibited three different types of amino acid sequence (named O1, O2 and O3), whichmay contribute to the relatively poor detection of P. ovale. The sensitivity of RDTs is still insufficient for accurate diagnosis of P. ovale malaria regardless of the type of antigens. Therefore, microscopic examination is preferable for the definitive diagnosis of P. ovale malaria. The sensitivity of RDTs was not high enough to accurately diagnose P. ovale malaria in Japanese travellers who were never infected with malaria previously. Thus, microscopic examination is required to ensure that P. ovale malaria is not overlooked."
359836,1.0,"This analysis applied 18-month follow-up data from a Phase III clinical trial to estimate the potential public health impact of the RTS,S candidate malaria vaccine in 42 malaria-endemic countries in sub-Saharan Africa using a cohort model. Two vaccination strategies were considered, with doses administered either at 6, 10 and 14 weeks (corresponding to adding the RTS,S vaccine to the EPI schedule) or at 6, 7-and-a-half and 9 months. Each strategy was compared with no vaccination to estimate the expected impact of RTS,S vaccination on averting clinical malaria cases, severe cases, malaria hospitalizations and malaria deaths in a birth cohort followed to the age of 10 years. Vaccination at age 6, 10 and 14 weeks would be expected to avert over five million cases of clinical malaria, 119,000 severe malaria cases, 98,600 malaria hospitalizations and 31,000 malaria deaths. Vaccination at age 6, 7-and-a-half and 9 months would be expected to avert almost 12.5 million cases of clinical malaria, 250,000 severe malaria cases, 208,000 malaria hospitalizations and 65,400 malaria deaths. The wide CIs reflect the combination of heterogeneity and uncertainty allowed for by the stochastic process in the model. The greatest uncertainty is around the impact on mortality."
359836,2.0,"After calibration, the model-predicted results closely matched the incidence of malaria observed in the control arm of the Phase III trial, and the published age distribution of malaria cases [14]. However, when extrapolating at the country level, the modelled estimates for the number of clinical malaria cases and malaria deaths were larger than the estimates for 2010 in the WHO World Malaria Report 2012, which estimated the number of clinical malaria cases in the WHO Africa region in all age groups at 174 million (upper bound 242 million), and the number of malaria deaths at 596,000 (upper bound 772,000) [26]. Possible reasons for the differences are The model is calibrated to the incidence of clinical malaria in the control arm of the Phase III trial based on the secondary case definition (parasite density of >0 and presence of fever). It may be possible that the secondary case definition would have captured cases of fever due to other illnesses and the malaria parasites observed were present as a co-morbidity rather than the primary cause. The primary case definition (parasite density of >5000 and presence of fever) would be more specific and lead to a reduction of estimates by 35 %. Estimated malaria episodes from WHO World Malaria report may not sufficiently adjust for under-reporting, although a correction factor is used; The model estimated malaria incidence based on reported proportion of children within categories of parasite prevalence from the MAP project. If this proportion varies over the year or the force of infection varies within a category, this is not captured by the model. Finally, the case-fatality rate for severe cases not treated in hospital is subject to a large uncertainty. An estimate from Thwing et al. [22] is applied in the model but this factor has a large impact on malaria mortality estimates (see Fig. 5a, b). However it should be noted that clinical vaccine efficacy does not change with the primary or secondary case definition."
359836,3.0,"Obtaining a realistic estimate of the number of severe malaria cases and malaria hospitalizations is important as these outcomes contribute to understanding the health burden of malaria. However, available data are limited and generally relate to populations with access to treatment. For example, the Phase III trial was conducted in settings where all patients had access to a high level of care. It was found that about half of the patients hospitalized with malaria met the case definition for severe malaria (parasite density <5000), which is similar to the value reported in a study of hospitalized patients in Tanzania, where 4261 patients of the 9337 with positive blood slides had severe disease (46 %) [20]. However, neither of these studies could provide data on the fraction of severe malaria cases that did not result in hospitalization, or on the risk of severe malaria developing in patients without access to treatment. The present analysis attempted to adjust for limited access to prompt and high-quality treatment for uncomplicated cases and limited access to hospitalization for severe cases."
2052982,1.0,"A rapid diagnostic test based on the detection of P. falciparum specific HRP-2 antigen is one of the few diagnostic tools available for health care workers in many resource limited settings in malaria endemic settings to assist them in differentiating between malaria fever and other causes of fever in many malaria. As previously reported, HRP-2 based RDTs displayed a low specificity (59%) in the current study area [18]. This low specificity of HRP2 based RDT was also reported in other malaria endemic areas [13, 26,27,28]. Therefore, expert microscopy was performed in the present study in the central laboratory of CRUN to further determine (by cross-checking) whether antimicrobials were appropriately prescribed by the attending health care staff. Despite the good attitude of health care workers to nearly always use malaria RDT in case of a febrile patient and act accordingly to the result of diagnostic testing (92.89 of adherence of health care workers to malaria RDT), the prescription of antimalarials is affected by the performance of the malaria RDT used in the study area [23]. Due to the persistence of HRP2 antigen after successful antimalarial treatment, malaria infection is overestimated (as a result of a false positive RDT) leading to an inappropriate prescription of antimalarials [13,14,15,16]. Furthermore, even if the diagnosis malaria was established by RDT, there was a tendency to prescribe antibiotics next to antimalarials too. Moreover, and even worse, the prescription of antibiotics becomes almost systematic when a malaria infection could be excluded. The majority of outpatients were visiting the rural health facilities where there is a lack of laboratory facilities that can confirm the actual cause of infection and this explains the inappropriate prescription of antibiotics at this level. In contrast, at the level of the referral hospital these facilities are available, but there the attending hospital health care workers might not feel confident to postpone antibiotic treatments in febrile children until the microbiology results become available in fear of treating a potentially life threating, but treatable, bacterial infection too late or overlooking such an infection. Given the fact that health care workers tend to adhere well to malaria RDT results as reported in our study area [this study and previous study by Ruizendaal et al. [29], it is most probable that health system may be able to deal with the problem of inappropriate prescription of antibiotics as long as the health care workers get the appropriate diagnostic tools. Therefore, the development of (near) point-of-care tests to screen for other causes of fever becomes essential to guide appropriate antibiotic prescriptions."
2052982,2.0,"The inappropriate prescription of antibiotics and antimalarial treatments observed in the present study could be a consequence of the rightful fear of health care workers to miss a treatable bacterial infection. Indeed, a significant number of potentially treatable etiologies were missed in the present study and others [18, 30]. Despite the tendency of systematic prescription of antibiotics to febrile children, an important number of children with bacterial infections did not receive appropriate treatment mainly at the referral hospital at the first contact compared to the rural health facilities. This observation can be explained by the availability at referral hospital of equipped laboratory allowing for a better diagnostic compared to health facilities where malaria RDT is the only diagnostic tool available. Moreover, in Burkina Faso, nurses working at health facilities are responsible for the primary management of patients, while at the referral hospital there are medical doctors who are better trained to make a specific prescription. The fear to overlook a potential bacterial infection by sending a sick child back home without prescribing medication leads to the antibiotic prescriptions to an important part of febrile children without bacterial infections. This tendency of systematic prescription of antibiotics to febrile children is considered in many setting to be more a social and behavioral issue than a medical problem [31]. To tackle this problem, there is a need to evaluate and understand the context in which the (over) prescriptions occur. Moreover, more diagnostic tools should become available that can be implemented in resource limited settings that can aid proper antibiotics prescriptions and thereby reducing the risk of inappropriate prescription and of emerging of antibiotic resistance. Antiparasitics drugs were not much prescribed in this study. Based on the laboratory findings, there was a high under-prescription of antiparasitics as actually a significant number of febrile children who actually had gastro-intestinal parasites did not receive appropriate treatment. Most likely, the more obscure symptoms associated with a parasitic gastro-intestinal infection, which in most cases do not cause fever, are probably overlooked in favor of other febrile etiologies that are more readily diagnosed."
2052982,3.0,"Despite the recommendation of the WHO to treat all children with a positive malaria RDT or microscopy result, few cases (4.52%) of febrile children who had a malaria positive RDT did not receive antimalarials. The reason for this non adherence was not assessed in the present study. Possibly, complementary information on previous drug use provided by the parents/guardians to the attending nurses suggested that these children did not require antimalarial prescriptions and that the RDT were positive in these cases was interpreted as a consequence of persisting HRP2 antigen (up to 4 weeks or even more) after successful treatment for malaria [13,14,15,16]. In contrast, 14% of the children with a malaria RDT negative result still received antimalarial prescriptions. It has not been determined in the present study what the rationale was behind this non-compliance. This lack of adherence to the protocols in place for the management of malaria has also been reported elsewhere [32]. Therefore, it is of utmost importance to further study the motivation of health care workers to not adhere to this protocol recommended by the WHO and the National Malaria Control Program (NMCP) and this should preferably be done, in combination with to further training and education of the health practitioners. A possible limitation of our study could be the lack of confirmation of pneumonia cases presumptively diagnosed by health care workers. Although the nasopharynx of some children was colonized by common bacterial pathogens, such as Staphylococcus aureus and Streptococcus pneumoniae, a relation of their presence with ongoing fever has not been established [22]. Therefore, it might be possible that some of these children did not have pneumonia. Moreover, RTI are the second cause of fever diagnosed presumptively by health facility nurses after malaria in the present study. The WHO guideline recommends the prescription of antibiotics to children with (suspected) pneumonia but not to children with acute bronchitis [1, 20, 33, 34]. In the present study, a radiological confirmation of pneumonia could not be done in children suspected of RTI, because this diagnostic technology is was not available at the participating health centres. Another possible limitation of the present research is the potential under-diagnosis of the number of UTI and GII cases as some urine and stool samples were not collected during recruitment. Finally, viral etiologies are known to be responsible for a significant number of fever episodes in children [17, 35, 36], but these were not studied in the present study as the CRUN laboratory does not have facilities to perform virus identification. Although viral febrile infections do not need antibiotic treatment, they can also be a cause of over prescriptions as there is fear of overlooking potential bacterial infections. A rapid diagnostic test (or combination of tests) that can broadly differentiate between bacterial and viral infections would be very helpful in this respect [37,38,39]."
1904879,1.0,"A prevalence of 27.7% for malaria was observed by microscopy in this study population. This prevalence was similar to the 29.6% reported by Apinjo et al. [28] in children <?15?years of age in the South West Region of Cameroon. On the contrary, higher prevalence of 46.7% and 36.9% in children less than 15?years has been reported in other areas in the Mount Cameroon area [2]. Besides a decline in prevalence, this study also observed a remarkable fall in geometric mean parasite density from 2131 parasites/?L [25] to 187 parasites/?L in this population. Similarly, the proportion of febrile children in this population was 3.4% and has also experienced substantial decrease from 27.1% [25] in Dibanda and 41.9% [34] in different communities of the Mount Cameroon area. The decline in morbidity in the study area is not surprising because recently, Dibanda has been experiencing urban transformation from a farm land site/village setting into a settlement area  new layout.  Various authors have postulated that increased urbanisation reduces the number of breeding places for Anopheles mosquitoes and consequently malaria transmission [35, 36]. Also, in Dibanda, there is a general change in house type from predominantly plank houses [28] to cement block houses which may have played a role in curbing the disease burden. It has been reported that a plank house has crevices for mosquito entry [37] and provides a favourable microenvironment for mosquitoes [38]. In addition, the decrease in malaria morbidity could also be the result of sustained control measures as reported by WHO [2] and Sumbele et al. [4]. Malaria prevalence was higher in females than males by microscopy and the findings are in line with Kimbi et al. [32] who reported that females spend more time outdoors at dusk and dawn than males to perform household chores and as such are more exposed to mosquito bites. It is not surprising that children under 5 years had the highest parasitaemia when compared with the older children. Children under 5 years are more vulnerable to the disease in areas of high transmission [2, 30, 34] as naturally acquired immunity builds up in older children following repeated exposure to the parasite. In line with the findings of Ebai et al. [34] in other parts of the Mount Cameroon area, children from individuals with no formal or with primary education were more infected with the malaria parasite than those with secondary or tertiary education. Higher levels of education are generally associated with improved knowledge and practices in relation to appropriate prevention and treatment strategies. The high prevalence of anaemia (72.7%) in children less than 15?years is not an uncommon finding in this part of the country, and this highlights anaemia as a major public health problem among the population in this area. The relationship between malaria parasitaemia and anaemia is well established in previous studies [2, 30, 39]. However, findings in the area [30] reported a low risk of anaemia attributable to malaria highlighting the insidious and important contribution of other inflammatory infections or diseases which were not investigated in the study."
1904879,2.0,"In the present study, CareStart RDT sensitivity (82.4) and specificity (76.6) did not reach the values indicated by the manufacturer (98% and 97.5%, respectively). The PPV was 57.4%, and the NPV was 91.9%. This could be explained by the difference between populations used to estimate these parameters. In the present study, we evaluated a population with all levels of parasitaemia, whereas the manufacturer s tests were done on samples with a density of >?200 parasites/?L of blood. The results from this study are consistent with similar studies reported in Africa. A study by Wanja et al. [40] in Western Kenya showed that CareStart (G0181, G0131 and G0141) had a sensitivity range of 86.9 95.48%, specificity range of 71.2 81.4%, PPV range of 59.2 72.8 and NPV range of 91.9 97.9. Also, Maltha et al. [41], in a reference setting showed the sensitivity of the CareStart Malaria HRP2/pan kit for the detection of P. falciparum to be 84.8 92.0%. Findings from this study demonstrated a higher CareStart RDT (G0141) performance characteristics when compared with a previous study carried out by Ndamukong-Nyanga [21] in the Mount Cameroon area (sensitivity?=?48.5%, specificity?=?84%, PPV =?62.3% and NPV?=?75.0%) even though their mean parasite density/?L of blood (2333, range 18 16,000) was higher. However, a lower sensitivity in performance could be attributed to a prozone effect which has been reported as a cause of false negative HRP-2 RDT [42]. On the other hand, a study by Maltha et al. [43] in Burkina Faso demonstrated higher sensitivity, specificity, PPV and NPV of 100%, 70.9%, 69.4% and 100%, respectively, for PfHRP2 detection. Likewise, a study by Diallo et al. [42] in Senegal demonstrated that CareStart  RDT showed high sensitivity (97.3%) and specificity (94.1%) with PPV and NPV of 97.3 and 94.1%, considering polymerase chain reaction as standard. The higher performance of the HRP2 RDT from these studies was probably due to their study design. Unlike our study which was on non-clinical cases and had as limitation a less sensitive gold standard the Giemsa-stained microscopy when compared with the highly sensitive PCR, these studies were all hospital based and the occurrence of fever was an inclusion criterion. The sensitivity of the RDT in this study increased with an increase in parasite density achieving a sensitivity of 96.1% at parasite density greater than or equal to 200 parasites/?L of blood, as recommended by WHO [17]. This is in line with several studies [40, 41, 44] where the sensitivity increased to >?95% at parasite densities greater than 200 parasites/?L of blood. This finding confirms the inability of RDTs to reliably detect malaria parasites at very low parasite densities. The CareStart RDT was positive for all febrile children with malaria (100%). This study also demonstrated that febrile children were 6.7 times at higher odds to be positive by RDT when compared with afebrile children. A sensitivity of 100% was reported by Maltha et al. [43] among hospitalised febrile children in Burkina Faso. Moreover, studies by Wilson [45] reported that patients with febrile illness in endemic areas are likely to be diagnosed with malaria. While the CareStart RDT could be considered a useful diagnostic tool for ruling out malaria in cases where good Giemsa microscopy is not available or appropriate in the health facility, its applications in communities and population-based screening with a higher proportion of asymptomatic cases are limited. Findings from the study showed a low positive predictive value (57.4%) indicating a high false positive rate. This could probably be due to prior infection and subsequent effective treatment as reported in the structured questionnaires. This study indicated that children who had a history of fever within a month prior to the survey were 4.6 times more likely to be positive by RDT. The HRP2 antigen detected by this RDT has been demonstrated in several studies to persist in the blood stream between 14 and 30?days before being cleared completely [46, 47]. Moreover, as a limitation in the study, rheumatoid factor which was not measured has been suggested to produce false positive due to binding IgG [48]. The relatively high negative predictive value (91.9%) in this study indicates the likelihood of the RDT to correctly identify a child without malaria as true negative, hence showing a substantive ability to differentiate between children who had malaria and those without. Hence, this RDT could be confidently used to confirm negative test patients as non-malaria patients. The false negative results produced by CareStart (G0141) could be accounted for by deletions or mutations of the HRP2 gene such that the parasite no longer produces the antigen or produces the mutant antigen that is not recognised by antibodies on the test strip [22]. This study revealed that children under 5 years were 2 times more likely to be positive for RDT when compared with older children. Similar results with a decrease in sensitivity of the Paracheck-Pf RDT in older age groups have been reported in Tanzania [49]. This is probably because younger children are more vulnerable to the disease and have higher parasite loads when compared with older children. This observation is in line with reports from other studies [2, 50]. Findings in this study also revealed higher odds of having a positive RDT among anaemic than non-anaemic children. This is probably because of the likelihood of anaemic children having Plasmodium infection and a higher parasite load. Malaria has been reported by several authors to contribute substantially to anaemia in malaria-endemic regions [39, 51] and this study attests to this."
544861,1.0,"Majority of the facilities still use syndromic approach in the diagnosis of malaria. This finding is not surprising as this had been previously reported from many endemic countries [6]. Using a syndromic approach in diagnosis of malaria means increased likelihood of unnecessary prescribing of antimalarials [11], because some other febrile illnesses that are not malaria might have been treated as malaria. Also the use of, laboratory diagnosis (RDTs and microscopy) was low in this study. The low use of microscopy at 39.2% for diagnosis in this study is similar to the findings of a previous study in Nigeria where a study based on an audit of 665 patients' records from public and private hospitals found that 45% of patients had diagnostic blood slides [20]. Laboratory diagnosis can improve rational provision of malaria treatment service as it has been found that prescribing anti-malarials only after laboratory confirmation reduced the total number of prescriptions by 68% in Malawi [21]. The level of awareness of RDTs by all the providers was not high enough for such an item of enormous utility as RDTs. This calls for the employment of means of creating awareness about RDTs among health workers. If people do not know about a new product and the likely benefits that could accrue from its use, they are not likely to use it and this may lead to market failure. Doctors, CHEWs and laboratory technologists were more likely to be aware of RDTs than nurses. This is not unusual for the laboratory technologists since their main job is to conduct tests. The respondents in urban areas and in public facilities were more likely to be aware of RDTs. The reason for this may be that more attention has been paid to public facilities by government and partner agencies in recent times to improve the case management of malaria in Nigeria."
544861,2.0,"If more than half of the respondents said RDTs were ever available at their facility of work, and yet the rate of use is low, it then becomes a cause for concern and a threat to the current effort to improve the case management of malaria. Some health workers gave the unreliability of RDTs as a reason for not utilizing available RDT kits. This suggests they do not trust the results despite the fact that RDTs have been found to have a sensitivity of 90.6% and a specificity of 95.9% in Nigeria [9, 10]. It has been noted that health workers still treat for malaria even when RDT result is negative [22]. However, most of the health workers who are still using RDTs tend to be satisfied with the results they get. It is possible that poor technique, or even poor preservation of the RDT kits could give rise to poor results which made some health workers to say they stopped using RDTs because it was not reliable. Heat-stability has been noted to be a major concern for some RDTs, especially under field conditions and the health workers may have been exposed to different brands of RDTs including those with health stability problems [14]. Interestingly, RDTs were more available in the rural facilities than urban facilities, a finding that favours the scaling-up of RDTs since a majority of Nigerians live in rural areas. However, the fact that the government was the source of RDTs for only 3 facilities is worrisome. How does W.H.O. intend to promote the RDT use if the government is obviously lacking any interest? This is further confirmed by the fact that the source of information on RDTs was never through formal training sessions promoted by government, as it should be. Although RDTs are still new in Nigeria and currently there are no policies in place on its usage in the diagnosis of malaria except for that from W.H.O, government should play a lead role and make RDTs available to more public facilities."
544861,3.0,"Surprisingly, the drug of choice for the treatment of uncomplicated malaria in the study areas was ACT. This is particularly a positive development in the push to improve the case management of malaria in Nigeria. However, this finding contrasts with a Nigerian survey of malaria control practices that showed that less than a fifth of the primary and secondary health facilities used the recommended ACT [2] and that monotherapies such as Chloroquine, SP, Quinine, Artesunate and Dihydroartemisinin were still widely used for treatment of malaria [26]. ACTs especially the recommended first-line types in the national treatment policy (Artemether-Lumefantrine (AL) and Artesunate+Amodiaquine (AA) were readily found in public facilities in diverse trade names. However, it will be noted that as a matter of policy, ACTs are supplied free of charge to children who are under 5 years in public health facilities in Enugu, Nigeria [27] and this may have accounted for the large presence of ACT in these facilities. The Nigerian malaria control programme also delivered 4.5 million courses of ACT in 2006 and 9 million in 2007 [4]. ACTs can also be purchased over the counter without a prescription, and can be dispensed by a non medical personnel. In Nigeria, pregnant women and children receive free SP and ACTs respectively from all public health facilities; however this does not apply to private facilities. In the private sector charges are fixed by the owners of the facilities while in government facilities there are often specified fees for services."
544861,4.0,"The study shows that ACTs were still not readily available in private facilities but were more available in both types of facilities than RDTs. This calls for strategies to ensure that both ACTs and RDTs are made available to private health care providers at a subsidized rate in form of public private partnership. But in doing this, there is need for sustainable monitoring systems as monitoring and influencing the quality of private services is recognized as a key component of effective malaria treatment [28] However, the fact that ACTs are readily available (and not RDTs) and are used and considering the fact that most health workers still employ syndromic approach for the diagnosis of malaria, it then means that some patients will be treated with ACTs without laboratory diagnosis. Parasitological diagnosis of malaria is an important parameter leading to the appropriate use of anti-malarial drugs. Improper and abusive use of ACTs without proper diagnosis will have a direct clinical and economic impact [6, 29, 30]. This therefore calls for interventions at policy and programmatic levels to improve treatment provision. And one obvious intervention will be ensuring that providers stocked adequate doses of RDTs and ACTs and subsequently used them so as to decrease unnecessary treatment and reduce societal costs of malaria. We did not review the brands of RDTs found in these facilities and we acknowledge this to be a limitation of the study. Further studies should audit the type of RDTs in these facilities as they vary greatly in effectiveness. The sampled facilities may not be a representative sample for the whole country and therefore the results may not be representative of the country. Nevertheless, we believe that this is a representative sample of all health facilities in the area of study and the state and therefore a good starting point in understanding the tremendous gap existing between the optimal, W.H.O. promoted policies of RDTs and ACTs and the real application in practice."
361983,1.0,"This study showed that in an area with moderate malaria transmission, more than a third of patients with positive HRP2-based RDT tests had a negative blood film and may have been incorrectly diagnosed as a case of clinical malaria due to persistence of the HRP2 antigen from an earlier infection. It is possible that a few patients with submicroscopic or low levels of parasitaemia might have been wrongly classified as negative with microscopy. However this number is likely to have been very small as blood films were read carefully by two experienced microscopists. In contrast, nearly all RDT positive patients in a site with low endemicity were true positives. False positive error rates declined with increasing age of patients, probably as the result of acquired immunity in clearing parasite antigens. Previous studies have shown that HRP2-based RDTs can lead to high false positive error rates. Swarthout et al reported that by using Paracheck-Pf , 73% of cases were still RDT test positive 35 days after treatment and that the false positive error rate correlated with initial parasite density [6]. Iqbal et al found nearly 35% of patients still had HRP2 antigenaemia 14 days after treatment despite negative blood films [9]. In another study, 61% of patients had positive HRP2-based RDT tests for more than two weeks after initiation of treatment [10]. These antigens are eventually cleared by anti-HRP2 antibodies, especially anti-HRP2 IgG [11]. The study also showed that the specificity of RDTs varied seasonally in the same area. At the mesonendemic site (Kebisoni), specificity increased as the true parasite rate (as determined by microscopy) decreased at the end of the transmission season. This may have followed from boosting of anti-HRP2 antibodies as a result of infections acquired during the preceding few months. The relatively higher specificity of RDTs at the hypoendemic site compared with the mesoendemic site could, on the other hand, be due to a very low probability of finding patients with recently cleared parasitaemia who sought treatment for non-malarial illnesses."
361983,2.0,"Sensitivity of RDTs was not affected by age of patient or fluctuation in parasite rates during different months. It was, however, affected by parasite density. Patients with high parasite densities were more likely to test positive than those with low parasitaemia. Other studies have also indicated that HRP2-based tests have high sensitivity which increases with parasite density [3]. However, sensitivity can vary from area to area. There are variations in the repeat section of the HRP2 protein between parasite isolates from different areas which might be a reason for wide variations in sensitivity of HRP2-based RDTs in different areas [12]. A study carried out in Uganda showed that the PPV was only 20% at a site of low endemicity (in Kabale District) whereas in other areas with higher endemicity it was much higher, while the NPV was uniformly high (> 97%) [13]. In the present study, estimates of both the PPV and NPV were high at the site with low endemicity in Kenya. Although it has been suggested in one study that sensitivity is affected by age-dependent immune status of patients independent of parasite density [14], no evidence of this phenomenon was found in the present study. It might be argued that some of the variability observed between the sites and during different parts of the year could have resulted from performance variability of the tests used, especially as climatic conditions affect the stability of the devices. Stability is usually more problematic with pLDH-based tests than with HRP2-based tests. Due to variable rates of use in different sites, the use of similar batches of the tests across all sites and seasons could not be ensured. Nevertheless, the devices were purchased at the same times for both countries, and from the same manufacturer during the entire study period. The devices were also stored and used within the recommended temperature and duration. Due to these and the fact that the sensitivity of the tests in Kenya and Uganda were similar (90.0% and 91.0%, respectively), variability in performance of the devices is unlikely to have played a major role. The use of RDTs is probably cost-effective in many situations. A simulation study has indicated that at a 95% confidence level, RDTs are cost-effective compared to presumptive treatment below 62% parasite prevalence rates [2]. However, cost-effectiveness of RDTs can be compromized if patients with negative RDT tests are prescribed antimalarials [15] as has been shown in both Tanzania [16] and Zambia [17] to frequently be the case. A danger of reliance on RDTs is that some patients who require malaria treatment may test negative and be given symptomatic treatment only. In this study, the fact that there were 9% false negative RDT tests among microscopically confirmed cases at Kebisoni, some of whom had high parasitaemia, shows the risk of relying on test results alone. Due to potential variations in the accuracy of RDTs by season, as suggested by the present study, seasonal use of these diagnostic tools may be necessary after careful cost-effectiveness studies in some areas, especially those with mesoendemic transmission."
361983,3.0,"One of the major implications of the findings of the present study is that cost-effectiveness of HRP2-based RDTs is greatly influenced by variations in their sensitivity and specificity between different areas, age groups of patients, and seasons. The study showed that the diagnostic accuracy of HRP2-based RDTs is relatively high in areas or seasons with low transmission, but more area-specific operational studies may be required to evaluate their cost-effective use under different transmission scenarios. For decisions involving the use of these tests, policymakers should take into account the cost implications of treating test negative patients [15], as well as the risk of not treating false negative patients. The cost-effectiveness of HRP2-based RDTs depends on a multitude of factors: overall diagnostic accuracy, prevalence and its seasonal fluctuations, seasonal changes in test specificity, age group of study subjects, parasite density, the relative cost of antimalarials and RDT tests, the relative treatment costs of test negative cases and the extent to which clinicians trust the outcomes. RDT test results should always be interpreted together with clinical assessment of the patient, allowing for fallibility of the devices [20]. In some vulnerable patients (e.g. children), the risk of leaving a false-negative case untreated for malaria may outweigh the costs of over treatment based on clinical diagnosis [4]. Authorities in charge of developing malaria diagnostic policies may have to interpret reports on specificity of HRP2-based RDTs and cost-effectiveness analyses on their use with some caution as there may be wide variations in the determinant factors of accuracy between different studies. In some areas, it may be useful to vary the use of HRP2-based RDTs according to factors such as transmission level, season and age group of patients, but such policies should be based on further area-specific investigations. Especially in situations where the diagnostic accuracy of RDTs is unlikely to be high, health services will need to strengthen microscopy."
451939,1.0,"Our study aimed at assessing the cost-effectiveness of RDT in a systematic manner. Overall, we identified fifteen studies that tried to delve out whether RDT was cost-effective compared with other commonly used malaria diagnosis methods and there was heterogeneity in population age, funding sources, economic and effectiveness measures, and other general study settings across studies. Our analysis took the influence of such variability into account and found that most studies provided supportive evidence in terms of the cost-effectiveness of RDT. However, there were still five studies that did not draw a clear conclusion [28, 33, 34, 36, 38]. This difference can be explained by the accuracy of RDT, the performance of its comparisons, clinicians  compliance with the diagnostic results, total treatment costs, and malaria prevalence. Therefore, we were unable to conclude which strategy would be the most cost-effective with certainty. A wide range of perspectives has been selected by the studies included. Although most of the studies under the societal and health sector perspectives supported the cost-effectiveness of RDT, some could not because of the uncertainty in the costs and an unclear WTP threshold. This might suggest that the diagnosis and treatment of malaria can be unaffordable to patients in many countries. The recommended first-line malaria treatment, artemisinin-based combination therapy (ACT), is expensive and possible increase in treatment costs over time due to therapy resistance and drug prices has been seldom considered by researchers in the field trials. A cost-effective intervention can be considered to receive public funding if it is a public good, or has important externalities and inadequate demand, or is catastrophically unaffordable and has no available insurance, or beneficiaries are poor when utility outcome is not available [43]. Given the expensive treatment costs, it is suggested that malaria case management with RDT should be included in the coverage of health insurance to substantially reduce the economic burden on patients and their families [44, 45]. Another key driver for the cost-effectiveness of RDT is its price [27, 30, 31, 33, 36, 41]. The price of RDT can be determined by its type as combo RDTs are usually more costly than the single tests. In our analysis, it is uncertain whether the cost-effectiveness of RDT could be influenced by the types of RDT. Also, the capability of combo tests to identify plasmodium species can largely influence the cost-effectiveness of RDT because the type of RDT selected will determine not only the accuracy of diagnosis [46], but also the following treatment received. Therefore, for most countries where multiple malaria species dominate, it is necessary to differentiate Plasmodium species such that proper treatment could be delivered. This systematic review included studies from low- and middle-income countries that were assumed to be malaria endemic. The cost-effectiveness of RDT compared to microscopy was not clear in regions with relatively low transmission settings given the uncertainty in how the routine microscopy was performed, i.e., the accuracy of microscopy and whether the microscope was used only for malaria detection. Current evidence suggested that RDT could be more cost-effective than microscopy [29, 37, 39, 41], and the relative advantage of RDT could be further enhanced if microscope was exclusively-used [37]. This could be explained by the fact that the demand for malaria diagnosis would be less in area where malaria prevalence is close to zero, and the cost per suspected patient would be largely increased when taking microscopy as the initial approach. Further studies are required to confirm this, especially in low transmission countries aiming at eliminating malaria. Moreover, facing the reduction of malaria prevalence and movements towards disease elimination [47], it is more common for countries to confront the threat of increasing malaria imported cases [6]. Usually, imported patients are either rural migrant workers or travelers to the endemic region, and tend to have lower parasite densities. The key challenge is to promptly and accurately identify malaria cases at all levels of health systems. Current malaria control programs have established either active or passive case detection systems. Active case detection requires health workers to seek out for patients, making it less feasible to maintain the use of microscopy as the initial approach when the prevalence is extremely low. In addition to this, our requirements for malaria elimination, especially a consistent diagnostic accuracy for community-based primary care, is beyond the capacity of routine microscopy due to the scarcity of well-trained microscopists [48], and essential laboratory supplies. This may limit the performance of microscopy [49], and contribute to misdiagnosis or over-diagnosis with a potential risk of over-consuming antimalarial therapies and drug resistance [50]. In fact, the poor performance of routine microscopy has been widely recognized, even in developed countries [51], and high capital investment of microscopy makes it more costly than RDT if local caseload is low [52]. Therefore, it is meaningful and economically important to introduce RDT in primary health care or remote region where microscopy is unavailable."
451939,2.0,"The CHEERS tool was used to assess the quality of evidence in our research, allowing to compare reporting quality across included studies. The majority of studies identified are of good and moderate quality, but we still found some studies showed poor compliance with the reporting guidance, especially lacking details of research methods. Explanation of model selection was lacked generally, and this might be because studies tended to have more concern about whether RDT was a quick and accurate way to detect malaria cases. It should also be noticed that the scores of studies aimed at measuring the cost-effectiveness of RDT were higher than those only evaluating the costs of the disease detection approaches. In addition, the CHEERS tool focuses on the quality of reporting, and it should be fully considered at the stage of study design, for example, by referring to the structural abstract proposed by NHS Economic Evaluation Database (NHS EED) and extracting basic characteristics and results of health economic evaluations to improve the quality of evidence."
451939,3.0,"This review is limited in the following aspects: firstly, the studies identified were conducted in a limited range of countries, most of which were located in Africa. Whether results obtained from the context can be transferable to other countries was not elaborated in the included studies. It is obvious that countries may vary in their widely-used malaria diagnostic methods and other features such as prevalence and the types of RDT. Differences in health care systems and reimbursement also limit the transferability of our results. Thus, caution should be taken when applying the results to other settings. Another limitation was inadequate data on costs and effectiveness, possibly due to differences in adopting primary and secondary outcome indicators among studies, adding to the difficulty in comparing ICERs obtained when they had the same perspectives. Therefore, no synthesized outcome was shown due to the wide difference across studies and the lack of evidence regarding health utility gained when using different malaria diagnostic techniques. We suggest that further economic evaluations of malaria detection methods should focus on health utility benefits for patients who are susceptible to the disease."
359124,1.0,"Most causes of fever in the tropics are transient, non-fatal illnesses. In the latter half of the 19th century, it was discovered that many of the infectious causes of fever that did have the potential to be fatal, including malaria, leishmaniasis, tuberculosis, sleeping sickness and others, were detectable by microscopy. For one of these, malaria, there was a relatively early pharmacological intervention thanks to the long history of quinine (cinchona) use in the Americas. The utility of microscopy in tropical fevers, and the availability of life-saving treatment for malaria, led to the wide advocacy of microscopy-based case management as the standard of care. When microscopy was found to be untenable in many parts of the world, due to the massive effort and resources required to maintain such a service in close proximity to the rural and poor populations widely at risk, syndromic management, classifying all 'malaria-like' fevers as malaria, again became the de facto standard of practice. For the last three quarters of a century then, since the discovery and development of chloroquine in the 1930s and 40s, the medical community has treated most fevers in malaria-endemic countries as malaria, forgoing the diagnostic process. This practice has been codified into national and international recommendations and training manuals for health workers, especially for fever in children. The common teaching has been 'fever equals malaria unless proven otherwise'. Clearly many lives have been saved by pushing for rapid, even community or home-based access to antimalarial therapy, regardless of diagnostic testing. In the many communities in which malaria has accounted for the majority of potentially fatal causes of fever, it has been hard to imagine any other approach, given the poor performance and relative unavailability of microscopy [1 6]. Over the past decade, though, a number of important changes have taken place in the epidemiology and control of malaria and in the diagnostic techniques available, that dramatically alter the balance of rational action in favour of parasite-based diagnosis over blind therapy of fever with anti-malarial drugs. Specific diagnosis of malaria is now not only possible, but necessary, and scaled up malaria control efforts, including elimination plans, must include expanded and quality assured use of parasite-based diagnostic testing and reporting of results."
1012871,1.0,"The Study was conducted in the outpatient department of paediatrics LUMHS Hyderabad from June to December 2010. All febrile children under 5 years of age classified as suspected clinical malaria confirmed slide microscopy, and also the effectiveness of RDT was measured by using expert microscopy as gold standard. The sensitivity (95%), specificity (91.6%), PPV (0.55%) and NPV (99.3%) of RDT were similar to that reported elsewhere.16 Overall, the performance of RDTs was similar to that of microscopic analysis performed by an experienced microscopist. Compared to microscopy, RDT specificity relative to that of microscopy was lower (91.0%) is similar to other studies.13,16,17 Our study has very low PPV which is similar to other of other studies 11,13 The low PPV is due to the fact that RDT detect the antigens but not the parasite as slide microscopy, and antigens may persist in the serum for couple of weeks even after successful treatment giving false positive results 17 . But RDT sensitivity was excellent (95.%) and an excellent NPV of 99.5%. These encouraging results justify using RDTs to diagnose malaria in areas that are most in need of low-cost diagnostic techniques. The risk of a false-negative RDT result depends on several factors: the RDT itself (brand and even lot variation, including performance in practice) and malaria species, density, and background prevalence. Well-performed laboratory studies have shown RDT sensitivities to be generally same 18."
2593776,1.0,"A number of studies on RDTs have been conducted, although measures of accuracy have varied widely, as a result of differences in methodology, study site epidemiology and type of RDT used i.e. histidine rich protein - 2 (HRP-2) and plasmodium specific lactate dehydrogenase (pLDH) and species specific pLDH or aldolase based test [27], [28], [17]. Ideally, bivalent RDTs will help to target anti-malarial treatment for P. vivax and P. falciparum by grass root workers who provide on the spot diagnosis and treatment in areas where microscopy cannot be established. The required sensitivity of a test may also vary with species, a less sensitive test may be acceptable for detection of P. vivax compared to detection of P. falciparum as severe outcomes due to missed diagnosis are less likely [29]. The present study assessed the performance of various bivalent RDTs in field population. Overall, highest sensitivity for P. falciparum was >95% and for P. vivax ?80%. The high frequency of positive smears in this study is consistent with previous studies in Central India [30]. Likewise the sensitivity and specificity estimates are consistent with previous studies [10]. Further, when the results of this study was compared with WHO, FIND and CDC product testing of RDTs, where the evaluation was performed against a standardized panel of cultured P. falciparum and frozen blood samples (200 2000 parasites/ l of blood) by experienced technicians in a research laboratory and not in field as done in the present study, the results of the WHO, FIND and CDC are comparable to this study with regard to diagnostic performance of five RDTs evaluated in both the studies. Additionally, first field based evaluation of commercially available RDTs regarding their temperature stability was also carried out in the present study. Among the 7 bivalent RDTs, most RDTs for P. falciparum showed a very high sensitivity over period at 35 and 45 C upto day 90 in terms of percentage of successful tests. While for P. vivax, the FIRST RESPONSE maintained good sensitivity over period at 35, 45 C upto day 90 and showed a decline in sensitivity on day 100. Parascreen was also good upto day 90 at 35 C only and showed a sharp decline in sensitivity on day 100. These results are consistent with those of WHO, FIND and CDC product testing of RDTs."
2593776,2.0,"Heat stability is vital to maintaining sensitivity of the test in the field [22], [30]. HRP-2 is a very stable antigen [31], while pLDH may degrade during long storage [11], [22]. Wide variations in stability between various RDTs were recorded in this study. The lacks of quality control of RDTs present a risk to patients through incorrect diagnosis and inappropriate anti-malaria treatment [32]. As a result for procurement, it is essential that careful consideration be given to stability results to ensure that RDTs works under extreme temperature. All the RDTs in this evaluation were packaged in individual envelopes that contain a desiccant. This allows the health worker to open the envelope of a test at the time of use in field limiting exposure to high humidity [12] as the field trial for stability testing was carried out during peak rainy season. The stability testing results presented here provide assessment of both, stability of the RDT and also the quality of its packaging [12]. However, there are some potential limitations in generalizing our results to predict the success of implementing RDTs at high temperature. Though temperature was held constant in this evaluation, humidity was not maintained. Temperature and humidity in field fluctuate with time of day and season and 100 days storage may not accurately predict long term stability under field conditions. Loss of parasite detections over this period indicates that chances of decline in sensitivity cannot be overruled [12]. It is worthwhile to mention here that field trial was carried out during July March thus all 3 seasons i.e. monsoon, autumn and summer were covered. An additional limitation of this study was that highly trained individual performed all the testing in this evaluation. In field settings malaria RDTs will often be used by health workers with limited training and supervision. Temperature upto 45 C is likely in uncontrolled storage in tropical countries and temperature may further increase during transport [33]. The overall agreement and Kappa values between pairs of observers were excellent for both positive and negative results. Likewise, for heat stability testing for P. falciparum generally agreement and Kappa values were good for all RDTs however, for P. vivax the Kappa was not good for most RDTs on day 90 and 100. Thus the use of RDTs widely in the programme will require considerable regulation and quality control [34]. Further research is required to determine why some RDTs examined were more susceptible to heat stress in order to improve their temperature stability."
2593776,3.0,"In view of this, while malaria RDTs can be applied in a number of settings, the greatest potential for impact on public health is in extension of access to accurate, parasite based diagnosis of malaria to regions and populations where good quality microscopy based testing is impractical to maintain, making possible the implementation of recent WHO/NVBDCP recommendation on parasite based diagnosis prior to anti-malarial therapy [25], [35]. However, if the RDTs are to replace microscopy in field for malaria diagnosis, they must be able to work with high level of reliability at high ambient temperature. Diagnostic testing by microscopy or RDT to a level of 200 parasites/ l will reliably detect nearly all clinically relevant infections in malaria endemic areas [10], [36]. However, as some countries move towards elimination, population immunity will be decreased and it will increasingly important to use diagnostic tests that detects low parasite densities [17], [37], [38]. Therefore, the ability to detect low parasite density infections reliably therefore, remains important as malaria elimination initiative is increasing in several countries."
2593776,4.0,"Nevertheless, the challenges associated with establishing the routine use of RDTs in rural remote setting where microscopy could not be established should be tackled carefully as the distribution of RDTs and antimalarials must occur hand in hand to ensure effective case management of febrile disease. These results also suggest that the quality of RDTs should be regulated and monitored more closely. We also noticed a wide range in pricing for RDTs ranging from 0.70$ to 2$. However, high price of RDTs is not an assurance of good performance. Therefore, continued search and eventually introducing other alternative and highly sensitive low cost malaria diagnostic methods should also be explored which are capable of detecting low parasitaemia at high ambient temperature."
362249,1.0,"With improvements in malaria control and relatively higher costs of antimalarial treatment, there is increased opportunity for cost savings through the introduction of rapid diagnostic tests in facilities and/or communities where microscopic confirmation of malaria diagnosis is not reliably available [28]. Amexo and others suggest that it is unethical to continue with high levels of malaria misdiagnosis in light of the introduction of expensive antimalarials and availability of cost-effective methods of diagnosing malaria (such as RDTs) [7]. This argument in favour of limiting antimalarial use to confirmed cases is strengthened by considering the enormous effect of drug pressure on antimalarial resistance and the potential for adverse reactions. This study shows that the introduction of RDTs is likely to be cost saving when a relatively more expensive ACT is used for treatment, provided no more than 52% of those patients clinically diagnosed to have malaria are found to be parasitaemic. This result holds even when relatively less expensive ACTs (e.g. AS+SP or the current preferential price to WHO for AL) are used, but the higher the price of the ACT, the greater the cost savings from introducing definitive diagnosis and the higher the cut-off point at which RDTs become cost-saving. Many countries in Africa have well below 60% of clinically diagnosed malaria patients being confirmed on RDT or microscopy. A study in Uganda found that only 57% of those clinically diagnosed as having malaria were actually parasitaemic [29]. From several studies undertaken separately in 15 countries, Amexo and others (2004) report that on average there is a 61% overestimation of malaria cases when clinical diagnosis is used [7]."
362249,2.0,"As malaria control improves as a consequence of widespread use of insecticide treated bednets [30], indoor residual spraying [23] and or ACT use [16 18, 31], the proportion of clinical malaria (fever) cases that would be definitively diagnosed as malaria will decrease. Consequently the introduction of definitive diagnosis with RDTs at the same time as the introduction of ACT would become increasingly cost saving, because there would be fewer suspected cases to test and even fewer cases to treat. Findings from our study show that the introduction of definitive diagnosis (using RDTs) is cost-saving even when we only consider a narrow perspective of costs related to malaria treatment (i.e. costs of RDTs and antimalarials). While the use of RDTs in all suspected cases has been shown to be cost-saving in some instances, our findings also show that targeting RDTs at the group older than six years and treating all children less than six years on the basis of clinical diagnosis is even more cost-saving. In semi-immune populations, young children carry the largest malaria disease burden and many healthcare providers would find it harder to deny antimalarials to those who test negative in this age-group."
362249,3.0,"Findings from the analyses show that results are sensitive to the effects of changes in age distribution of the symptomatic population, and price of ACTs and price of RDTs. Increasing proportions of older patients result in increased cost savings with RDT introduction (due to lower costs of antimalarials for children, and fixed cost of RDTs). As the prices of antimalarial treatment increase, RDT implementation becomes increasingly cost saving. However, when less expensive ACTs are introduced, such as for the current WHO preferential price of $1.44 per adult treatment, the RDT price to the healthcare provider should be $0.65 or lower for RDTs to be clearly cost saving in populations with between 30% and 52% of clinically diagnosed malaria cases being confirmed malaria cases."
362249,4.0,"The analysis of the potential benefits of introducing RDTs in combination with ACTs is more complex if the majority of malaria treatment is self-administered or sought in the private and informal sectors. Furthermore, for treatment of fever cases testing negative for malaria, some healthcare providers may still use antimalarials and there may be an increased risk of irrational use of other drugs, particularly antibiotics. This necessitates integrated training and supervision of health workers in rational drug use for the treatment of non-malaria fever cases and enhanced drug utilisation monitoring, particularly at the time of RDT introduction. Village health volunteers in Laos were found to only require minimal training (one hour) to sustain reliable use of RDTs over a 10 month period [32]. In contrast, a study conducted in Zambia, where specific training and supervision of healthcare providers was not provided, significant underutilization of RDTs or the inappropriate prescription of antimalarials when patients test negative have been documented [5]. However, a study in Tanzania concluded that use of rapid diagnostic tests, with a single training emphasising that negative malaria tests should lead to alternative diagnoses being considered, did not lead to lead to any reduction in over treatment for malaria [6]. Although the additional benefits of avoiding the use of ACTs to patients in whom the malaria diagnosis is negative could not be included in this study, these are likely to be substantial. Assuming education interventions succeed in ensuring that test results are accepted by healthcare providers, patients and caregivers, excluding malaria would facilitate earlier diagnosis and treatment of the actual cause of the disease for those who are malaria test negative, and would minimise the treatment seeking costs related with repeat visits and the productivity losses associated with prolonged illness. It has been argued that the effects of malaria misdiagnosis fall most heavily on the poor and vulnerable who are least able to withstand prolonged ill-health and the associated missed opportunities for earning an income [9]. In addition, the use of definitive diagnosis would provide more reliable data on malaria cases, hence allowing for accurate forecasting of required antimalarials for planning and budgeting and better monitoring of the effectiveness of malaria control interventions. Also, drug pressure, and consequently, the rate of spread of antimalarial resistance could be decreased."
362249,5.0,"It is also important to note that clinical diagnosis may fail to pick up some patients who actually have malaria. Luxemburger and others found that none of the malaria symptoms alone or in combination proved to be a reliable predictor of malaria, and at best clinical diagnosis would result in prescription of antimalarials in 29% of the non-malaria febrile illnesses and 49% of the true malaria cases (suggesting that 51% of the infections would go untreated initially) [7]. It is important to note that RDTs are unlikely to be used if malaria is not clinically suspected and so will not address the problem of false negatives arising from clinical diagnosis. Furthermore, the extent to which RDTs are cost-effective depends on their accuracy in diagnosing malaria. Studies have found varying levels of sensitivity and specificity of the different RDT products on the market. Swarthout and others (2007) recently reported 52% specificity of Paracheck-Pf in Democratic Republic of Congo and noted that as many as 92% of children were still false positive at day 28 following treatment with artemether-lumefantrine [34]. Similar results are reported by Kleinschmidt and others (2007) for Equatorial Guinea [35]."
361337,1.0,"The study findings suggest that stakeholders have a positive attitude towards the anticipated malaria vaccine and that their acceptance of the vaccine remains high despite the fact that it would be used parallel with other existing intervention strategies. Interestingly, the acceptance level also remains significant though the malaria vaccine is less likely to provide full protection. This outcome could be a reflection of how malaria is seriously perceived in the communities being studied. Furthermore, they may be willing to accept the new malaria interventions as long as they will (to some extent) contribute to the reduction of malaria, especially among children. Similarly, a study in Kenya also found that participants understood that malaria is a serious problem that no single tool can be used to combat it, which influences their acceptance of malaria vaccine [14]. Acceptance of malaria vaccine was also observed in studies conducted in Ghana [13] where the views of various professions and communities also reflected a positive opinion towards the introduction of malaria vaccine as a preventive tool. The study finding that stakeholders would still maintain the acceptance of malaria vaccine in the context of existing malaria intervention strategies is in line with the overall idea of introducing the vaccine which is not meant to replace the existing malaria interventions but rather to compliment it [11]."
361337,2.0,"In this study, social cultural aspects emerged as factors associated with the acceptance of malaria vaccine. These factors include religion (Christians were more willing than Muslims to accept the vaccine), religion (Ndali tribe was less willing to accept the vaccine than the other tribes), and civil servants were more willing to accept the vaccine than the farmers. This finding corroborates with evidence from other countries in Africa [24] and elsewhere [25] where religion and ethnicity were found to influence health care utilization. Specific evidence also indicates that religion and ethnicity are associated with vaccine awareness and acceptance i.e measles vaccine and Human Papillomavirus (HPV) [25, 26]. The differences in vaccine acceptance based on religion, ethnicity and occupation as observed in this study could also reflect that people s values, preferences and expectations would sometimes constrain their acceptance of a particular health care programme. These could originate from the culture in which the social interaction is taking place, which in turn govern their decisions about how they should pursue a recommended health intervention [27]. Although other studies have found that the quality of care i.e. congestion, delays, and the perceived attitudes of the health care providers [14], access to services, reliability of services fear of side effects, and parental beliefs and conflicting priorities [28] constraints immunization services, this study shows that in addition to individual and health system factors, the social cultural aspects may play a significant role in influencing the differential acceptance of vaccination programmes. This is central in this paper, and it lends support to the views of other researchers that people may not automatically use a health intervention once introduced [14], and in the context of a vaccine, if the known barriers are not addressed may lead to under-utilization of immunization coverage [16, 29]."
361337,3.0,"Currently there is a strong recognition globally that health is socially determined and that social-structural aspects are responsible for health inequity. As found in this study, religion and ethnicity may play a significant role towards inequity in immunization coverage. Health inequity is known to be a set back to the wider health development, and this could be addressed by examining the wider social and structural aspects that increase vulnerability to diseases [30 33]. Evidence in Nigeria indicates that the community tailored interventions have proven to be effective in increasing the utilization of polio vaccination [34]. As such, the public health communication strategy that seek to promote the available immunization services as well as the anticipated malaria vaccine could be made effective if tailored within the broader social aspirations and cultural differences existing in the locally contextualized environment."
361337,4.0,"This study also found that the community and other professionals have multiple expectations and questions that relate to the anticipated malaria vaccine. It is important that the Tanzanian Immunization Department, malaria vaccine initiative, and other malaria stakeholders clarifies the questions and expectations prior to or parallel with the introduction of the malaria vaccine and provide the correct knowledge about the added value of malaria vaccine in lay man s language to avoid any misconceptions about the anticipated malaria vaccine. The voices of communities and that of the health care professionals are important and should be considered for better informed decisions, policy recommendation, planning and designing of a communication strategy. Failure to account for community s prior information that could enlighten policy makers on what is needed to be considered before the implementation of the intervention was found as one of the factors that could delay the public acceptance of the proposed intervention [14]."
360871,1.0,"The wording for recommendations was finalized by MPAC during their closed session following the two-and-a-half days of open sessions; conclusions have been included in the summaries of the meeting sessions above, and links to the full set of meeting documents have been provided as references. Position statements and policy recommendations made by the MPAC are approved by the WHO Director-General, and will be formally issued and disseminated to WHO Member States by WHO-GMP or if more appropriate, the WHO Regional Offices. Conclusions and recommendations from MPAC meetings are published in the Malaria Journal as part of this series."
360871,2.0,"MPAC provided suggestions for the agenda for its next meeting to the WHO-GMP Secretariat. Feedback will also be given to and received from the global malaria community at the RBM Board meeting in May 2013, and through the publication of and correspondence regarding this article. Ongoing engagement with and attendance by interested stakeholders at MPAC meetings continues to be encouraged. In addition to open registration for MPAC meetings, which will continue (via the WHO-GMP website starting in July) and attendance by four standing observers (RBM, the Global Fund, UNICEF, Office of the UN Special Envoy for malaria), the active participation of seven rotating NMCP representatives, and all six WHO Regional Malaria Advisors, was strongly welcomed."
995302,1.0,"For PDPs to achieve their goals, partnership is essential. PDP access staff can initiate these partnerships by bringing together different parties, such as scientists, manufacturers, regulators, and implementers. This combination of perspectives from the scientific, commercial and public health worlds can support more informed decision making [34]. In order to reach multiple countries, the involvement of WHO headquarters, country and, in particular, regional offices has been and will remain critical, particularly for diseases such as HIV/AIDS, TB and malaria that have a significant number of WHO staff. Regional WHO offices can track the progress of multiple countries as they move through the multi-part decision process. Table 4 describes possible partners, including WHO, and some of their advantages and disadvantages. Manufacturers have traditionally supported some aspects of country decision-making and all the aspects of product launch. However, many originator companies may have limited experience of introduction into low and middle income country markets (some Indian and Chinese generics may be more established in these markets). Companies may also be concerned that they could be perceived to be self-serving if supporting decision-making around the introduction of a new product directly in countries. Thus, the initial information sharing and country decision making step will generally require the involvement of other actors, including PDPs. An interesting example of division of labor comes from Uganda, where PATH, a not-for-profit that has worked extensively as a PDP, is supporting a demonstration project for human papilloma virus (HPV) vaccines (E. Mugisha, pers. comm.). Before any activities started, PATH signed a Memorandum of Understanding (MOU) with the Government of Uganda (GoU) to specify who would do what. The GoU committed to provide health services delivery infrastructure, human resources in the districts, and EPI staff for delivery of the vaccine. The two PATH technical staff members, located in the WHO Uganda office, provided technical and logistical support. PATH also provided transport allowances (but no per diems) to health workers in the field and funded local university researchers to conduct the formative research and operations research. WHO and UNICEF participated in a technical advisory committee set up by the Ministry of Health (MoH) to oversee the demonstration project, and also helped with monitoring of vaccination. The relevant pharmaceutical company (GlaxoSmithKline Biologicals (GSK)) donated and shipped vaccines to Uganda, but had no other role in the project. UNFPA and other stakeholders provided input on reproductive health issues, and NGOs (e.g., CARE and Save the Children) helped with mobilization in the districts. Thus, PATH served as the glue across the various organizations in support of the MoH."
995302,2.0,"Despite these challenges, successful PDP support of country decision making is possible. The case studies below illustrate that PDPs have taken many distinct approaches to facilitating country decision making. These are examples of what has been done, rather than normative descriptions of what would be ideal. They were selected to represent a range of modalities (e.g., vaccine, drug, and insecticide) and disease areas, and the kinds of activities conducted at different stages of the decision process (presented below in roughly chronological order). Different PDPs have undertaken a range of activities, such as those shown in Tables 1 and 2, with this selection depending on needs identified by each PDP, typically in consultation with partners like WHO and countries. Not surprisingly, the more extensive experience generally lies with the PDPs who have approved products. Engagement prior to product availability For DNDi, country engagement begins with the identification of needs by and with endemic country stakeholders. Certain key research organizations in endemic countries contributed to DNDi's founding, are represented on its Board of Directors, and greatly inform the definition of needs and of the related Target Product Profiles (TPPs). These organizations also facilitate clinical and intervention trials and, ultimately, national decisions on adoption. Three additional PDPs, whose activities are outlined below, do not yet have products approved by regulatory agencies but nonetheless conduct activities related to country decision making. For malaria vaccines, MVI initiated a 3 year process with countries, WHO and, in the later stages, the Roll Back Malaria Partnership (RBM) to develop decision-making frameworks, initially for 9 individual countries and then for the African region [33]. The final framework builds upon existing WHO guidelines [22], and lays out what data are needed from different sources (global vs national), in different thematic areas (disease burden, other malaria interventions, impact, financial, efficacy, safety, programmatic, sociocultural), and at different times (pre-licensure, licensure, and post-licensure); it also notes whether each is essential or desirable. It provides a similar framework for policy processes. The framework process has and will structure dialogues around malaria vaccines with countries around technical issues. In some countries, it has led to the formation of ongoing structures that have begun to collect data to inform an eventual decision (e.g., technical working groups - MVI now sponsors three of these). In contrast to MVI, the Global Alliance for TB Drug Development (TB Alliance) is entering an area that has existing products. There was, however, relatively little analysis of the market, of decision making, or of how new products would be considered. TB Alliance therefore focused on conducting sequential stakeholder studies in the following areas: the size and structure of the existing TB drug market; what local stakeholders want from a new TB regimen; how the experience with past TB regimen changes can inform future approaches; and what producers and products are dominant in the private sector. These studies helped to identify issues and categories of data relevant for future country decision making [12, 31], initiate engagement with local stakeholders, provide opportunities for the promotion of regimen change issues in international fora, and frame conversations with local stakeholders during TB Alliance participation in WHO review missions. Finally, the findings of each study influenced the design and content of the next, and provided essential feedback for the research and development team [2]. TB Alliance selected this approach due to the opportunities and challenges presented by the availability of existing TB treatment regimens and partnerships. The long-term aim of IVCC is to facilitate the development and introduction of new insecticides. Already, however, IVCC is engaging country decision makers to address an identified gap in field implementation - the monitoring and evaluation of vector control programs. This gap is addressed via IVCC's Malaria Decision Support System (MDSS), which is used to track clinical and survey data and insecticide resistance (T. McLean, pers. comm.). This tool is applicable to a wide range of diseases and, in addition to monitoring and evaluation, it supports the management of advanced insecticides, and decision making on adopting new vector control products. Based on existing partnerships with the ministries of health, IVCC has validated the methodology in 3 countries - Mozambique, Malawi, and Zambia - with varied infrastructure and ecological environments; it is now planning wide-ranging implementation. As a central objective, the MDSS should be adopted and owned by the national malaria control program and serve their information needs. Country consultations and regional meetings Early and frequent consultations with countries are essential for the development of products that are suited to end users [2]. For DNDi, input is channeled via disease platforms, which were formed to assist and strengthen clinical research around specific diseases in a geographic area, e.g., Visceral Leishmaniasis (VL) in East Africa, sleeping sickness in West Africa, and Chagas Disease in Latin America. These platforms include country program staff, researchers, regulatory officials, NGOs and WHO and meet twice a year. Platform members became natural partners for country decision making as they gather relevant information on in-country issues, programs, and processes and convey key information to in-country decision makers. The Pneumo-ADIP (now part of the International Vaccine Access Center (IVAC)) built on experience with Hib and Hepatitis B vaccine introductions to support pneumococcal vaccine introduction [9]. Under the Pneumo-ADIP, the establishment of surveillance networks had two positive outcomes: it provided the requested data on projected coverage and impact and, via annual investigators' meetings, led to the identification of local advocates. In addition, regional meetings organized in collaboration with WHO provided an opportunity to check back in and to move countries to put their decisions and proposed actions on paper by presenting their conclusions in front of others (L. Privor-Dumm, pers. comm.). These meetings included EPI managers, directors of health services, researchers, pediatricians, economists, and sometimes donors and financing people from MoH or other ministries, and were a particularly useful mechanism to support decision-making in countries not directly targeted through other interactions. MMV and its drug development partners have also made extensive use of country-level dialogues such as subregional meetings (of WHO AFRO and Roll Back Malaria) and, in select cases, day-long workshops (G. Jagoe, pers. comm.). These provide opportunities to give product-specific briefings and to reinforce recommendations of normative entities (primarily WHO) in terms of best practice for the development and revision of treatment guidelines and for the correct use of new, quality medications in combination with proper diagnosis (case management). Longer-term programmatic collaborations in specific-countries are very limited; the focus is on any initiative (e.g., piloting of an affordable medicines private sector subsidy in Uganda) that address specific access challenges and could serve as guiding lights for policy makers and funders across the larger stage of all malaria endemic countries. In terms of impact, as of September 2010, almost 42 million treatments of Coartem  Dispersible (co-developed with MMV) had been delivered to 32 countries [37]. Regional meetings have been convened by the TB Alliance and partners to gain consensus around regulatory issues [38]. Existing TB drugs were developed more than 40 years ago, in a very different regulatory environment. Agreement was needed on the regulatory approach to development of not just individual new drugs, but new regimens. With the participation of national TB program managers in these meetings, these individuals became part of the conversation about what types of evidence would be available for decision making. Implementation studies as a bridge to adoption When existing evidence is insufficient, implementation studies may be necessary. In India, the Institute for One World Health (iOWH) has supported studies to generate data for advocacy and decision making on VL treatment and elimination. In collaboration with a research institute of the Government of India, they documented the incidence of VL, the financial burden of disease, households' willingness and ability to pay, and treatment-seeking behaviors in both public and private sectors (R. Sarnoff, pers. comm.). In addition, building on the necessary Phase 3 study, iOWH sponsored a Phase 4 study with an effectiveness module that provided training, clinical support, and guidance on pharmacovigilance reporting, and demonstrated effective delivery in public and private facilities. The clinical trial investigators formed a core constituency for local advocacy for improved products. At the national level, iOWH leadership engaged with key stakeholders in the Indian government, World Bank and WHO to inform them of the progress of the studies, identify their key questions and concerns, and address future funding issues. Training modules and community communication models were developed for smooth transfer to the national authorities. DNDi has also used intervention or field trials as an essential step to demonstrate feasibility and generate necessary data for adoption into national programs (F. Camus-Bablon, pers. comm.). For example, Brazil conducted a 25,000-subject malaria intervention trial prior to adopting artesunate and mefloquine (ASMQ) for treatment of falciparum malaria in the Amazon basin. The trial monitored the effects of ASMQ introduction; during the study, a significant impact on malaria cases and related hospitalization also resulted from a more rational use of complementary resources such as insecticides, a detection and reporting system, and the training of local human resources. In this study, one year after the introduction of the ASMQ fixed-dose combination (FDC) and the treatment of 17,000 patients, P. falciparum malaria cases were reduced by nearly 70% and malaria-related hospitalizations dropped by over 60%. Following the study, the Brazilian National Program updated the national malaria treatment guidelines and introduced ASMQ FDC as the first line treatment in the region. Advocacy is also a key component of DNDi implementation work, to inform both international audiences and endemic countries. In some areas, DNDi relies on pharmaceutical and other international partners. For example, WHO Neglected Tropical Diseases (NTD) department and M decins Sans Fronti res (MSF) are key drivers for the adoption of nifurtimox-eflornithine combination therapy (NECT) for the treatment of sleeping sickness, which, within a year, has been adopted in the national treatment policy of nine endemic countries and ordered by six. Sanofi-Aventis (SA), within two years of WHO pre-qualification, was planning to distribute 50 million artesunate-amodiaquine (ASAQ) treatments in 2010. Today, ASAQ is registered in 27 African countries and in India. SA is conducting a 15,000 patient pharmacovigilance program in partnership with DNDi and MMV in Ivory Coast, and developed a specific package for social interventions and home based management programs. Evaluation of impact There are significant challenges in estimating the impact of PDPs on country decision making. First, most PDPs are relatively young, being established in the last 10 years. Given the time required to develop a product, many have not yet had products launched, and the product launches that have occurred are recent. Second, there is not yet agreement on how to measure impact - in particular, whether to focus on usage (e.g., number of individuals treated; see the data on new product usage noted in this paper) or on process (the number of countries conducting a policy process and reaching the decision that is best for their particular situation; see [39] for an example). Third, if a product is suboptimal - too expensive, insufficiently efficacious, or too difficult to use, for example - it is unlikely to be rescued by PDP support of country decision making, even if those support activities are well executed. In other words, success under the first (usage) definition is only likely when the new product continues to meet identified country needs. This is why access input is essential throughout the product development process [2]. There have, however, been efforts by PDPs to define metrics of success and to evaluate PDP work in support of country decision-making. For impact using the process definition of success, one example is the malaria vaccine decision-making framework [39]. The development of this framework has been independently evaluated [40]. Out of 84 respondents from 10 countries, 90% felt that the framework developed will be extremely or very useful in preparation for a decision (i.e., in deciding what activities to undertake prior to having a licensed product), and 88% indicated the same for taking a decision after a vaccine is licensed. Facilitators were reported to be neutral instead of supporting one product. A 2007 study by the GAVI Alliance tried to quantify the impact of the Pneumo- and Rotavirus ADIPs and Hib Initiative as compared to what may have happened if they had not been in place [41]. The authors found that the Pneumo-ADIP is likely to shave at least five years off the time from development to availability of vaccines in the poorest countries, and that the work of the Rotavirus ADIP may result in the poorest countries accessing vaccines only one year after availability in the developed world, which is years and decades shorter than historically. Based upon this, the authors reported value in terms of lives saved and hospitalizations averted. Given the constraints noted above, the current paper does not aim to provide a rigorous evaluation of PDP impact and strategies, but instead provides a situation analysis, reflecting the range of strategies undertaken by PDPs and detailing the rationale behind their choice. It is clear that having no engagement specifically around new products leads to lengthy delays in availability [1]. At the same time, it is too early to determine the optimal strategies for each situation and type of intervention. However, the current analysis provides an important baseline or reference point for a later impact analysis of PDP work on access."
363993,1.0,"Throughout this period of increased vaccine effort, WHO has had an important role in supporting various aspects of basic research projects through the WHO/UNDP/World Bank Special Programme (WHO/TDR) and has also played a major normative role in providing guidance on many aspects of vaccine development. Landmark meetings were held to highlight the need for improved adjuvants, by bringing together representatives of groups involved in developing and testing new potential products, recognizing that malaria antigens were often involved in the first wave of testing novel agents [7]. WHO worked with funding agencies to convene meetings of scientists, regulators and others to review the state of the art of vaccine development, reviews of key ethical issues [8], and maintains an ongoing record of malaria vaccines under development (known as the Rainbow Table) [3]. Perhaps most importantly, WHO has worked with its advisory committees to gather best evidence then provide guidance for vaccine trial designs that ensure that results generated are relevant for establishing subsequent policy recommendations, to assist national programmes in malaria-endemic countries. WHO s advisory committees are a key mechanism for identification of research priorities for immunization and for development of consensus-based guidance on clinical development and testing of vaccines. The WHO Malaria Vaccine Advisory Committee (MALVAC) succeeded IMMAL and VDR Committees, and provides advice to WHO on strategic priorities, activities and technical issues related to global efforts to develop vaccines against malaria, with emphasis on the public health needs of developing countries. In the five years 2008 2013 MALVAC has convened eight key meetings of experts, together with several working groups, whose detailed assessments have provided valuable consensus views on priorities and best practice for selected vaccine-related research and development strategies. By providing the leading independent global malaria vaccine forum for funding agencies, sponsors and investigators, WHO has increased collaboration between key malaria vaccine R&D stakeholders. WHO has also improved the comparability of key endpoints by convening technical groups to provide consensus based protocols and Standard Operating Procedures, ensuring extensive consultation amongst stakeholder groups. The technical areas for these consultations have included design and conduct of sporozoite challenge trials, Standard Operating Procedures for malaria microscopy in challenge trials, [9] optimization of clinical challenge trials for asexual blood stage vaccines, measures of efficacy of malaria vaccines in phase IIb and phase III trials, workshops on standardization of malaria vaccine immunoassays, evaluation of assays and trial designs to be applied to interventions against malaria transmission, development of whole organism vaccines for malaria endemic countries, and priorities in research and development of vaccines for Plasmodium vivax and their evaluation. Recently, WHO highlighted the need for information-sharing among HIV, TB and malaria vaccine communities and working with NIAID convened a technical forum on heterologous prime-boost immunization across the three diseases [10]. Furthermore the MALVAC committee will have a key role in advising WHO on the updated version of the Malaria Vaccine Technology Roadmap [11]. These informed and impartial opinions and recommendations are summarized here to provide guidance on harmonization of strategies that will help to ensure high standards of practice, and comparability between centres and the outcome of vaccine trials. Reference is made to reports of the meetings (and to selected other relevant publications) that provide the more detailed discussions on which the recommendations are based. The text below summarizes the reports of individual meetings and should not be considered to be the position or policy of the WHO. For outcomes of individual meetings, readers are directed to the individual meeting reports. There is a distinct WHO advisory mechanism which provides advice on vaccine candidates in advanced development and approaching possible availability for use. This is known as the Joint Technical Expert Group (JTEG) on malaria vaccines, reporting jointly to the WHO Strategic Advisory Group of Experts on Immunization, and to WHO s Malaria Policy Advisory Committee [12]. MALVAC will continue to provide advice to WHO on the longer term malaria vaccine R&D considerations, with JTEG available to provide evidence reviews for possible policy recommendations when products become sufficiently advanced."
363993,2.0,"Consensus-based guidance on clinical trial design A vital part of the development of candidate malaria vaccines is the careful planning of all phases of clinical trials, always with a view to enhancing comparability between vaccine clinical trials, sites, and alternative development programmes. A second important objective is generation of data to terminate or advance projects appropriately. Phase I trials are used to determine whether candidate vaccines have the required profile of safety and immunogenicity, with Phase IIa trials in malaria designed to provide actionable information on efficacy, safety and immunogenicity including controlled human malaria infection. They are screening trials intended to select candidate vaccines to take forward into field trials, and to select which vaccine formulations to terminate. These proof of concept studies have usually started with adults in non-endemic areas before moving to the target group of children of an endemic area. Phase IIb and III proof of principle field trials require progressively larger numbers of subjects depending on the primary endpoint to be measured and the controls necessary for comparison. Phase III trials are traditionally designed with the primary objective of providing data suitable for regulatory filing. However a focus in malaria vaccines has been to ensure that data are also suitable for evidence-based public health policy recommendations as far as possible. This can avoid the need for additional Phase III trials.  Challenge trials in malaria-na ve volunteers Sporozoite challenge trials for pre-erythrocytic and blood-stage vaccines Controlled Human Malaria Infections (CHMI) [9, 13] are used to assess candidate vaccine efficacy in malaria-naive individuals. Challenge trials in volunteers are important as they inform future clinical trials   whether or not to proceed; dosing, route, schedule and vaccine presentation. This allows iterative improvement of the vaccine construct and its use. Field trials for pre-erythrocytic vaccines should be dependent on first achieving a pre-determined level of efficacy in challenge trials. Future efficacy field trials of new vaccine candidates or combinations may well have to be non-inferiority trials (if RTS,S, for example, is licensed) in regions of reduced transmission, potentially requiring very large sample sizes, thus making them costly and complex with subjects enrolled in several centres. The need to achieve greater confidence in expectations of efficacy prior to initiating multi-million dollar field studies in large numbers of individuals means that even greater reliance may be placed on carefully standardized CHMI studies. They will also be important for the same reasons in go/no go criteria and prioritization of blood stage vaccines (Figure 1). The significant additional challenges with P. vivax challenge trials are considered later. Blood stage challenge trials As an alternative to sporozoites, parasitized erythrocytes can be used to challenge vaccinated individuals [14]. They may have advantages over sporozoite challenge for testing the efficacy of blood-stage vaccines since they allow use of a standardized and low dose challenge and, consequently, a longer period for assessment of induced blood stage immunity before parasites are detected and treatment is required. In contrast, the large sporozoite challenge required to ensure that every control subject is infected may lead to heavy infection and a large numbers of parasites leaving the liver and, as a consequence, earlier detection of parasites requiring treatment.  Rigorous safety depends on having a well-characterized source of erythrocytes free from adventitious agents that meet stringent blood product safety requirements. This is the area of particular concern that for some investigators may still outweigh the perceived benefits of blood-stage challenge. The antigenicity of infected red blood cells in vaccinated volunteers is also a safety issue to be considered. Quantitative PCR monitoring of infection is valuable as it provides a detection threshold of asexual parasitaemia of approximately 20 parasites/ml, well below that possible by microscopy, and potentially increases the time for observations and assessment of efficacy before treatment is required.  There are, however, some drawbacks to use of blood-stage challenge. The challenge bypasses skin and liver stages of infection and removes the possibility of detecting protective cellular immune responses against the late liver stage or antibody against merozoites released from the liver. A low inoculation of parasitized erythrocytes has no counterpart in naturally acquired blood stage infection. Further development of blood stage challenge trials and comparisons with sporozoite challenge are desirable. Development of multiple antigenically distinct parasite strains for evaluation of heterologous protection is also required. Independent evaluation of methods used for modelling parasite growth curves should be made to assist decision-making.  Phase IIb and Phase III malaria vaccine trials Detailed recommendations were made on the implications of different measures of efficacy as they affect vaccine impact, comparability of trials, licensure and wider public health benefits [15]. The conclusions were:  Field evaluation of a vaccine has to be done in the context of other control measures. Assessing efficacy is complicated for malaria where first infection (or vaccination) gives only partial protection against re-infection and the same individual may have multiple episodes of clinical malaria. The primary measure of efficacy is commonly reported as the incidence of first episode of infection or of clinical malaria. This takes no account of subsequent episodes and, from a public health perspective, reduction in the total number of events in some defined time period following vaccination is more relevant than measuring time to first event as the primary efficacy endpoint.  Incident malaria infection is a prerequisite of clinical malaria but, additionally, there can be incident infections without clinical symptoms. If protection against incident malaria infection could be used as a correlate for protection against clinical malaria, this would allow smaller trial sizes and less cost (in malarious areas where many trials are likely to be conducted, incidence of infection is high, with the vast majority of susceptible individuals expected to experience infection in a one to three month period at the peak of local transmission).  A high incidence of malaria and heterogeneity of exposure make estimates of efficacy difficult. In particular, waning of efficacy and heterogeneity in exposure cannot be distinguished by measuring the proportion of individuals remaining disease free at different times after vaccination. Boosting from natural infection should be evaluated. Trial designs in endemic countries will need to be able to detect duration of protection at least up to two years after vaccination, and to rule out deferred or rebound increases in mortality as a consequence of increasing susceptibility following a period of vaccine-induced immunity.  Evaluation of efficacy in the context of existing malaria preventative interventions is a priority. The dosing schedules required for successful vaccination may have implications for feasibility of scale-up. Possible interference with EPI vaccines in infants should be assessed. The possibility of reduced transmission in some field trial sites will need to be addressed in field trial design; larger enrolments may be required, and because age at risk will have extended beyond the ages for EPI, vaccines may also need to be administered to the older children at risk.  Safety, immunogenicity and efficacy should be established prior to a vaccine being given to specific high risk groups (e.g. infants, in pregnancy, and immune-compromised individuals), Monitoring and evaluation plans should include both pharmacovigilance and a sustainable disease burden monitoring system. However these pose major challenges in many malaria-endemic settings.  Impact on malarial transmission (see below) is a key part of assessment of any potential new malaria intervention, although it may not be included in the first Phase III trial. The future combination of single or multi-component candidate malaria vaccines with other malarial interventions should be considered.  As a general rule methods for design and analysis of Phase III trials should be registered in publicly available data bases before results are unblinded. Data sharing will increase understanding of the likely public health impact of a new vaccine in the context of existing control measures.  Post-licensure, data should be acquired on long-term effectiveness in multiple transmission settings and changing control measures and epidemiology, noting the consequences when immune individuals have long periods of reduced exposure, for example whether they become more susceptible to the consequences of later episodes of malaria. The long-term safety must also be evaluated.  Measures of efficacy of interventions against malaria transmission Reduction in transmission remains the fundamental goal of malaria control and measuring changes in transmission allows a better understanding of the interactions between different interventions (vector control, treatment, vaccination) and their combined impact. Some currently available anti-malarial interventions lead to a reduction in transmission even in highly malarious areas, and transmission reduction is the key metric in a malaria elimination strategy.  Better measures of transmission are needed but, as yet, there are no agreed and standardized measures of malaria transmission and these may need to be different in areas of high and low transmission. Measurement of transmission is costly and time-consuming, requiring repeated observations. In some very low transmission areas, incident infection leads to disease and may, therefore, be a surrogate for ongoing transmission. However, in other areas of very low transmission, substantial numbers of asymptomatic infections have been detected with sensitive molecular tools. At low transmission intensity (for example in some of the environments in which transmission blocking vaccines are likely to be tested and introduced) estimates have very large confidence intervals and surrogates are required (see below). Transmission blocking vaccines (TBV) specifically target sexual or sporogonic stage parasites, or mosquito midgut antigens. However, the so-called vaccines that interrupt malaria transmission (VIMT) may have their primary effect on other of the three main life-cycle stages but, additionally, may have a significant indirect effect on transmission.  The key effect required for sexual stage or mosquito antigen targets is substantial reduction in the proportion of infected mosquitoes. By contrast, the key effect required for pre-erythrocytic vaccines when viewed as VIMT, is a major reduction in the proportion of humans carrying sexual stages.  A WHO MALVAC meeting considered measures of efficacy of anti-malarial interventions against malaria transmission [16]. The objectives were to evaluate current methods for measurement of malaria transmission and assessment of assays and clinical trial designs that should be applied to measurement of reduction of transmission (of Plasmodium falciparum). The measures considered necessary included:  (i) Epidemiological (incidence of new infections)  (ii) Standardized assays of transmission from humans to mosquitoes (Figure 2) [16, 17]  (iii) Entomological (estimating new human infections by mosquito measures)  (iv) Surrogate serological and molecular measures, e.g. sero-negativity in young children as transmission declines."
360654,1.0,"Understanding the local context of diseases and the communities  perceptions of the experimental interventions serves as prerequisite for how the intervention will be accepted and utilized when it becomes a policy that has to be implemented as a programme. Several studies have documented the need and how best to educate communities [20 23]. This study s findings, which are similar to studies documented on the need to understand the local context of diseases, have benefited from the experiences of such studies and has identified findings as an important step in ensuring the malaria vaccine will be acceptable to communities where this was tested and serve as the springboard for integrating the vaccine into other communities that did not necessarily participate in the trials. It is significant to understand how community members perceive a malaria vaccine in order to inform the country s malaria control managers and officials responsible for the EPI for planning purposes. Our results are similar to that found in Kenya [20]. Knowledge of childhood vaccinations was widespread among respondents. Local names were provided for them, and this could have some positive effects on childhood immunization programmes. Caregivers will at given times be able state which immunizations their children had received and those that they have not received. However in some parts of Mozambique, caregivers mentioned vaccines that do not currently exist [24], and this could have some negative effect on childhood immunization and overall child care."
360654,2.0,"The role of religious beliefs in shaping perceptions and consequently decisions need not be over emphasized. Religious beliefs have the potential of diluting community perceptions on a given phenomenon. The boycott of the polio vaccination programme in some parts of Nigeria was partly because of the belief that the vaccine could be contaminated with anti-fertility, HIV and cancerous agents. This was mainly a concern held by Muslim clerics in Muslim-dominated states and by the Supreme Council for Sharia in Nigeria (SCSN) [12]. Unlike the Nigerian incident, this study revealed a contrary position; religious beliefs that could serve as a potential barrier to the acceptance of a malaria vaccine were virtually unknown in the study area, not even among religious leaders. Religious leaders could therefore be a conduit for a mass introduction of malaria vaccines. In this study there was a community belief children should not be seen until they are more than a month old. In a recent study in Mozambique it was reported that a common taboo among some communities was that a newborn should have limited exposure to adults with  hot bodies  and  hot blood  [24]. The belief about exposure of newborns was not limited to adults only, as reported in the Mozambique study. This belief however is unlikely to affect the administration and coverage of new vaccines, such as malaria vaccines, since some known childhood immunizations such as OPV are given within the first month of life and their coverages are high. Women who deliver carry their children to immunization points for such immunization schedules."
360654,3.0,"The contribution of the environment is key when chronicling factors that contribute to the breeding of mosquitoes. Recent evidence in Tanzania showed that most vector control measures target life-threatening mosquitoes through the use of chemicals belonging to the pyrethroid class; however resistance of mosquito populations to the pyrethroid class of chemicals is increasing dramatically [23]. Community members recognized the role the environment plays in the breeding of mosquitoes. They recognized challenges to malaria vector control, especially when there is growing resistance to available insecticides by mosquitoes. They will therefore welcome a vaccine as one of the ways of surmounting this problem."
360654,4.0,"The popularity of vaccines against tuberculosis, measles and poliomyelitis compared to hepatitis B and Haemophilus influenza type B vaccines is because the former vaccines have been in existence over several years compared to the latter. Similar experience was found among Hong Kong Chinese adults, where insufficient awareness of hepatitis B infection was observed. The poor knowledge of this infection may have resulted from the poor knowledge of the universal neonatal and adult hepatitis B vaccination [25]. Health managers and professionals should be aware of the results of this study and revamp their effort towards education on the relatively new vaccines."
360628,1.0,"What is the most appropriate regulatory agency, given the overlap between pharmaceuticals (drug), vector control, and indirect impact on malaria? The proposed use of the drug ivermectin to reduce malaria transmission by its mosquito-killing effect implies the mass administration of the drug to humans; hence, regulatory approval should come from the drug section of a human health agency. The FDA Center for Biologics has been conceptually supportive of a transmission-blocking vaccine. Moreover, it has stated that it could rule on products not primarily intended to be marketed in the United States. It would also rule on products that would have a community effect leading to delayed personal benefit, a key obstacle for licensure for transmission-blocking vaccines [10]. There is no available data on the status on these discussions with the EMA, although it does have Article 58, which supports its offering an opinion on a product to be used primarily in endemic areas outside of EMA s primary geographic remit in collaboration with the WHO and relevant non-EU regulatory authorities. Further discussion with both regulatory agencies by a potential sponsor would be required.  Potential regulatory pathway for ivermectin for malaria vector indication If the goal is application for approval for novel use of the licensed product, then regulatory approval for drug repurposing could be sought via the 505(b)(2) pathway [11]. The 505(b)(2) has the advantage of allowing the use of evidence from studies not conducted by the sponsor, alleviating costs and reducing time to approval. An FDA draft guidance specifically for developing treatment and prophylactic products for malaria was drafted in 2007 [12].  In any case, new tools and/or indications need to be proven effective [13] and safe [14]. For an ivermectin-based vector control tool, the best clinical trial design to demonstrate both safety as well as public health impact on malaria transmission is a pivotal cluster randomized trial with sufficient power to assess both key endpoints. It should demonstrate added value on top of standard vector control tools, which should serve as the referent. As the impact and risk/benefit ratio of ivermectin MDA is expected to vary according to the baseline transmission, the selection of the scenario for the first study is key [15]. Note that this design was successfully utilized to definitely demonstrate the impact of other vector control tools, specifically LLINs [16].  For novel applications or new formulations, the FDA expedited approval process could be an option [17]. The FDA Expedited Approval Process aims to  facilitate and expedite development and review of new drugs to address unmet medical need in the treatment of a serious or life threatening condition (using) fast track designation, breakthrough therapy designation, accelerated approval, and priority review designation  [17]. For malaria elimination, both the challenges of residual transmission and insecticide resistance could meet the criterion on unmet medical need, and make ivermectin a good candidate for this approach. Further discussions with regulatory agencies will be needed.  However, the accelerated approval scheme is based on the use of surrogate biological markers of efficacy [18]. In the case of ivermectin, the reduced survival seen in vectors feeding on treated subjects may be an appropriate surrogate marker but is unlikely to lead to regulatory approval, much less policy recommendation and implementation at country level."
1449365,1.0,"The World Health Report has classified interventions with cost-effectiveness ratios of less than the country s per capita GDP as highly cost-effective and those which are 1 3 times the per capita GDP as cost-effective[35]. Most of the interventions we have identified in this study have cost-effectiveness ratios which are well below the Tanzania s estimated GDP per capita of US$ 550[36], hence they can be considered as highly cost-effective. On the other hand, Tanzania has a per capita expenditure on health of about US$ 14 per year[37], which is below the US$ 40 recommended by WHO to finance essential health interventions[38]. This means its ability to implement and scale-up even what can be considered as a highly cost-effective intervention is limited. Our literature review shows that only a few pharmacoeconomic studies have been conducted in Tanzania. Nine out of the twelve studies were on drug therapies and vaccine against infectious diseases which are responsible for more than two-third of the disease burden in sub-Saharan Africa[39]. Nine studies were published within the last ten years, of which six are less than five years old indicating an increasing focus on this research area (Table2). Antimalarial and antiretroviral drugs were the most researched drugs, which mean that to some extent researchers have responded to the importance of the two diseases for the burden of diseases in Tanzania (Table1). Half of the identified studies were on interventions targeting pregnant women and children under the age of five years, reflecting concerns for the high mortality rates for these vulnerable groups in Tanzania."
1449365,2.0,"HIV/AIDS is the number one priority health problem in Tanzania, and affects the most productive age group ranging from 15 59 years, hence impairing the country s economic growth[40]. About 20 per cent of the mortalities for admitted patients above five years of age recorded in Tanzania each year are due to HIV/AIDS and Tuberculosis[41]. Our study found two pharmacoeconomic studies on prevention of mother-to-child transmission (PMTCT) and none on case management of HIV/AIDS. PMTCT programs are in transition in Tanzania, responding to the current recommendations consisting of two prophylactic options provided by the WHO. Option A consists of zidovudine (AZT) which is initiated on week 14 of pregnancy, followed with single dose nevirapine (sd-NVP) plus lamivudine (3TC) at the onset of labour until delivery. AZT and 3TC are then continued for 7 days postpartum. Option B is composed of triple ARV drugs which are also initiated on week 14 of pregnancy until one week after cessation of breastfeeding[42]. The task of choosing which option to implement rests on individual countries and should be based on the feasibility, acceptability, safety and costs[42]. This is a practical example where pharmacoeconomic analysis should be used to guide medicine selection.Tanzania has opted to implement option A[43], however, without being guided by cost-effectiveness comparison evidence for option A and B. An economic evaluation study by Robberstad et al. at Haydom Lutheran Hospital in Northern Tanzania showed that option B was highly cost-effective in the Tanzanian settings with incremental cost-effectiveness ratio of US$ 162 per DALY averted. This regimen was however 40 per cent more expensive than sd-NVP but 5 times more effective[21]. Since option A at the time of the study was not being implemented at the study site, they did not make cost-effectiveness comparisons of option A and B relative to sd-NVP. Drug costs for option B relative to option A which were approximately up to five times in 2009, have been reduced significantly down to two times by the end of 2011[44]. WHO has recently released a new PMTCT update advising countries to adopt the use of option B plus, where a pregnant woman is placed on option B for life regardless of CD4 cell count or clinical staging[45]."
1449365,3.0,"Malaria is second after HIV/AIDS on the disease burden in Tanzania. On average about 46 per cent of all in-patient and out-patient cases registered in the healthcare facilities each year are due to malaria[41]. Malaria is the leading cause of morbidity and mortality among children under the age of five years[40, 41]. Malaria during pregnancy is also associated with low birth weight[46], which is recognized as the single greatest risk factor for neonatal and infant mortalities in sub-Saharan countries[47]. A recent study showed that the burden of malaria among adults has been highly underestimated. According to the findings of this study, malaria is also the major cause of deaths among adult populations[48]. Our review found four pharmacoeconomic studies on malaria, two of them being on malaria case management. Tanzania has changed its national malaria treatment policy twice over the past ten years due to drug resistance to formerly effective antimalarials. These policy changes involved replacement of chloroquine (CQ) with sulphadoxine-pyrimethamine (SP), which was subsequently replaced by artemether-lumefantrine (ALu)[49, 50]. Both SP and ALu were at the time the most cost-effective antimalarials compared to alternatives which were available[25, 26]. Our review of treatment guidelines and other relevant policy documents showed inconsistent use of pharmacoeconomic evaluations during malaria treatment policy change. As a result the decision to change to ALu unlike that of changing to SP has been criticized for largely being based on the efficacy rather than cost-effectiveness comparisons[51]. The other two studies were on presumptive treatment of malaria using SP in infants (SP-IPTi) and Deltaprim (a combination of pyrimethamine and dapsone) plus Iron in infants and pregnant women. Studies from African settings have shown that SP-IPTi could reduce episodes of clinical malaria, anaemia and rates of hospitalization in infants by 30, 21 and 38 per cent respectively[32]. As a result SP-IPTi has been adopted by WHO since 2010 as a new malaria intervention strategy targeting infants residing in areas with moderate to high malaria transmissions, but with low resistance to SP[52]. SP-IPTi was demonstrated to be highly cost-effective in Tanzania with incremental cost-effectiveness ratios of US$ 1.57 (0.8-4.0) and US $ 3.7 (1.6-12.2) per malaria episode and DALY averted, respectively[27]. Even though Global Fund and other donors have made financial resources available to support the implementation of this intervention[53], SP-IPTi has not yet been adopted in Tanzania. Studies from the Northern and Southern areas of the country have reported low protective efficacy results from the use of this intervention[54, 55]."
1449365,4.0,"Diarrhoea is ranked third on the disease burden in Tanzania and is considered the second main cause of deaths among children under the age of five years worldwide after malaria[56]. Oral rehydration salts (ORS) reduce the duration of diarrhoea episode and replaces the lost water and electrolytes hence preventing the occurrence of dehydration. When Zinc is given as an adjunct therapy for 10 14 days, it has been proved to reduce the duration of acute diarrhoea by 25 per cent and treatment failure or death due to persistent diarrhoea by 42 per cent. It also prevents episodes of subsequent infections for up to three months[57, 58]. In 2004, WHO and UNICEF recommended that countries adopt the use of Zinc and low osmolarity oral rehydration salts (lo-ORS) in their revised guidelines for treatment of diarrhoea[59]. Zinc was included in WHO model list of essential medicines in 2005 based on the evidence of cost, efficacy, safety and cost-effectiveness in the management of diarrhoea[60]. We found one pharmacoeconomic study by Robberstad et al. on Zinc as adjunct therapy which reported it to be cost-effective in Tanzania[23]. Tanzania adopted the new diarrhoea treatment guidelines which incorporated the use of Zinc in July 2007[61] followed by its listing in the national essential medicine list the same year[13]. Our review of documents revealed that a task force committee which was composed of representatives from the government, WHO, UNICEF, and non-governmental organization was formed to advocate for adoption of Zinc[61]. However there is no evidence of whether economic evaluation was among the criteria on which the local decision was based apart from the WHO/UNICEF recommendation."
1449365,5.0,"Injuries/trauma and emergencies is ranked fourth on the disease burden in Tanzania[62]. Victims of injuries/trauma often require blood transfusions to replace the massive amount of blood lost. Other recipients of blood transfusion include pregnant women, patients coming from surgery and those with anaemia. Pregnant women in African settings who need blood transfusions during or after delivery often suffer preventable deaths due to shortages of blood supplies[63]. Even though blood transfusion is considered a lifesaving intervention, it also exposes its recipients to blood-borne viral infections such as HIV/AIDS and Hepatitis B. In Tanzania the average HIV/AIDS prevalence among blood donors has been estimated to be 9 per cent[41]. Shortages of blood supply for transfusions and risks of disease transmissions make alternative options not requiring blood transfusions more attractive. We found two pharmacoeconomic studies on Tranexamic acid (TXA)   an antifibrinolytic drug which reduces post-operative blood loss and transfusion requirements to injury victims[64]. TXA can reduce the risks of death due to bleeding by 21 percent if administered within three hours after injury[34]. For elective surgery, TXA reduces the requirement of blood transfusion by one-third and the volume per transfusion by one unit[33]. The incremental cost-effectiveness of administering TXA to bleeding trauma patients in Tanzania was 48 US$ per LY gained[20], while the incremental cost-effectiveness for surgical bleeding was US $ 93 per life saved[29]. Despite being reported to be very cost-effective in Tanzania, TXA injection is not on the national essential medicine list, but has recently been added to the WHO s model list of essential medicines[65]."
1449365,6.0,"TB is ranked sixth on the disease burden in Tanzania in spite of being recognized as having one of the most successful national TB programs in the world, with a treatment success rate of 88 per cent[37]. We found one relatively old economic evaluation study by Murray et al. which compared the cost-effectiveness of short-course versus long-course anti-TB chemotherapies. The study showed that short-course chemotherapy was less costly per death averted and per LY saved when compared to the long, 12-months chemotherapy for both hospital and ambulatory care[31]. The short-course strategy was found to be very cost-effective with incremental cost-effectiveness ratio of 1 4 US$ per life year saved. In areas with an organized healthcare system the short-course regimen increased the cure rate by a quarter when compared to the standard therapy[31]. Short-course chemotherapy was already introduced in Tanzania before the publication of the study conducted by Murray et al. However, our review of documents showed that the decision to adopt the use of short-course chemotherapy was grounded on evidence of better treatment outcomes at less costs shown by the short-course regimen in Tanzania[66]."
359097,1.0,"The successful eradication of smallpox holds many lessons for malaria eradication efforts, despite the considerable differences between the programmes. Smallpox succeeded as a collection of individual country programmes each deriving local solutions to local problems, yet with an important role for WHO and other international entities to facilitate and enable these efforts by ensuring the best possible tools were available, maintaining the disease s profile globally, fundraising, and arm-twisting in reluctant countries to ensure coordinated action. The documented experience of smallpox programmes suggests that such coordinating efforts must be nimble and flexible to stay relevant to rapidly changing country situations, and burdensome bureaucracy must be avoided if international agencies such as WHO are to add value rather than increasing the challenge of disease elimination."
359097,2.0,"Smallpox programme leaders stress the importance of empowering countries to solve problems locally. Where a strategy or tool has been proven to work well, efficient mechanisms for sharing those experiences are essential. Yet each country will need to adapt those effective approaches given their diversity of populations, systems, strengths, and weaknesses. Global leadership for malaria eradication must ensure countries are able to access the most effective tools available and understand the best principles for how to use them, but the smallpox experience suggests there is no script to be followed in elimination, no simple set of check-boxes that if ticked will result in success. Countries did benefit from the provision of international technical advice and logistical support, helping build staff capacity. The particular importance of administrative support to national programmes suggests distinct cadres of staff can add substantial value to malaria elimination programmes: advisors from public health backgrounds can help with technical aspects, but logistical experts are needed to help plan and execute efficient operations. The smallpox experience also emphasizes the critical importance of hiring programme leaders and managers who are enthusiastic about spending time with communities and local programmes, and who are creative thinkers who can derive context-appropriate solutions to the challenging problems that will inevitably arise."
359097,3.0,"Smallpox eradication is reported not to have involved a substantial increase in domestic budgets, but rather was achieved by better managing programmes and streamlining how they spent the available funds. A clear lesson is that data-driven approaches that target resources to the places where they are most needed will be more successful for elimination than mass attempts to achieve universal coverage everywhere; such a shift in mindset proved similarly successful in the eradication of rinderpest, with surveillance-targeted vaccination proving much more impactful than total coverage [96]. Minimizing inefficiencies in malaria programmes to ensure available funds have the greatest possible impact should be a high priority, even while the malaria community continues to advocate for increased funding from donors. In addition, the importance of flexible funding even in small amounts was repeatedly stressed. Setting up a central malaria account that can be rapidly and flexibly used for filling gaps and bypassing bottlenecks could be an important step towards enabling malaria eradication. Means of reducing dependence on donor-funded commodities, such as investment in local manufacturing, may also need to be considered."
359097,4.0,"Building a malaria elimination programme that is visible for fundraising and that has its own discrete, measurable milestones will drive programmes to hold themselves accountable and focus on achieving results rather than just distributing commodities. However, nesting those programmes within basic health services is critical to leverage routine case management and reporting, increasing the sustainability and reach of the programme. While government programmes may direct the fight against malaria, the experience of smallpox eradication also suggests affected communities and the private sector will have critical roles in whether success is achieved."
359097,5.0,"An innate limitation of this review is that it depends upon the published literature, which is constrained by the availability of viewpoints of those who have published [66]. Smallpox was a global undertaking with diverse contributions of healthcare workers at all levels of international and national programmes, yet accounts in the literature are primarily written by director-level staff from the United States and Europe. Accordingly, this review is biased substantially towards the viewpoints of those few individuals who dominate the literature."
362307,1.0,"CEA is a method for evaluating the relative efficiency of alternative interventions and thus can provide important information for assessing the potential implications of the numerous malaria vaccine candidates. This study used stochastic simulations of P. falciparum malaria epidemiology, combined with a case management model, to simulate the cost-effectiveness of potential malaria vaccines under various transmission settings and delivered via different modalities. This is an extension of previous research on pre-erythrocytic vaccines delivered via the EPI[12]."
362307,2.0,"The simulations presented suggest that the cost-effectiveness of candidate malaria vaccines is likely to differ substantially according to the transmission intensity and to the delivery modality adopted. They also suggest that alternative vaccine delivery modalities to the EPI may sometimes, but not always, be more cost-effective than the EPI. In general, at moderate vaccine prices, most vaccines and delivery modalities simulated are likely to present cost-effectiveness ratios, which compare favourably with those of other malaria interventions [41 43], making them potential attractive malaria control strategies, from an economic perspective, in malaria endemic countries."
362307,3.0,"These simulations have various limitations, as described in the companion article on the epidemiological effects[14]. For the economic analysis, one of the most important limitations is related to the relatively simple case management model used to assess the impact of malaria vaccines on the costs to the health system and to patients. As the case management model used is the same for all scenarios simulated, the relative cost-effectiveness of the vaccines modeled, and, therefore, the comparisons, should only be slightly affected by it. However, further research and modeling of health system characteristics in malaria endemic settings is required. Additionally, the vaccine delivery modalities modeled may not be feasible to implement in all settings as the coverage and the effectiveness of malaria vaccines is likely to depend strongly on the characteristics of the health systems where they will be implemented, including any other malaria intervention being delivered. For instance, the simulations assumed an EPI coverage rate of 89%, which is probably higher than found in some malaria-endemic countries. Lower EPI coverage rates could have an impact on the comparisons between different delivery modalities."
362307,4.0,"Other limitations of this study include that the comparisons of malaria vaccines   or of combinations of them- with different characteristics, are based on the same assumed vaccine price. In practice, the price might vary according to the characteristics of the vaccines, in particular for combinations of vaccines. This might be important for the result that MSTBV combinations were more efficient than vaccines without MSTBV, especially when delivered via EPI with mass campaigns.  While modeling the costs of different vaccine delivery modalities, the fact that vaccine delivery costs might vary as a function of coverage (as it is the case for other interventions[44, 45]) was not taken into account. This aspect was not considered due to the lack of solid evidence on vaccine delivery costs by coverage levels, especially for mass campaigns.  Despite these limitations, the simulations presented provide interesting information for vaccine developers on the potentials of different candidate malaria vaccines. Previous simulation of the cost-effectiveness of PEV[12] suggested that at moderate to low vaccine prices, a vaccine providing partial protection, and delivered via the EPI, may be a cost-effective intervention in countries where malaria is endemic. The simulations presented in this article, also show that these types of vaccines are more effective and cost-effective in low transmission settings, and that the additional costs of delivering a PEV under other modalities than the EPI are likely to be higher than the additional health benefits. The only exception is for the scenario of mass vaccination (added to routine EPI) in low transmission and for high vaccine efficacies and high coverage. In contrast to PEV, BSV are predicted to be more effective and cost-effective at higher transmission settings than low transmission."
362307,5.0,"Combinations of BSV and PEV are predicted to be more efficient than PEV, in particular in moderate to high transmission settings, but compared to BSV, combinations are more cost-effective in mostly moderate to low transmission settings. The cost-effectiveness ratios of the other delivery modalities simulated are higher than those for EPI alone in almost all scenarios, with the exception of adding mass campaigns to EPI in the lowest transmission setting. Combinations of MSTBV and PEV or PEV and BSV do not increase the effectiveness or the cost-effectiveness compared to PEV and BSV alone when delivered through the EPI (including with the addition of booster doses). However, when applied with EPI and mass vaccinations, combinations with MSTBV provide substantial incremental health benefits and low incremental costs in all transmission settings. These combination vaccines are therefore predicted to be interesting only for the settings where mass vaccination achieving relatively high coverage rates would be feasible."
362307,6.0,"According to these simulations, adding booster doses to the EPI is unlikely to be a cost-effective alternative to delivering vaccines via the EPI for any vaccine and transmission setting   i.e. the incremental health benefits are rather low despite the additional costs. Mass vaccination improves effectiveness, especially in low transmission settings, and in some scenario the cost-effectiveness ratios compare favourably with those of delivering the vaccine via the EPI only   the incremental costs are lower than the incremental health benefits. However, increasing the coverage of mass vaccination over 50%, often leads to incremental costs that exceed the incremental health benefits. In some scenarios, the lowest cost-effectiveness ratios are reached at intermediate coverage rates of campaigns. This result is particularly relevant as it is due to the indirect effect of the vaccines, and not to the increasing vaccine delivery costs of achieving high coverage rates. In some of the mass vaccination scenarios the simulations predict that local elimination of the parasite would be, in principle, possible. In some of these cases, at moderate vaccine prices, the simulations also predict that the cost-effectiveness ratios of achieving local elimination might be relatively low despite the fact that often the incremental costs of achieving high vaccine coverage are higher than the incremental benefits. However, the cost-effectiveness analyses of these simulations include only part of the economic implications of malaria elimination."
