,ID,TITLE,DOI,WEB,Result_ID_WEB,Discussion_ID_WEB,Result_Content,Discussion_Content
0,639480,"""Effective reinforcement learning following cerebellar damage requires a balance between exploration and motor noise""","""10.1093/brain/awv329""",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4949390,__sec10,__sec15,"Behavioural analyses
Experiment 1
The experiment was designed to compare the learning and retention of visuomotor rotations under error-based feedback and two types of reinforcement feedback. Figure 2 A shows the time series of reach angles for the different groups. All groups could reach within the target zone during the baseline phase. During the perturbation phase all groups learned and on average were within the final target zone centred on 15°. The open- and closed-loop groups were able to match the desired duration time window for most trials and did not have a significantly different proportion of trials outside the range (invalid trials, 5.7% and 3.7%, respectively, P = 0.18). During retention, when both the cursor and outcome feedback were removed, each group initially maintained their learned reach angle, though the error-based group showed some decay over this period.
 
A mixed model ANOVA of reach angle showed no group effect, but a significant effect of experimental phase [ F (4,228) = 79.176, P <  0.001; Greenhouse-Geisser corrected, F (2.267,129.198) = 79.176, P < 
0.001] and a significant group × phase interaction [ F (2,228) = 6.849, P <  0.001]. Thus, the three groups responded differently across phase. We used post hoc comparisons to examine the key features of learning and retention. We found differences at baseline with the open-loop group having a significantly more negative reach angle compared to the error-based group ( P <  0.05). In addition the closed-loop group showed significantly more learning in the early perturbation phase compared to the error-based and open-loop groups (both P <  0.001). All groups showed significant learning from baseline to the late perturbation phase (all P <  0.001) and similar reach angles early in retention, but in late retention the error-based group showed significant decay from late perturbation compared to the other groups ( P <  0.001). This suggests that learning with error feedback yielded reduced retention of the learned movement relative to reinforcement feedback.
 
Figure 2 B shows total learning (difference between Baseline and End Perturbation) and the per cent early and late retention (measured as the ratio of Early and Late Retention angle to End Perturbation). One-way ANOVA on total learning revealed a significant main effect of group [ F (2,57) = 5.446, P <  0.01], driven by a significant difference between the error-based and open-loop group. This effect was preserved in Brown-Forsythe testing of mean differences, which accounts for unequal variances within groups [ F (2,38.439) = 5.446, P <  0.01].
 
One-way ANOVA showed similar early retention (mean of first 40 retention trials) for all groups [F(2,57) = 2.833,P = 0.067] but a significant difference in late retention [mean of last 40 retention trials F (2,57) = 5.056, P <  0.05] and post hoc test showed this was driven by the decay in the error-based group compared to the open-loop group ( P <  0.05). To examine differences in retention between groups when equated for total learning (and therefore discounting potential effects of the number of valid trials), we selected 10 subjects (of 20) from each of the open-loop and closed-loop groups so as to match their total learning mean and variance to the error-based group. To do this we computed the mean and variance of all possible subgroups of 10 subjects in the open-loop and closed-loop groups, respectively. We chose the subgroup that minimized the Jensen-Shannon divergence between that subgroup and the error-based group’s total learning distributions. Figure 2 C shows the time series of reach angles for these matched subgroups and the error-based group. Group means for total learning were not significantly different following matching, [ F (2,37) = 3.215, P = 0.052; Brown-Forsythe, F (2,17.182) = 2.645, P =  0.10; Fig. 2 D]. Analysis of both early and late retention in the matched subgroups revealed a significant main effect of group [early retention, F(2,37) = 6.573, P <  0.01; late retention,F (2,37) = 8.916, P <  0.01;Fig. 2 D]. The main effect for late retention was preserved following Brown-Forsythe tests [ F (2,19.406) = 9.097, P <  0.01] indicating that the result was robust to unequal variance across groups. In both early and late retention, main effects were driven by significantly reduced retention in the error-based group compared with the two reinforcement-learning groups. Thus, learning with reinforcement feedback yielded enhanced retention of the learned reach angle compared with online error feedback.
 
To examine variability in performance we analysed within subject standard deviations in the reach angles of the overall groups over the 40 baseline trials. One-way ANOVA revealed a significant main effect of group [ F (2,57) = 7.251, P <  0.01], which was repeated following Brown-Forsythe testing [ F (2,42.044) = 7.251, P <  0.05]. Post hoc comparisons revealed that the effect resulted from significantly greater within subject reach angle variability in the two reinforcement groups compared to the error-based group (both P <  0.01). The main effect of group was replicated when within subject reach angle standard deviation was analysed in the resampled reinforcement groups matched to the performance of the error-based group [ F (2,37) = 7.496, P < 0.01; Brown-Forsythe, F (2,20.587) = 5.695, P <  0.05]. Overall, these findings suggest that online error feedback reduces behavioural variability compared with binary reinforcement feedback.

 Experiment 2
This experiment was designed to compare reinforcement learning and error-based learning in individuals with cerebellar damage and age-matched, healthy controls (older controls). Both groups could, on average, reach within the target zone during the baseline phase of both tasks ( Fig. 3 A and B). By the late perturbation phase, control subjects were on average within the final target zone (centred on 15°) in both tasks. However, the cerebellar group on average only reached the final target in the error-based task. Both groups maintained their learned reach angle throughout retention in the closed-loop task. In the error-based learning task, older control subjects initially maintained their learned reach angle, but their performance decayed over the retention period. In contrast, the cerebellar subjects showed no retention as their performance immediately returned to close to baseline.

We used a mixed-model ANOVA to examine differences in reach angle between the groups, tasks and phases. We found significant main effects of group [ F (1,21) = 8.6, P <  0.01], task [ F (1,21) = 6.3,P <  .05] and phase [ F (4,84) = 50.1,P < 0.001; Greenhouse-Geisser corrected,F(2.4,49.8) = 50.1,P < 0.001]. The interaction among these factors was also significant [F(4,84) = 4.73,P < 0.01].Post hocanalysis showed that both groups learned from baseline to the late perturbation phase in both tasks (for the cerebellar group in the closed-loop task,P < 0.05, all others, P < 0.001). Total learning was greater for the error-based task compared to the reinforcement task [F(1,21) = 24.7,P < 0.001,Fig. 3C]. Although there were differences in the mean learned reach angle between older control group and cerebellar group in the closed-loop task (P < 0.01), total learning was not significantly different across the groups in either task (P = 0.235).
 
In the closed-loop condition, the cerebellar and older control group did not have a significantly different proportion of invalid trials (10.9 and 11.5%, respectively, P = 0.90) whereas the young controls had only 1.7% invalid trials. We used a bootstrap analysis to compare reach angles at the end of the perturbation phase as well total learning, both adjusted for the number of valid trials (see ‘Materials and methods’ section). This showed that the cerebellar group had a significantly smaller reach angle at the end of the perturbation phase compared to the older and younger groups (P < 0.001), but the older group was not significantly different from the younger group (P = 0.24). However, for total learning all groups showed significantly different amounts of learning with the older learning more than the cerebellar group (P = 0.026) and the younger learning more than the older group (P < 0.001).

 
In the error-based task, both groups showed a significant decay from late perturbation to early and late retention (older control:  P <  0.001, P < 0.05, respectively; cerebellar: bothP < 0.001). However, the decay in early retention was much greater for the cerebellar group as they returned to near baseline performance ( P < 0.001).
 
To take into account the final learning level, we examined the per cent early and late retention ( Fig. 3 D and E). Mixed model ANOVA of per cent early retention revealed a significant main effect of task [
F (1,21) = 54.8, P <  0.001] and a significant interaction among factors group and task [ F (1,21) = 15.2,P < 0.01, Fig. 3 D]. Post hoc analysis showed that the cerebellar group had reduced retention in the error-based task relative to the closed-loop task ( P <  0.001) and reduced retention in the error-based task compared to controls ( P < 0.01). ANOVA of per cent late retention revealed a significant main effect of task [ F (1,21) = 11.3, P < 0.01, Fig. 3 E], which was driven by reduced retention in both groups the error-based task compared to the closed-loop task. Together these results suggest that cerebellar patients retained their learning in the closed-loop task similarly to age-matched controls. Conversely, while control participants showed some early retention of the adaptation in the error-based task, the cerebellar group showed no such retention.
 
We were surprised that the cerebellar group showed almost complete adaptation in the error-based task, despite no retention. This discrepancy could arise if the cerebellar group relied on online visual feedback to control their reaches. In support of this, examination of the initial portion of the trajectory in the error-based group appears more directed toward the visual location of the target late in learning ( Fig. 4 , middle) and only curves towards the target zone as the movement progresses. In contrast the initial trajectory in the closed-loop task (in which visual feedback cannot be used) is aimed towards the target zone. In early retention ( Fig. 4 , right) the error-based group make movements almost straight to the target suggesting that the curvature seen in the later perturbation movements are driven by visual feedback. In the closed-loop task, however, the cerebellar group maintains some of their initial trajectory deviation in the early retention period. This suggests that the cerebellar group used online feedback to correct their movements in the error-based task resulting in the appearance of complete adaptation, but without any true updating of feedforward models of the correct movement.
 
To examine movement variability we analysed baseline within subject reach angle standard deviation for each subject in the two tasks. Mixed model ANOVA revealed a significant main effect of group that was the result of greater reach angle variability in the cerebellar group compared to controls [ F (1,21) = 14.0, P <  0.01]. The group × task interaction was also significant [ F (1,21) = 8.3, P < 0.01]. Within the older control group, reach angle variability was greater in the closed-loop task compared to the error-based task ( P <  0.01). Baseline reach angle standard deviation in the cerebellar group was related to their clinical impairment. The kinetic function sub-score of the International Cooperative Ataxia Rating Scale (ICARS), which is the component of the scale directly related to arm ataxia, was correlated with baseline variability in both the error-based ( r = 0.8, P <  0.01, Fig. 5 A) and closed-loop tasks ( r = 0.6, P <  0.05, Fig. 5 B). No other significant correlations were found between dependent variables and ataxia rating scores.
Modelling
 Many of the cerebellar subjects showed significant learning and retention in the reinforcement condition. Further, we saw individual differences in reinforcement learning within the cerebellar and control groups with some subjects showing substantial learning and others showing little learning. In an attempt to understand these differences, we developed a simple mechanistic model of the reinforcement learning task and fit each subject’s closed-loop data. The model considered the reach angle executed on a given trial to be the result of an internal estimate of the ideal reach angle (i.e. to counter the current rotation applied) with the addition of two sources of variability: motor noise and exploration variability. The important difference between these two sources of variability is that we assume participants are unaware of their motor noise, but have full awareness of their exploration variability. When an action is rewarded, the subject updates their internal estimate of reach angle based solely on the contribution of exploration variability. When an action is unrewarded, the reach angle is not updated. The model has two parameters, the standard deviations of the Gaussian distributions that generate the motor noise and exploration variability. We fit the model to each individual subject’s data using maximum likelihood estimation.
 
Three typical subject’s data (a cerebellar, old and young control) are shown along with a typical simulation for each (
Fig. 6
A–C and parameter estimates, see ‘Materials and methods’ section). The cerebellar subject had the largest motor noise, followed by the older control with the young control having the smallest amount of motor noise. In contrast, the younger control had the largest exploration variability. These parameters led to slower learning for the cerebellar and older control and faster learning for the younger control. Figure 6 D shows that group means match reasonably well to the mean of the simulations (averages of 10 000 simulations with each subjects fitted parameter).
 
The model predicts that the amount of learning and reward depends on both the motor noise and exploration variability. Figure 7 A shows the proportion of rewarded trials in late learning from the model for different settings of the motor noise and exploration variability. This shows that performance tends to decrease with motor noise and intermediate values of exploration variability lead to the greatest reward. Therefore for a given level of motor noise there is an optimal level of exploration variability that will lead to maximal adaptation. Achieving a good level of adaptation thus requires a balance between exploration and motor noise.
 
Examining the fits across the subjects (Fig. 7 B parameters with 95% confidence intervals) shows that there is wide variation of the parameters but that they tend to cluster by group. In addition there is variability in the per cent learning in the late perturbation period across subjects (percentage learning is shown as fill of the circular symbols in Fig. 7 B). The reason for this is that the model is probabilistic in nature and therefore even with identical parameters for exploration variability and motor noise the actual time series of reaches and adaptation will vary each time the model is run. For example, sometimes the model will be lucky and draw random samples of motor noise with less variability and sometimes the model will be unlucky and draw samples with more variability. We examined this by looking at the distribution of adaptation expected from the model when we performed many thousands of simulations and then determined where a participant’s actual adaptation lies within this distribution. Across all 34 participants we find on average that participants’ adaptation was ranked at 58 (where 1 is the model’s best and 100 the model’s worst performance) and that this rank was not significantly different from the expected value of 50 (P =  0.11). Importantly, even though there was variation in performance predicted by the model, the way in which a participant changed their reach angle as a function of current reach angle and reward (or lack thereof) allowed us to extract model parameters with reasonably narrow confidence limits.

 
Analysis of mean parameter values yielded significant differences between groups. One-way ANOVA of motor noise revealed a main effect of group [?
m
;
F(2,31) = 7.6,P < 0.01; Brown-Forsythe,F(2,19.9) = 7.2,P < 0.01;Fig. 7C] that resulted from significantly greater motor noise in the cerebellar group compared to both young (P < 0.05) and age-matched controls (P < 0.01). One-way ANOVA of exploration variability also revealed an effect of group [?e;F(2,31) = 6.4,P < 0.01; Brown-Forsythe,F(2,15.2) = 5.7,P < 0.05,Fig. 7C], which resulted from significantly greater exploration in young controls compared to both the cerebellar group (P < 0.05) and older controls (P < 0.01). Importantly, one-way ANOVA of the proportion of rewarded trials over the perturbation phase of the experiment revealed a main effect of group [ F (2,31) = 8.232, P <  0.01] where the reward rate was significantly lower for the cerebellar group (0.57 ± 0.04) than both the older (0.73 ± 0.04) and young control (0.76 ± 0.03) groups (both P < .01).

Although we have phrased the model as motor noise and exploration variability, a mathematically similar way of expressing this is that there is a total variability in a participant’s reach direction and that he or she is only aware of a proportion of this variability and corrects for this proportion when rewarded (see ‘Materials and methods’ section). We replot the parameter estimates as total variance and the proportion corrected for in Fig. 7D. One-way ANOVA of total variance revealed a main effect of group [F(2,31) = 9.9,P < 0.001; Brown-Forsythe,F(2,21.1724) = 10.0,P < 0.01] where both cerebellar (P < 0.001) and young control (P < 0.05) groups showed significantly greater variability than the older control group, but were not different from each other. Analysis of rho showed a significant main effect of group [F(2,31) = 4.8,P < 0.05; Brown-Forsythe,F(2,11.2) = 4.1,P < 0.05] where the cerebellar group was aware of a smaller proportion of their total variability compared with both older and young controls. This comparison was significant between the cerebellar and young control group (P < 0.05).","
We examined the learning and retention of a visuomotor rotation using error-based and reinforcement feedback, and whether these mechanisms depend on the integrity of cerebellar function. Reinforcement schedules produced better retention compared with error-based learning. Moreover, using a closed-loop reinforcement schedule, where the reward was contingent on prior performance, produced rapid learning. Cerebellar patients could learn under the closed-loop reinforcement schedule and retained much more of the learned reaching pattern compared to when they performed error-based learning. However, cerebellar patients varied in their learning ability in the reinforcement condition, with some showing only partial learning of the rotation. We developed a computational model of the reinforcement condition and found that learning was dependent on the balance between motor noise and exploration variability, with the patient group having greater motor noise and hence learning less. Our results suggest that cerebellar damage may indirectly impair reinforcement learning by increasing motor noise, but does not interfere with the reinforcement mechanism itself.
 
We based the open-loop task on prior work showing binary reinforcement could drive visuomotor learning in controls (Izawa and Shadmehr, 2011). However, in open-loop paradigms, subjects sometimes lag behind the perturbation to such an extent that they end up receiving no reward and no longer adjust their reaches or explore sufficiently to reacquire the target zone. We, therefore, included a closed-loop condition to mitigate this problem and to ensure that the reward schedule was set as close to 50% in the early period of learning. This was done by rewarding any reach that exceeded the average of the last 10 reaches in the desired direction (i.e. countering the rotation). This closed-loop paradigm led to more rapid learning than the open-loop reinforcement paradigm, with similar final levels and retention across our subjects.
 
While we designed our study to assess reinforcement and error-based learning in isolation, in general these mechanisms will work together in real-world situations. In the reinforcement task participants clearly do not have access to error information, whereas in the error-based task they have both error and task success (reinforcement) information. Nevertheless, our results show clear differences between the two paradigms, which suggests that we are largely studying distinct processes—one that is driven primarily by error that is not well retained and another clearly driven by reinforcement that is well retained. This is consistent with previous studies examining interactions between the two learning mechanisms, showing enhanced retention of an error-based learning process when reinforcement is also provided (Shmuelofet al., 2012;Galeaet al., 2015). In addition, combining error-based and reinforcement learning has been shown to speed up learning (Nikooyan and Ahmed 2015).

Cerebellar patients have known deficits in error-based learning(Martinet al., 1996;Maschkeet al., 2004;Richteret al., 2004;Morton and Bastian, 2006). Yet, the patients in our study were able to follow the gradual perturbation in the error-based condition. Analysis of their reach trajectories suggests that this ability relied on the patients using online visual feedback of the cursor to steer their reaching movements toward the target. As reach angles were calculated using the endpoint of each movement (this was necessary to compare the error-based task to the reinforcement tasks where reaches were rewarded based on movement endpoint), this feedback-dependent compensation made them appear to be learning. However, when visual feedback was removed to assess retention, the ability to correct the movement online was immediately lost (i.e. no retention). Thus, cerebellar patients did not truly adapt to the visuomotor rotation in the error-based task. Consistent with this, is that cerebellar patients cannot follow a gradual rotation when only endpoint cursor feedback is provided (Schlerfet al., 2013).

Reinforcement learning has been posited as a spared mechanism of motor learning following cerebellar damage. Consistent with this,Izawaet al.(2012)showed that cerebellar subjects could counter a gradual visuomotor rotation and generalize the new reach pattern to other targets when they learned with concurrent online visual cursor feedback and binary reinforcement feedback. They suggested that the cerebellar patients were relying on a reinforcement mechanism because they did not change the perceived location of their hand in a proprioceptive recalibration test, whereas control subjects did. Such proprioceptive recalibration is thought to be a hallmark of error-based adaptation (Synofziket al., 2008). Here we specifically show that cerebellar patients can use binary reinforcement feedback alone to alter their reaching movements, although some patients were able to learn more than others. All patients, regardless of how much they learned, showed almost complete retention in the reinforcement task. This is in in stark contrast to the same patients showing a complete lack of retention in the error-based paradigm.

The failure of some cerebellar patients to learn in the reinforcement task could be due to either deficits in the reinforcement learning mechanism itself, or deficits in other mechanisms which might limit the usefulness of reinforcement learning (or a combination of the two). Cerebellar damage causes reaching ataxia, indicated by curved and variable reaching movements (Bastianet al., 1996). Patients with cerebellar damage also have proprioceptive impairments during active movements that are consistent with disrupted movement prediction (Bhanpuri et al., 2013). Together these studies suggest that cerebellar damage increases motor noise and/or reduces the sensitivity with which they can locate their own arm. In other words, this leads to variability in reaching that cannot be estimated by the brain. We suspected that such variability (which we consider a form of motor noise) might interfere with reinforcement learning.

To test this hypothesis we used a simple model of the reinforcement task in which each subject was characterized by two sources of variability—one that they were unaware of, which we call motor noise, and one that they were aware of, which we term exploration variability. We chose to use a simpler model than the one used byIzawa and Shadmehr (2011), in which reward learning was based on a temporal difference learning algorithm. This algorithm requires specification of a range of parameters a priori (e.g. motor noise, discount rates, motor costs). In the temporal difference rule the current reward is compared to the expected reward to drive learning. However, due to the closed-loop nature of our paradigm, which set the perturbation so that the expected reward rate was always close to 50%, getting a reward was in general better than expected and failing to get a reward worse than expected. Therefore, we simplified the rule to update the motor command only for success and maintained the motor command for failure. This allowed us to avoid setting any parameters a priori and we fit two parameters to characterize subjects’ learning. Moreover, rather than fit squared error we were able to use a full probabilistic model using maximum likelihood, which allowed us to test whether our model was adequate to explain the subjects’ data. The model provided a good fit for the vast majority of subjects and showed that the patients’ had increased motor noise, but similar exploration variability compared to the matched controls. In other words, the majority of variability contributing to cerebellar patients’ behaviour could not be used by the motor system for learning. When reinforcement was received on successful trials, updating of internal estimates of the correct action (i.e. reach angle) was impaired—estimates could only be updated based on a small proportion of the movement variability (corresponding to the exploration component) resulting in less learning. The younger group had similar motor noise to the older control group but had higher exploration variability, which led to improved learning.
 
Previous work has noted that increased motor variability in the rewarded dimension of a reinforcement learning task is associated with more successful performance (Wuet al., 2014). These results suggested that behavioural variability might be a deliberate output of the motor system that is necessary during learning to explore the task space, find the optimal response and yield maximal reward. Our results are in general agreement with these findings as in a reinforcement learning task, exploration variability is essential. In general, for reinforcement learning the optimal amount of exploration variability will depend on the level of motor noise. Therefore, for a fixed level of motor noise subjects should ideally learn to set their exploration variability so as to have maximum adaptation. Although we have phrased the model as exploration variability and motor noise (Fig. 7C), a mathematically similar way of expressing this is that there is a total variability in a participant’s reach direction, but he or she is only aware of a proportion of this variability or only corrects for a proportion when rewarded (Fig. 7D). Under this interpretation of the model, the cerebellar subjects have higher overall variability and correct for less of this variability when they are successful.

 In summary, we have shown that cerebellar patients are able to use binary reinforcement feedback alone to learn and retain a visuomotor reaching task. However, their motor noise interferes with this process. Future work is needed to determine if motor noise can be reduced or if increasing exploration variability can benefit these patients."
1,2436073,"""Stress affects instrumental learning based on positive or negative reinforcement in interaction with personality in domestic horses""","""10.1371/journal.pone.0170783""",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5419560,sec009,sec012,"Effects of type of reinforcement and exposure to stressors unrelated to the learning task
In the absence of stressors unrelated to the learning task, the learning performance (number of trials to reach stage 1 and number of stages reached) did not differ between the two types of reinforcement (Kruskal-Wallis trials to reach stage 1: K = 7.07, P = 0.07, Kruskal-Wallis trials to reach stage 1: K = 8.68, P = 0.03, Dunn test PR vs. NR, Ptrials to reach stage 1 = 0.80, Pstages reached = 0.78, Fig 3A and 3B). However, when stressors unrelated to the learning task were added, learning performance significantly decreased with positive reinforcement and tended to decrease with negative reinforcement (Dunn test PR vs. PR+ES: Ptrials to reach stage 1 = 0.03, Pstages reached = 0.04; Dunn test NR vs. NR+ES: Ptrials to reach stage 1 = 0.06, Pstages reached = NS; Fig 3A and 3B).
 In the absence of stressors unrelated to the learning task, the observed behaviours did not differ between reinforcement types (Table 2, comparison between PR and NR). However, when stressors unrelated to the task were added, horses exhibited significantly more alert postures, startle reactions and glancing at the audience horse under both reinforcements contingencies; in the NR group, they also showed more ears pointing backward and blowing (Table 2, comparison between PR vs. PR+ES or NR vs. NR+ES). No significant differences were observed between the four groups in the number of times the horses sniffed or nibbled the experimenter, explored the environment, defecated or crossed the wooden bar.
 In the absence of stressors unrelated to the task, the increase in salivary cortisol concentrations did not differ between groups, neither in session 1 nor 3. When stressors unrelated to the task were added, the increase in concentration was significantly higher but only in the case of positive reinforcement (Dunn test PR vs. PR+ES: PSession 1 < 0.05, PSession 3 < 0.01; Table 3).
 Relationships between personality and learning performance
 Horses learning with negative reinforcement without stressors unrelated to the task (NR) that required the shortest time to reach the stage 1 criterion (i.e. the best performers among the NR horses) were the most fearful and active (correlation between the number of trials to reach stage 1 and a) intensity of the startle response during the surprise test: rs = -0.57, P = 0.03; and b) number of areas crossed: rs = -0.61, P = 0.02).
 Horses learning with negative reinforcement and exposure to extrinsic stress (NR+ES) that reached the highest number of stages (i.e. the best performers among the NR+ES horses) were the least fearful and active (correlations between the number of stages reached and a) number of glancing at the novel area: rs = 0.54, P = 0.04; b) total number of blowing: rs = 0.55, P = 0.03; c) latency before eating during the novel area test: rs = 0.48, P = 0.07 (tendency); d) number of contacts with the novel object: rs = -0.45, P = 0.09 (tendency); and e) number of areas crossed: rs = -0.54, P = 0.04). In addition, the fastest horses to reach the stage 1 criterion in the NR+ES group (the best performers) were also the least fearful, the closest to humans and the most active (correlations between the number of trials to reach the stage 1 criterion and a) number of contacts with the novel object: rs = -0.52, P = 0.04; b) number of contacts with the passive human: rs = -0.57, P = 0.03; and c) number of areas crossed: rs = -0.51, P = 0.05).
 By contrast, only one negative correlation between a variable related to fearfulness and the level of learning performance was found in each group of horses with positive reinforcement (PR and PR+ES). This correlation indicated that horses reaching the highest number of stages (i.e. the best performers among PR and PR+ES horses respectively) were the least fearful in both groups (correlations between the number of stages reached and a) the number of glancing at the novel area in PR group: rs = -0.51, P = 0.05; and b) the number of glancing at the novel object in PR+ES group: rs = -0.57, P = 0.03).","Our study shows that exposure to stressors before testing has a negative effect on learning performance, mainly under conditions of positive reinforcement. In addition, learning performance appears to be differentially related to personality according to the type of reinforcement and the presence of extrinsic stress. In the absence of stressors unrelated to the task, the most fearful horses were the best performers when they learned with negative reinforcement but the worst when they learned with positive reinforcement. When stressors unrelated to the task were applied, the most fearful horses were consistently the worst performers, particularly with negative reinforcement learning.
Stressors unrelated to the task impair learning performance according to the type of reinforcement
 In the absence of stressors unrelated to the task, we did not observe any effect of reinforcement type on learning performance and behavioural or physiological parameters. However, in the presence of stressors unrelated to the task, learning performances decreased or tended to decrease in both groups (NR+ES and PR+ES). Impairment of attention induced by the stressors could be involved [1]. Horses exposed to stressors unrelated to the task displayed more startled reactions and alert behaviours oriented outside of the learning task and directed at the audience horse than horses not exposed to stressors unrelated to the task, suggesting a decrease in attention toward the learning task itself, as observed previously [13]. Interestingly, this learning deficit appeared to be smaller in horses learning with negative reinforcement than in horses learning with positive reinforcement. This result could be explained by the fact that negative reinforcement in itself could be considered a stressor directly related to the cognitive task, known to alleviate learning performance by focusing attention toward the learning task ([1,3], e.g. [25]). This focus of attention toward the task could have counterbalanced the impact of the stressors unrelated to the task in the NR group. In addition, acute stress is known to decrease food motivation [26–28]. An adaptative response to stress consists for an organism to prepare itself to react to this threat, generally based on a fight or flight response. Escaping becomes then the priority other eating. Within a behaviour analytic framework, fear could serve as a potential establishing operation [29], increasing the effectiveness of escaping as a reinforcer. In contrast, that fear could serve as an abolishing operation for food as a reinforcer. Therefore, an impaired motivation for food rewards may also explain why learning performance was less impaired with negative than with positive reinforcement (i.e. food reward).
 Different patterns of behavioural and physiological responses between PR+ES and NR+ES horses were observed but did not provide an explanation for which group was more stressed. Elevated salivary cortisol levels indicate that the stress level might have been higher in horses learning with positive reinforcement, since PR+ES horses showed increased cortisol levels. However, NR+ES horses but not PR+ES horses showed more ears pointing backward and blowing, which might both indicate discomfort or stress [30–32]. One reason that may explain those differences is the history with escaping a stressor that changes according to the reinforcement type. Indeed, during each learning session, NR+ES experienced several times the avoidance of a negative outcome (i.e. tactile stimulation) by going actively into a safe compartment (i.e. pointed compartment). Experiencing this active escaping repeatedly may help reduce cortisol, whereas PR+ES horses did not experience such active process to cope with stress. However, even though the present results suggest that different patterns of stress response may emerge, they do not allow establishing clearly which reinforcement type was more stressful, and further investigations are needed to improve our knowledge of discomfort and stress measurements in horses.
 Importance of personality in relationship between stress and learning performance
 Our study reveals the existence of relationships between personality and learning performance. However, because personality is only one factor among others of predisposition of the learning performances, these relationships are tenuous. That explains why, at a statistical level of 5% and with only 15 animals per group, only a few variables of personality are significantly correlated with learning performance. However, what is interesting is that that these relationships vary with the presence of stressors unrelated to the task and type of reinforcement, especially for the dimension of fearfulness. In the absence of stressors unrelated to the task, the most fearful horses appear to be the best performers with negative reinforcement learning (NR group) but the worst when they had to learn with positive reinforcement learning (PR group). The fact that fearfulness might be advantageous when animals learn with negative reinforcement but disadvantageous when they learn with positive reinforcement is in agreement with previous experiments (active avoidance in guppies [33], instrumental task in horses [22,34,35], instrumental task of discrimination in ravens [36], instrumental cooperative task in rooks [37]). It is possible that the positive effect of negative reinforcement (considered as stressor) on performance, through the focus of attention toward the task, might have been accentuated in fearful individuals and could explain their improved performance in the absence of stressors unrelated to the task. By contrast, there were no stressors related to the learning task in the PR group, and the fearful horses might have been more easily distracted by the external environment. When stressors unrelated to the task were added, learning performance of the most fearful horses appeared to be consistently impaired, independent from the type of reinforcement (PR+ES and NR+ES groups). The disruption of cognitive and attentional processes caused by the stressors unrelated to the task is likely to be more pronounced in fearful individuals who react more strongly to various forms of stressors (e.g. [37], synthesis: [38,39]).
 Finally, our study indicates that other dimensions of personality may also influence learning performance. Locomotor activity was positively related to performance in negative reinforcement learning, independent from exposure to stressors unrelated to the task. We hypothesize that locomotor activity may broadly reflect a tendency to initiate actions, since we previously observed such a positive effect with learning tasks requiring a displacement of body position similar to the present task [22], but also when requiring different types of movement (touching an object with the nose: [13]). In addition, reactivity to humans was negatively correlated with learning performance in the NR+ES group only. The horses less frightened by humans might have been less affected by the stressors unrelated to the task because they were more attentive to human cues.
 We have to mention again that with 15 animals per group, only a few significant correlations between variables of personality and learning performance were revealed. To reveal the whole influence of personality on learning abilities, it would be ideal to perform this kind of studies on a larger amount of subjects, maybe several hundreds. Unfortunately, it is quite impossible to test this number of subject, within the equine species. However, we have to underline the fact that, taken together, all the experiments carried out on horses reveal consistent links between personality and learning abilities [10,13,14,22,40]. It would mean that these correlations are not obtained by chance, but more likely reflect a real influence of personality.
 Conclusion
 The present study contributes to a better understanding of the influence of stress on learning performance by showing the importance of the nature of the stress (related or unrelated to the task) and personality. Our results also provide important clues to more personalized training according to each animal’s needs. Indeed, the present study shows that positive and negative reinforcement may lead to equivalent learning performance in the absence of stressors unrelated to the task. Considering that previous studies highlighted long-lasting promising effects of positive reinforcement on training, welfare and relationships with humans [32,41–46], our results provide additional evidence for promoting the use of positive reinforcement in horse training, while the use of negative reinforcement still dominates traditional training methods [47,48]. However, the present results also show that the use of food reward as reinforcement may not be adequate in stressful conditions. Indeed, the loss of food motivation induced by stress could render the reinforcement inefficient and constitute an additional source of stress. In addition, our analyses including personality suggest that stress is a key factor in understanding how animals differ in learning performance according to their personality. Predicting how fearful animals react when they face a learning challenge therefore not only requires an evaluation of stress level but also of the nature of the stress (related or unrelated to the task) and their possible interactions. Future investigations are needed to define more precisely when the switch from favourable to unfavourable conditions for learning occurs according to the type and the intensity of a stressor and how it relates to personality."
2,1649970,"""Roles of OA1 octopamine receptor and Dop1 dopamine receptor in mediating appetitive and aversive reinforcement revealed by RNAi studies""","""10.1038/srep29696""",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4944188,__sec1,__sec5,"Suppression of gene expression by dsRNA injections of OA1, Dop1 and Dop2 in the cricket
We first studied the effects of double-stranded RNA (dsRNA) injection of the OA1, Dop1 or Dop2 gene on the expression levels of target gene mRNA in the cricket head. As a control, the amount of OA1, Dop1 or Dop2 mRNA in the brains of DsRed2 RNAi crickets was also studied. Groups of crickets were injected with 20?pmol OA1, Dop1, Dop2 or DsRed2 dsRNA into the head hemolymph. Target sequences of RNAi for OA1, Dop1 and Dop2 are shown in Methods and in Fig. 1. The amounts of mRNA in the heads of dsRNA-injected crickets were measured by quantitative real-time PCR (qPCR) at 48?hr after injection, and the relative amount of mRNA transcribed from the target gene was compared to that in intact crickets. The levels of OA1, Dop1 and Dop2 mRNA in DsRed2 RNAi crickets were similar to those in intact crickets (Fig. 2). In contrast, expression levels of OA1, Dop1 and Dop2 mRNA in the heads of OA1, Dop1 and Dop2 RNAi crickets were, on average, 31%, 29% and 25% of those in intact crickets. Statistical comparisons showed that the levels of OA1, Dop1 and Dop2 mRNA in OA1, Dop1 and Dop2 RNAi crickets were significantly lower than those in DsRed2 RNAi crickets (OA1: t?=?3.68, df?=?4.60, p?=?0.017; Dop1: t?=?3.09, df?=?8.26, p?=?0.014; Dop2: t?=?2.31, df?=?15.28, p?=?0.035, two sample t-test with Welch’s correction). All RNAi crickets exhibited normal viability, and their locomotor activities and feeding behaviors were not distinguishable from those of intact animals. Evidence suggesting intact sensory and motor functions of those RNAi crickets is described below.
Effects of the suppression of OA1, Dop1 and Dop2 genes on acquisition and retention in appetitive conditioning
 We then studied the effects of gene silencing by RNAi on acquisition and retention performances in appetitive olfactory conditioning. We used conditioning of maxillary palpi extension response (MER) for evaluation of the conditioning effect8. We have reported that presentation of peppermint or apple odor to the antennae of crickets rarely (in less than 20% of crickets) induces MER and that pairing of each odor with water leads to an increase in the probability of MER8.
 We first studied acquisition performance in appetitive conditioning in groups of crickets injected with dsRNA. Four groups of crickets were injected with 20?pmol dsRNA of OA1, Dop1, Dop2 or DsRed2. Two days after the injection, they were subjected to 4-trial conditioning to associate an odor with water reward with inter-trial intervals of 5?min. The absence or presence of MER to 3-sec odor (CS) presentation prior to presentation of water (US) was recorded (Fig. 3a).
 All groups exhibited significant increases in the percentages of MER to the CS with increasing the number of conditioning trials (Fig. 4a, Cochran’s Q test: OA1: ?2?=?9.8, p?=?0.02; Dop1: ?2?=?32.3, p?=?0.00000046; Dop2: ?2?=?32.2, p?=?0.00000048; DsRed2: ?2?=?31.9, df ?=?3, p?=?0.00000056). Between-group comparison, however, showed that the percentage of MER to the CS in the OA1 RNAi group was significantly lower than that in the control (DsRed2 RNAi) group in the 4th trial (i.e., after the third training, see Fig. 3) (Fisher’s exact test, adjusted by Holm’s method: p?=?0.012, sample numbers shown in the figure). In contrast, the percentages of MER to the CS in the Dop1 and Dop2 RNAi groups did not significantly differ from that in the DsRed2 RNAi group (Fisher’s exact test, adjusted by Holm’s method: Dop1: p?=?0.781, Dop2: p?=?0.189). The results indicate that silencing of OA1 significantly reduces acquisition in appetitive learning, but that of Dop1 or Dop2 does not. Impairment of appetitive learning by silencing of OA1 is not due to impairment of the perception of water US, because the crickets exhibited MER to a drop of water applied to the mouth or palpi. Moreover, perception of CS and behavioral responses to the CS are intact in the OA1 RNAi crickets, as evidenced by their intact aversive learning described below.
 Next, retention performance was tested at 30?min after 4-trial appetitive conditioning. The Dop1, Dop2 and DsRed2 RNAi groups exhibited high percentages (more than 70%) of MER to the odor paired with the US (paired odor or CS), and the percentages were significantly greater than those of MER to the odor not used in training (novel odor) (Fig. 4b, McNemar’s test: Dop1: ?2?=?11.1, df?=?1, p?=?0.0000013; Dop2: ?2?=?9.09, p?=?0.0026; DsRed2: ?2?=?19, p?=?0.0000013). In the OA1 RNAi group, on the other hand, percentage of MER to the CS did not significantly differ from that to the novel odor (?2?=?0, p?=?1). Thus, the Dop1 and Dop2 RNAi groups exhibited normal retention of CS-specific appetitive memory, but the OA1 RNAi group did not.
 Effects of the suppression of OA1, Dop1 and Dop2 genes on acquisition and retention in aversive conditioning
 In aversive conditioning experiment, we used vanilla or maple odor, to which crickets exhibited high percentages (more than 70%) of MER8. We observed that repeated presentation of these odors alone led to a decrease of MER percentages8, obviously due to habituation. In order to discriminate pairing-specific decrement in percentages of MER due to aversive conditioning from this non-associative effect, we used a differential conditioning procedure in which one of the two odors was paired with 20% sodium chloride solution (paired odor) and the other was presented alone (unpaired control odor) (Fig. 3b)8, and the percentages of MER to the paired and unpaired odors were statistically compared.
 Four groups of crickets were injected with dsRNA of OA1, Dop1, Dop2 or DsRed2, and two days later they received 4-trial differential aversive conditioning training with 5-min inter-trial intervals (Fig. 3). The percentages of MER to the unpaired odor significantly decreased with the increase of the trial number in the OA1, Dop1 and Dop2 RNAi groups (Cochran’s Q test: OA1: ?2?=?22.6, p?=?0.000050; Dop1: ?2?=?11.6, p?=?0.0088; Dop2: ?2?=?26.6, p?=?0.0000071). DsRed2 RNAi group also showed a decrease of percentages of MER to the unpaired control odor, but the decrease was not statistically significant (?2?=?3.5, df?=?3, p?=?0.317). This was obviously due to a slightly lower percentage of MER in the first trial, which appeared to reflect incidental data variation due to small sample size.
 Percentages of MER to the paired odor significantly decreased with progress of training in all groups (Fig. 5, Cochran’s Q test: OA1: ?2?=?59.9, p?=?0.00000000000062; Dop1; ?2?=?13.7, p?=?0.0034; Dop2: ?2?=?57.5, p?=?0.0000000000020; DsRed2: ?2?=?24.1, df?=?3, p?=?0.000023). In OA1, Dop2 and DsRed RNAi groups, percentages of MER to the paired odor were significantly lower than those to the unpaired odor in the 3rd trial (McNemar’s test: OA1: ?2?=?4.57, p?=?0.033; Dop2: ?2?=?9.31, p?=?0.0023, DsRed2: ?2?=?4.5, df?=?1, p?=?0.033) and 4th trial (OA1: ?2?=?5.4, p?=?0.020; Dop2: ?2?=?12, p?=?0.00053; DsRed2: ?2?=?6.4, p?=?0.011), indicating that aversive conditioning is successful. In the Dop1 RNAi group, on the other hand, the percentages of MER to the paired odor did not significantly differ from those to the unpaired odor in the 3rd trial (McNemar’s test: ?2?=?0.29, p?=?0.59) and 4th trial (?2?=?0.25, p?=?0.62). The results suggest that silencing of Dop1, but not that of OA1 or Dop2, impairs acquisition in aversive conditioning.
 Retention performance was tested at 30?min after 4-trial differential aversive conditioning. The OA1, Dop2 and DsRed2 RNAi groups exhibited significantly lower percentages of MER to the CS (paired odor) than to the unpaired odor (Fig. 5, McNemar’s test: OA1: ?2?=?4.5, p?=?0.034; Dop2: ?2?=?7,14, p?=?0.0075; DsRed2: ?2?=?9, df?=?1, p?=?0.0027). In the Dop1 RNAi group, on the other hand, the percentages of MER to the paired odor did not significantly differ from those to the unpaired odor (McNemar’s test, ?2?=?0.29, p?=?0.59). Thus, the OA1 and Dop2 RNAi groups exhibited normal retention of CS-specific aversive memory, but the Dop1 RNAi group did not.","There has been a discrepancy in the results of studies on neurotransmitters mediating reinforcement signals in crickets and fruit-flies, and the purpose in this study was to fully resolve the discrepancy. Results of studies using transgenic fruit-flies have suggested that different types of dopamine neurons mediate both appetitive and aversive reinforcement signals via the Dop1 receptor12,13,14,15,16,34,35. On the other hand, results of our pharmacological studies have suggested that octopamine and dopamine mediate appetitive and aversive reinforcement signals in crickets3,4,5,6,7,8,36. In accordance with this, moreover, our recent study using Dop1-knockout crickets showed that they are defective in aversive learning but not in appetitive learning19. The results of that study, however, were not conclusive, since the aversive learning defects may be due to defects in the development of brain circuitry.
 In this study, we observed that OA1, Dop1 and Dop2 RNAi crickets exhibited ca. 70–75% reduction in expression levels of these genes. These results suggest that silencing of the expression of these genes is successful, though we could not determine amounts of these receptor proteins in the brains of dsRNA-injected crickets because antibodies against these receptors were not available. We observed that Dop1-silenced crickets exhibited impairment of aversive learning but not appetitive learning, whereas OA1-silenced crickets exhibited impairment of appetitive learning but not aversive learning. The impairments in RNAi crickets were not due to defects of odor (CS) or water (US) perception or motor function necessary for performance of MER, because the Dop1-silenced crickets exhibited normal appetitive MER conditioning and the OA1-silenced crickets exhibited normal aversive conditioning. It can be argued that different experimental procedures for appetitive and aversive conditioning (absolute or differential conditioning and the use of different odors) might be the reason for the different effects of Dop1 and OA1 gene silencing. However, this is unlikely because we previously showed that effects of dopamine and octopamine receptor antagonists are conserved among experiments with an absolute or differential conditioning procedure and among different kinds of odors used as the CS (see discussion in ref. 8). The consistency between the results of the present gene silencing study with those of the pharmacological and Dop1 gene knockout study using CRISPR/Cas9 (cited above) strongly suggests that dopamine mediates aversive reinforcement via Dop1 receptors and octopamine mediates appetite reinforcement via OA1 receptors in associative learning in crickets.
 Our observation that silencing of OA1 but not that of Dop1 or Dop2 impairs appetitive learning suggests that impairment of appetitive learning by administration of octopamine receptor antagonists observed in our previous studies3,4,5,6 is due to their effect on OA1 receptor. We observed that epinastine and mianserin impair appetitive learning but not aversive learning and, since these drugs are known as potent antagonists of insect octopamine receptors37,38, we suggested that octopamine mediates appetitive reinforcement but not aversive reinforcement. A recent study in honey bees, however, suggested that these drugs antagonize not only OA1 but also Dop239, which raised the possibility that the impairment might be mediated by blockade of the Dop2 receptor, instead of or in addition to the OA1 receptor. The finding in the present study that silencing of Dop2 does not impair appetitive learning refutes this possibility.
 Our suggestion that OA1 but not Dop1 mediates appetitive reinforcement with water reward in crickets fundamentally differs from the suggestion based on results of studies using transgenic fruit-flies that Dop1 mediates appetitive reinforcement with sucrose or water reward in addition to aversive reinforcement12,13,14,15,16,34,35. The different conclusions regarding neurotransmitters mediating appetitive reinforcement obtained in studies on crickets and fruit-flies cannot be ascribed to a slight difference in conditioning and testing procedures (see discussion in ref. 8). We thus conclude that neurotransmitters mediating appetitive reinforcement indeed differ in crickets and fruit flies, whereas those for aversive reinforcement are the same.
 The results of our studies in crickets raise a question about the diversity and evolution of reinforcement systems for associative learning among animals. There is evidence suggesting that dopamine neurons mediate appetitive reinforcement in mammals1,2, mollusks40 and fruit-flies14,15, whereas octopamine neurons have been suggested to mediate appetitive reinforcement in honey bees9,10. The diversity of neurotransmitters mediating appetitive reinforcement among invertebrates and vertebrates should emerge as a fascinating research subject.
 The fact that appetitively reinforcing neurons in mammals and crickets use different neurotransmitters may indicate that they mediate different information for reinforcement. Results of our recent study, however, suggested that the signals that these neurons mediate are conserved between mammals and crickets. In mammals, it has been shown that whether appetitive learning occurs is determined by the discrepancy, or error, between the predicted reward and the actual reward41 and that certain classes of midbrain dopamine neurons mediate reward prediction errors1,2. In crickets, our behavioral and pharmacological studies suggest that octopamine neurons mediate reward prediction error33. We are currently investigating whether dopamine neurons mediate punishment prediction error in aversive learning in crickets.
 We found no impairment in appetitive or aversive learning in Dop2 RNAi crickets. Since we did not evaluate to what extent the level of Dop2 protein is reduced in Dop2 RNAi crickets and since Dop2 is known to be expressed in some Kenyon cells in crickets24, more studies are needed to examine the possibility that Dop2 plays some roles in olfactory learning and memory. Insects possess several types of octopamine and dopamine receptors other than Dop1, Dop2 and OA125,26,27. Expression of these genes in the cricket brain and the possible participation of these receptors in learning and memory remain as subjects of our future study.
 There is dense expression of Dop1 mRNA in Kenyon cells of crickets24. A high expression level of OA1 mRNA in Kenyon cells of the mushroom body has been reported in the fruit-fly31 and honey bee32, and there is an urgent need to confirm this in crickets. The mushroom body is known to play critical roles in olfactory learning in fruit-flies29, honey bees30, cockroaches42 and crickets43. Anatomical and physiological characterization of dopaminergic and octopaminergic neurons that make synapses with Kenyon cells is the next step for elucidation of the neural mechanisms of appetitive and aversive learning in crickets.
 We conclude that the neurotransmitter and the receptor mediating appetitive reinforcement signals differ in crickets and fruit-flies. This urges us to examine neurotransmitter mechanisms for associative learning in different species of insects to evaluate the diversity, and evolution, of reinforcing mechanisms in insects. Moreover, consideration of the ubiquity and diversity of reinforcing mechanisms for associative learning in vertebrates and invertebrates should become an important research subject in neuroscience."
3,1962556,"""Increasing Motor Noise Impairs Reinforcement Learning in Healthy Individuals""","""10.1523/ENEURO.0050-18.2018""",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6088368,s4,s5,"Experiment 1
The first experiment examined how adding noise affects reinforcement motor learning in healthy subjects. Figure 3a shows the group mean time series for all three experimental conditions in which no noise, or low or high levels of noise were added (green, yellow, and red curves, respectively). Participants’ average reaches during the baseline phase of each condition, both before and after the addition of noise, were very similar (Fig 3b). This suggests that simply adding noise did not impair their baseline ability to perform the task.
 When subjects performed the control condition (i.e., with no noise added), they showed strong learning to the initial rotation, followed by a return to baseline, and finally strong learning of the second opposite rotation. In this condition, subjects hit a mean reach angle of 12º (in the direction countering the rotation) across the final 40 trials of each rotation phase (Fig 3b, green). Similar levels of adaptation (?10º) were seen when subjects had a low noise level added (Fig 3b, yellow), although they had a significantly lower reward rate (61% vs 77%). However, in the high-noise condition (Fig. 3b, red), subjects showed significantly reduced adaptation (?5º) and a lower reward rate (46%).
 We performed a repeated-measures ANOVA for average hand reach angle in the final 40 trials of each block (Fig 3a, BL1, BL2, R1, and R2) and condition. This showed a significant main effect of block (F(2,20) = 47.950, p < 0.001; Greenhouse–Geisser corrected, F(1.248,12.480) = 47.950, p < 0.001) and condition (F(2,20) = 10.402, p = 0.001) as well as a significant block by condition interaction (F(4,40) = 12.349, p < 0.001; Fig. 3b). Post hoc analysis showed no significant differences between the first and second baseline blocks in any of the experimental conditions (all p > 0.05). However, in all three conditions, there was significant adaptation during both the first rotation block (all p < 0.001) and the second rotation block (control and low noise, p < 0.001; high noise, p = 0.028) compared with their respective baselines. In the rotation block, reach angle adaptation in the high-noise condition was significantly reduced compared with both the control (p = 0.002) and low-noise (p = 0.006) conditions. No significant difference was found in learning between the control and low-noise conditions (p = 1.00). Overall, these results suggest that participants were able to use reinforcement feedback to significantly alter their reach angle in all three experimental conditions, but this was reduced in the high-noise condition.
Figure 3c shows that the reinforcement rate was highest in the control condition and dropped in the low-noise and high-noise conditions (72%, 61%, and 46%, respectively; F(2,30) = 37.405, p < 0.001). Note that the reinforcement rate showed a statistically significant decrease with each level of added noise (all post hoc tests, p < 0.001).
 To parse the influence of reinforcement feedback on subjects’ behavior in the noise conditions, we computed the absolute change in reach angle as a function of whether a trial was rewarded or not (Pekny et al., 2015) and whether the true reach was in or out of the reward zone. Figure 4 shows that subjects changed their reach angle more following unrewarded trials compared with rewarded trials, regardless of whether they were in or out of zone. The main effect of reward for the low-noise and high-noise conditions were F(1,10) = 29.179, p < 0.001 and F(1,10) = 29.179, p < 0.001, respectively. In the low-noise condition, there was also a main effect of reach accuracy (F(1,10) = 9.299, p = 0.012). Finally, there were significant reward by accuracy interactions in both the low-noise condition (F(1,10) = 7.006, p = 0.024; Fig. 4a) and the high-noise condition (F(1,10) = 14.270, p = 0.004; Fig. 4b).
 Experiment 2
 The second experiment examined whether slower learning in the high-noise condition of experiment 1 resulted from reinforcing errors by clamping the reinforcement rate at 33.8% (clamp) in a new group of participants. This was done to match the proportion of the trials reinforced in the high-noise task when the hand reach angle was correct. The high-noise and clamp tasks were now similar, except that the latter was not reinforced on any error trial (12% of trials in the high-noise task). To ensure that the reinforcement rate was indeed held at a fixed value throughout the clamp condition, we analyzed the group mean reinforcement rate at the following four phases of the task: the final 40 trials of the second baseline phase; the final 40 trials of the first rotation; the final 40 trials of the return to baseline; and the final 40 trials of the second rotation. Repeated-measures ANOVA showed no significant differences across the phases (F(3,27) = 1.637, p = 0.204).
 A paired-samples t test showed that the difference in reinforcement rate between the control and clamp conditions was significant (t(9) = 14.1524, p < 0.001; Fig. 5b). Figure 5a shows the group mean time series for the two conditions. The average reach angle during the baseline phase of the clamp condition was similar to that during the control condition, indicating that the reduction in reinforcement rate did not impair participants’ baseline ability to perform the task. We also found that the lower reinforcement rate of the clamp condition did not affect the learning of the initial rotation, return to baseline, or the learning of the second opposite rotation. Repeated-measures ANOVA showed only a significant main effect of block (F(2,18) = 30.734, p < 0.001; Greenhouse–Geisser corrected: F(1.286,11.572) = 30.734, p < 0.001; Fig. 5c). Post hoc analysis showed that the main effect was driven by significant differences between the two baseline blocks and the rotation block (both p < 0.001). There was no significant difference between the first and second baseline block (p = 1.00). Within both the control and clamp conditions, there was significant adaptation in both the first and second rotations (all p < 0.001) compared with both baseline blocks. This suggests that the reduced learning rate in the high-noise group from experiment 1 was due to the 12% of erroneous reaches that were rewarded when ?hand was outside the reward zone.
 As in experiment 1, we analyzed the absolute change in reach angle as a function of reinforcement feedback. Repeated-measures ANOVA showed only a significant main effect of reward (F(1,9) = 12.490, p = 0.006; Fig. 5d). That is, participants showed a greater change in reach angle following unrewarded trials compared with rewarded trials. There was no effect of condition (F(1,9) = 0.416, p = 0.535) or reward by condition interaction (F(1,9) = 0.469, p = 0.511). This suggests that although participants altered their behavior following unrewarded trials more than following rewarded trials, they did not change their behavior from the control and clamp conditions.
 Comparison with patients with cerebellar damage
 We compared subjects’ learning of the first rotation in experiment 1 to the performance of a group of individuals with cerebellar degeneration who learned a single 15º rotation using the same closed-loop reinforcement feedback as part of a previous study (published in Therrien et al., 2016). Figure 6 shows the time series data comparing the performance of the group with cerebellar damage to the groups in control (Fig. 6a), low-noise (Fig. 6b), and high-noise (Fig. 6c) conditions of experiment 1. The group of patients with cerebellar damage was able to reach within the target zone at baseline, but were unable to reach to the new target zone in the rotation block. This differed from the control and low-noise conditions of experiment 1, where participants were able to learn to rotate their reach angles to the new target zone. Similar to the patient group, participants were unable to reach the final target zone in the high-noise condition of experiment 1. However, participants in the high-noise condition still showed a faster learning rate than patients with cerebellar damage.
 Using independent-samples t tests, we compared the mean values of the group with cerebellar damage for early learning rate, total learning, and reinforcement rate for each condition of experiment 1 (Fig. 6d–f). Participants in the control condition showed significantly greater total learning (t(21) = ?3.2648, p = 0.004), early learning rate (t(21) = 4.3910, p < 0.001), and reinforcement rate (t(21) = ?3.8324, p < 0.001) compared with the group of patients with cerebellar damage. Participants also showed significantly greater total learning (t(21) = ?3.9907, p < 0.001) and early learning rate (t(21) = 5.3784, p < 0.001) in the low-noise condition compared with the patients with cerebellar damage. The difference in reinforcement rate between the low-noise condition and the group of patients with cerebellar damage was not significant (t(21) = ?1.1919, p = 0.247).
 Finally, comparing the high-noise condition to the patients with cerebellar damage, we found that total learning was not significantly different between groups (t(21) = ?1.861, p = 0.249). However, there were significant differences in early learning rate (t(21) = 3.6563, p = 0.002) and reinforcement rate (t(21) = 2.4292, p = 0.024). The early learning rate was greater for participants in the high-noise condition compared with patients with cerebellar damage. However, patients with cerebellar damage showed a greater reinforcement rate than participants in the high-noise condition. Overall, these results suggest that adding noise to the reaches of neurologically healthy participants’ impaired reinforcement learning in our task, but did not completely replicate the behavior of the patient group.
 Modeling results
 The objective of experiment 1 was to test the hypothesis that increased variability from motor noise would impair reinforcement learning in a similar way to patients with cerebellar degeneration. While behavioral data showed similar total learning between the high-noise and patient groups, the high-noise group showed a faster early learning rate. To determine the source of this discrepancy, we modeled subjects learning in the three conditions of experiment 1 using a simple mechanistic model of the reinforcement learning task. The reach angle executed on each trial is modeled as the sum of an internal estimate of the correct reach angle and two sources of behavioral variability, as follows: exploration variability and motor noise. The model assumes that participants have access to the variability from exploration, but do not have access to motor noise. As a result, if a reach is reinforced, the model only updates the internal estimate of the correct movement by the draw of exploration variability on that trial. When a reach is not reinforced, the internal estimate is not updated. The model has three parameters: the SDs of the Gaussian distributions that generate motor noise and exploration variability following rewarded and unrewarded trials. We fit the model to each subject’s data in each condition of experiment 1 as well as each subject’s data in each condition of experiment 2. We also fit the data for each patient with cerebellar damage. The model was fit using maximum-likelihood estimation.
 Comparing mean parameter values between subjects in experiment 1 and the group of patients with cerebellar damage revealed significant differences in motor noise (Fig. 7a). Patients with cerebellar damage had significantly greater motor noise than participants in the control (t(21) = 3.5707, p = 0.002) and low-noise (t(21) = 3.1929, p = 0.004) conditions of experiment 1. The difference between patients with cerebellar damage and participants in the high-noise condition was not significant (t(21) = 1.3225, p = 0.200), suggesting that this condition effectively matched the motor noise of patients with cerebellar damage. There were no differences between groups across the conditions of experiment 1 for exploration variability following rewarded trials (control condition: t(21) = ?0.6409, p = 0.529; low-noise condition: t(21) = 1.0964, p = 0.285; high-noise condition: t(21) = 1.2381, p = 0.229; Fig. 7b). However, patients with cerebellar damage showed significantly smaller exploration following unrewarded trials compared with participants in all three conditions of experiment 1 (control condition: t(21) = ?2.5702, p = 0.018; low-noise condition: t(21) = ?4.5549, p < 0.001; high-noise condition: t(21) = ?2.8990, p = 0.009; Fig. 7c). Thus, adding noise in experiment 1 did not reduce participants’ exploration following errors relative to the control condition. This left them greater variability from which to learn compared with patients with cerebellar damage.
 There were no differences in fitted parameter values between the control and clamp conditions of experiment 2 (Fig. 7d: ?m, t(9) = ?1.7678, p = 0.111; Fig. 7e: ?e after rewarded trial, t(9) = ?0.2686, p = 0.794; Fig. 7f: ?e after unrewarded trial, t(9) = ?0.4655, p = 0.653). This suggests that withholding reinforcement of correct movements did not change participants’ motor noise or exploration variability relative to the control condition.
 To determine the goodness of fit of our model, we compared the mean reaching behavior over subjects in each condition of experiments 1 and 2 to the mean of the model simulations. This resulted in R 2 values of 0.95, 0.91, and 0.66 for the control, low-noise, and high-noise conditions of experiment 1. For experiment 2, R 2 values were 0.87 and 0.76 for the control and clamp conditions, respectively.
 Finally, to examine the importance of each model parameter we compared the full three-parameter model to two reduced models (one with no motor noise and one with exploration variability) that did not depend on whether the previous trial was rewarded or not (Therrien et al., 2016). We did not examine a model with motor noise only because some exploration variability is needed to show learning. Model comparisons using the BIC showed that the three-parameter model best fit the data for experiments 1 and 2, while the reduced model with exploration that was independent of reward on the previous trial best fit the data for patients with cerebellar damage (Table 1).","We examined whether perturbing neurologically healthy individuals by adding noise to their reach endpoints would impair reinforcement learning in a manner similar to what has been observed in individuals with cerebellar damage (Therrien et al., 2016). Adding a low level of noise, to increase participants’ baseline variability by 50%, did not impair learning relative to a control condition where no noise was added. However, adding a high level of noise (to increase baseline variability by 150%) significantly impaired learning. Increasing variability affects the mapping of hand location to reward. That is, in the presence of noise it is possible for the hand to be within the reward zone yet not be rewarded or, conversely, be rewarded when outside it. To assess whether reinforcing errors could account for impaired learning with high noise, we performed an additional experiment in which we artificially reduced (clamped) the reinforcement rate to match the reinforcement corresponding to reaches where both the hand and noisy locations were in the reward zone. In contrast to the noise conditions, in this additional task participants were never rewarded when the hand location was outside the reward zone. Reducing reward yielded learning similar to that in a control condition. Together, these results suggest that the reduced learning in the high-noise condition was driven by the reinforcement of incorrect behavior, rather than not reinforcing correct behavior. Finally, comparing performance in the high-noise condition to that of a group of patients with cerebellar damage showed similar total learning between the groups, but faster early learning in the high-noise condition.
 Similar to previous work (Pekny et al., 2015), we found a larger change in reach angle following unrewarded trials, suggesting that participants tend to explore more following errors than after successful movements. However, when noise was added, this exploratory behavior was modulated by whether outcome feedback matched the true hand position. That is, participants showed greater change following unrewarded trials when the hand reach angle was outside the reward zone (i.e., an appropriate withholding of reward) compared with when the hand reach angle was actually correct (i.e., a false withholding of reward). In experiment 2, the change in reach angle was also greater for the unrewarded versus rewarded trials. However, there was no difference between control and clamp conditions. This is in contrast to the results of Pekny et al. (2015), who found that clamping reinforcement at a lower level increased variability following unrewarded trials. This discrepancy may have been the result of methodological differences between the two tasks. In their study, Pekny et al. (2015) clamped the reinforcement rate during a prolonged period where no rotation perturbation was applied. Thus, many subjects would have reached a plateau in performance before experiencing the clamp. A sudden reduction in the reward rate under these conditions may have prompted subjects to change their behavior to search for a new solution to the task. In our study, however, the clamp was applied during the rotation phase. Here, subjects would naturally experience changes in the reinforcement rate as the task solution changed with each rotation. As a result, subjects in our study may have been less likely to change their behavior, relative to the control condition, on the introduction of clamp.
 Adding a high level of noise to reaches of healthy participants matched the total learning of a group of patients with cerebellar damage. However, healthy participants still showed a faster early learning rate than the patient group. To describe how variability from noise influenced learning in our task, we expanded a model developed in our previous work (Therrien et al. (2016). The simple mechanistic model assumes that trial-to-trial variability in subjects’ reach angles stems from two broad sources termed “exploration variability” and “motor noise.” The important distinction between these sources of variability is that the sensorimotor system has access to the amount of exploration on any trial, but it does not have access to the motor noise on that trial. Although the model is framed in terms of motor noise and exploration variability, it is equally valid to view the motor noise as proprioceptive noise (or a combination of both motor and sensory noise), so that this noise limits the ability to localize the limb. As a result, when a reach is reinforced, the motor system can only learn from the magnitude of exploration that contributed to it. Thus, high motor noise may decrease the efficiency of learning by altering the mapping of the reach angle to the reinforcement signal. Here, we allowed exploration to vary depending on whether the previous trial was rewarded or not. Fitting the model to an individual participant’s task performance revealed that added noise increased the fitted motor noise in healthy participants to match that found in patients, but there were group differences in exploration variability. While patients with cerebellar damage showed similar exploration following rewarded trials compared with healthy control subjects with and without added noise, their exploration following unrewarded trials was reduced. This suggests that the patient group was less able to modify their behavior following errors than healthy participants, even when the level of noise was matched between groups.
 A discrepancy in error sensitivity between our high-noise condition and the patients with cerebellar damage could have arisen for a number of reasons. Studies of visuomotor adaptation have shown that healthy individuals are able to detect false or variable feedback and explicitly alter their behavior so as to learn normally (Bond and Taylor, 2017; Morehead et al., 2017). Added noise in the present study was akin to providing participants with false feedback. Given that they had normal proprioceptive precision, it is possible they were aware of a discrepancy between the movements performed and the feedback received, which may have reduced their sense of agency over feedback about performance errors (Parvin et al., 2018). Furthermore, healthy participants may have been able to use an estimate of the discrepancy to adjust their response to achieve more rewarding feedback. In contrast, pathological motor variability from cerebellar damage is considered to be the product of faulty predictions of limb states (Miall et al., 2007; Miall and King, 2008), which result in poor compensation for limb dynamics and interjoint interaction torques during movement (Bastian et al., 1996; Bhanpuri et al., 2014). Therefore, in patients with cerebellar damage, noise may increase uncertainty about the movement performed—that is, decrease proprioceptive precision (Bhanpuri et al., 2013; Weeks et al., 2017a,b). While the feedback resulting from such movements can also be viewed as false, patients with cerebellar damage are likely to be less able to detect and estimate the discrepancy, making it difficult to detect the source of errors.
 Previous work has addressed how motor noise can alter learning in a variety of motor tasks. There are several studies of error-based learning that have artificially added noise into various sensorimotor tasks. These have shown that, although performance degrades, participants change their behavior so as to be close to optimal in performance given the noise (Baddeley et al., 2003; Trommershäuser et al., 2005; Chu et al., 2013). Our finding that motor noise can impair motor learning is in agreement with a recent study of reinforcement learning by Chen et al. (2017). The purpose of that study was to understand the similarities between motor reinforcement and decision-making using tasks that were designed to have similar structures. They found that the decision-making task was learned faster and suspected that this was due to the motor noise present in the motor reinforcement task. In a separate experiment, they measured the level of motor noise outside of the reinforcement learning task and showed that the level of noise was inversely related to learning. That is, participants with more noise learned slower. However, they were able to equilibrate performance by artificially adding noise into the decision-making task. This suggested, as in our experiment, that variability from noise limits the ability to learn from reinforcement feedback.
 In conclusion, we have shown that adding external noise to the movements of neurologically healthy individuals alters reinforcement learning in a motor task. Our findings suggest that high levels of noise primarily impair learning through the attribution of reinforcement to incorrect behavior. Not reinforcing correct behavior did not impair learning in our task, suggesting that it is less detrimental to the motor system. Additionally, adding noise to healthy individuals’ reaches reduced total learning to a level similar to that of a group of patients with cerebellar damage. However, healthy participants showed a faster initial learning rate. We suggest that this may result from a discrepancy between the form of noise in the present study and the source of noise in the patients with cerebellar damage. That is, the added noise in our experiment did not disrupt participants’ estimate of their actual behavior. This left a sufficient proportion variability accessible to the sensorimotor system, which may have supported a faster learning rate."
4,440047,"""A neural network model for the orbitofrontal cortex and task space acquisition during reinforcement learning""","""10.1371/journal.pcbi.1005925""",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5771635,sec002,sec006,"We describe our results in three parts. We start with using our network to model a classical reversal learning task. We take advantage of the simplicity of the task to explain the principal ideas behind the network model and why we think the network resembles the OFC. Then we show such a network may be applied to a more complex scenario, both in the task structure and in the temporal dynamics, in which the OFC has been shown to play important roles. Finally, to further illustrate the similarities between our network model and the OFC, we demonstrate how the selectivity of the neurons in the network may resemble experimental findings in the OFC during value-based decision making.
Reversal learning
 In a classical reversal learning task, the animals have to keep track of the reward contingency of two choice options that may be reversed during a test session [9, 28]. Normal animals were found to learn reversals faster and faster, which has been used as an indication of their ability of learning the structure of the task [7]. Such behavior was however found to be impaired in animals with OFC lesions and/or with lesions that contained fibers passing near the OFC [9, 29]. These animals were not able to learn reversals faster and faster when they were repeatedly tested. The learning impairments could be explained by a deficit in acquiring and representing the task structure [7].
 Our neural network model consists of a state encoding layer (SEL), which is a reservoir network. It receives three inputs and generates two outputs (Fig 1A). The three inputs from the input layer (IL) to the SEL are the two choice options A and B, together with a reward input that indicates whether the choice yields a reward or not in the current trial. The outputs units in the decision-making output layer (DML) represent choice actions A and B for the next trial. The inputs are provided concurrently and the neural activity of the SEL at the end of the trial is used to determine the SEL’s output (Fig 1B). The connections from the IL to the SEL and the connections within the SEL are fixed. Only the connection weights from the SEL to the DML are modified during the training with a reward dependent Hebbian rule, in which the weight changes are proportional to the reward prediction error and the pre- and post-synaptic neuronal activities.
 The network is able to reproduce animals’ behavior. The number of the error trials that takes for the network to achieve the performance threshold, which is set at 93% in the initial learning and at 80% in the subsequent reversals, decreases as the network goes through more and more reversals (Fig 1C). Interestingly, a learning deficit similar to that found in OFC-lesion animals is observed if we remove the reward input to the SEL (Fig 1C). As the OFC and its neighboring brain areas such as the ventromedial prefrontal cortex (vmPFC) are known to receive both the sensory inputs and reward inputs from sensory and reward circuitry in the brain [30–32], removing the reward input from our model mimics the situation where the brain has to learn without functional structures in or near the OFC.
 Neurons in the SEL, as expected from a typical reservoir network, show highly heterogeneous response patterns. Some neurons are found to encode the stimulus identity, some neurons encode reward, and others show mixed tuning (Fig 2A). A principal component analysis (PCA) based on the population activity shows that the network can distinguish all four possible task states: choice A rewarded, choice A not rewarded, choice B rewarded, and choice B not rewarded (Fig 2B and S1 Fig). The first three principal components capture 92.0% variance of the population activity.
 The ability to distinguish these states is essential for learning. To understand the task acquisition behavior exhibited by our model, we study how neurons with different selectivity contribute to the learning (Fig 2C and S2 Fig). We find that readout weights of the SEL neurons that are selective to the combination of stimulus and reward inputs (e.g. AR and BR) are mostly affected by the learning. The difference between the weights of their connections to the outputs A and B keeps evolving despite repeated reversals. In contrast, the weights of the output connections of pure stimulus-selective neurons (e.g. A and B) only wiggle around the baseline between reversals. Once the network is trained, the expected rewards from AR/BN and BR/AN inputs are exactly the opposite (S3 Fig).
 The difference between these two groups of neurons explains why our network achieves flexible learning behavior only when the reward input is available. Let us first consider the AR neurons, which are selective for the situation when choice A leads to reward. In these A-rewarded blocks, the connections between the AR neurons and the DML neuron of choice A are strengthened. When the reward contingency is reversed and now choice A leads to no reward, the connections between the AR neurons and choice A are not affected very much. That is because the group of AN and then BR neurons instead of the AR neurons are activated in the blocks when choice A is not rewarded. As the result, the connections between the AN neurons and the DML neuron of choice B are strengthened and the connections between the AN neurons and the DML neuron of choice A are weakened. When the reward contingency is flipped again, the connections between the AR neurons and the DML neuron of choice A are strengthened further. This way, the learning is never erased by the reversals, and the network learns faster and faster. In comparison, let us now consider the A neurons, which encode only the sensory inputs and are activated whenever input A is present. In the A-rewarded blocks, the connections between the A neurons and the DML neuron of choice A are strengthened. In B-rewarded blocks, the connections between the A neurons and the DML neuron of choice A are however weakened when the network chooses A and gets no reward, and the learning in the previous block is reversed. Thus, the output connections of A neurons only fluctuate around the baseline with the reversals. They do not contribute much to the learning, and the overall behavior of the network is mostly driven by neurons that are activated by the combination of reward input and sensory inputs. Removing R deactivates these neurons and leads to the structure agnostic behavior.
 The importance of the neurons that are selective for the combination of stimulus and reward inputs can be further illustrated by a simulated lesion experiment. After the network is well-trained, we stop the training and test the network’s performance with a proportion of neurons randomly removed at the time of decision (Fig 2D). The neurons that are removed are either 50 randomly chosen neurons, 50 A neurons, or 50 AR neurons. This inactivation happens only at the time of decision making, therefore the state encoding in the reservoir is not affected. The inactivation of AR neurons produces the largest impairment in the network’s performance. Compared to the network with random inactivation, the network with AR-specific inactivation cannot reach the criterion we set previously within a block in more than 50% of the blocks and makes significantly more errors to reach the criterion in the blocks that it does. Inactivation of A-selective neurons produces much smaller performance deficits.
 It is important to note that although the reinforcement learning algorithm employs the same small learning rate for both the intact network and the “OFC-lesion” network, the former only requires a few number of trials to acquire a reversal in the later stage of training, indicating the reversal behavior may not have to be slow with a small learning rate. In fact, once the network is trained, learning is no longer necessary for the reversal behavior. The network takes very few trials to adapt to reversals without learning (Fig 2E). That is because the association between input AR/BN and decision A and the association between input BR/AN and decision B have been established in the network.
 Two-stage Markov decision task
 We further test our network with a two-stage decision making task. The task is similar to the Markov decision task used previously in several human fMRI studies and used to study the model-based reinforcement learning behavior in humans [6, 33–36]. In this task, the subjects have to choose between two options A1 and A2. Their choices then lead to two intermediate outcomes B1 and B2 at different but fixed probabilities. The choice of A1 more likely leads to B1, and the choice of A2 is more likely followed by B2. Importantly, the final reward is contingent only on these intermediate outcomes, and the contingency is reversed across blocks (Fig 3A). Thus, the probability of getting a reward is higher for B1 in one block and becomes lower in the next block. The probabilistic association between the initial choices and the intermediate outcomes never changes. The subjects are not informed of the structure of the task, and they have to figure out the best option by tracking not only the reward outcomes but also the intermediate outcomes.
 We keep our network model mostly the same as in the previous task. Here, we have two additional input units that reflect the intermediate outcomes (Fig 3B). To demonstrate our network model’s capability of encoding sequential events, the input units are activated sequentially in our simulations as they are in the real experiment (Fig 3C). We also add a non-reward input unit whose activity is set to 1 when a reward is not obtained at the end of a trial. The additional non-reward input facilitates learning but does not change the results qualitatively.
 For a simple temporal difference learning strategy without using any knowledge of task structure, the probability of repeating the previous choice only depends on its reward outcome. The probability of repeating the previous choice is higher when a reward is obtained than when no reward is obtained. The intermediate outcome is ignored. However, this is no longer the case when the task structure is taken into account. For example, consider the situation when the subject initially chooses A1, the intermediate outcome happens to be B2, and a reward is obtained. If the subject understands B2 is an unlikely outcome of choice A1 (rare), but a likely outcome of choice A2 (common), a reward obtained after the rare event B2 should actually motivate the subject to switch from the previous choice A1 and choose A2 the next time. The subject should always choose the option that is more likely to lead to the intermediate outcome that is currently associated with the better reward.
 To quantify the learning behavior, we first evaluate the impact of the previous trial’s outcome on the current trial. We classify all trial outcomes into four categories: common-rewarded (CR), common-unrewarded (CN), rare-rewarded (RR) and rare-unrewarded (RN). Here, common and rare indicate whether the intermediate outcome is the more likely outcome of the chosen option or not. Glascher et al [6] showed that the model based learning led to a higher probability of repeating the previous choice in the CR and RN conditions. This is also what we observe in our network model’s behavior (Fig 4A).
 To illustrate how the network acquires the task structure, we define the task-structure index, which represents the tendency of employing task structure information (see the Method). The task-structure index grows larger as the training goes on (Fig 4B). It indicates that the network learns the structure of the task gradually and transits to a more efficient behavior from an initially task-agnostic behavior.
 Similar to our findings in the first task, the network without the reward input in the SEL behaves in a task-agnostic manner. It does not show the transition that indicates the learning of the task structure (Fig 4B). We further quantify the contribution of task structure information to the network behavior using a model fitting procedure previously described by Glascher et al. [6], and the network without the reward input shows a significantly smaller weight for the usage of task structure, suggesting it is worse at picking up the task structure (Fig 4C and S4 Fig). When the network time constant is sufficiently long, the task-structure dependent behavior is not because the intermediate outcomes occur after the first stage outcomes so that the former having a stronger representation in the network at the time of decision (S5 Fig).
 Again, a PCA on the SEL population activity shows that the SEL distinguishes different task states (Fig 4D). The first three principal components explain 83.97% variance of the population activity. Because the structure of the task in which the contingency between the first stage options and the intermediate outcomes is fixed, the network only needs to find out the current reward contingency of the intermediate outcomes. We found that the learning picks out the most relevant neurons that encode the contingency between the intermediate outcomes and the reward outcomes (B1R, B2R, etc.). Their connection weights to the DML neurons show better and better differentiation of the two choices throughout the training (Fig 4E). In contrast, the connection weights of the neurons that encode the association between the first stage options and the reward outcomes (A1R, A2R, etc.) are less differentiated.
 These results suggest that the network acquires the task structure. It understands that the contingency between intermediate outcomes and reward outcomes is the key to the performance. Thus, its choice only depends on the interaction between the intermediate outcome and the reward outcome of the last trial, but not on the other factors (Fig 4F and 4G). The network behavior is similar to the Reward-as-cue agent described by Akam et al. [37].
 Value representation by the OFC
 Previous electrophysiology studies have shown that OFC neurons encode value during economic choices [11, 13]. In a series of studies carried out by Padoa-Schioppa and his colleagues, monkeys were required to make choices between two types of juice in different amounts. The monkeys’ choices depended on both their juice preference and the reward magnitude. Recordings in the OFC revealed multiple classes of neurons encoding a variety of information, including the value of individual offers (offer value), the value of the chosen option (chosen value), and the identity of the chosen option (chosen identity) [38, 39].
 Here we show that our network model may explain this apparent heterogeneous value encoding in the OFC. We model the two-alternative economic choice task by providing two inputs to the SEL, representing the reward magnitude of each option with range adaption (Fig 5A). The input dynamics are similar to that of the sensory neurons [40]. The network model reproduces the choice behavior of monkeys (Fig 5C)[11].
 Then we study the selectivity of the SEL neurons. Just as in the previous experimental findings in the OFC, we find not only neurons that encode the value of each option (offer value neurons, middle panel in Fig 6A), but also neurons that encode the value of the chosen option (chosen value neurons, left panel in Fig 6A). Furthermore, a proportion of neurons show the selectivity for the choice as previously reported (chosen identity neurons, right panel in Fig 6A). We classify the neurons in the reservoir network into 10 categories as described in Padoa-Schioppa and Assad [11]. Interestingly, we are able to find neurons in the reservoir in 9 of the 10 previous described categories (Fig 6B and 6C). The only missing category (neurons encoding other/chosen value) was also rarely found in the experimental data. Although the proportions of neurons encoding each category are not an exact copy of the experimental data, but the similarity is apparent. This is surprising given that we do not tune the internal connections of the SEL to the task. The results are robust across different input connection gains, noise levels in the SEL, and dynamics of the input profiles (S6 Fig). The heterogeneity that is naturally expected from a reservoir network takes much more effort to explain with recurrent network models that have a well-defined structure [40, 41].","So far, we have shown that a simple reservoir-based network model may acquire task structures. The more interesting question is that why the network is capable of doing so and how this network model may help us to understand the functions of the OFC.
Encoding of the task space
 We place a reservoir network as the centerpiece of our model. Reservoir networks are large, distributed, nonlinear dynamical recurrent neural networks with fixed weights. Because of recurrent networks’ complicated dynamics, they are especially useful in modeling temporal sequences including languages [42, 43]. Neurons in reservoir networks exhibit mixed selectivity that maps inputs into a high dimensional space. Such selectivity has been shown to be crucial in complex cognitive tasks, and experimental works have provided evidence that neurons in the prefrontal cortex exhibit mixed selectivity [44–46]. In our model, the reservoir network encodes the combinations of inputs that constitute the task state space. States are encoded by the activities of the reservoir neurons, and the learned action values are represented by the weights of the readout connections.
 There are several reasons why we choose reservoir networks to construct our model. First reason is that we would like to pair our network model with reinforcement learning. Reservoir networks have fixed internal connections; the training occurs only at the readout. The number of parameters for training is thus much smaller, which could be important for efficient reinforcement learning. Generality is another benefit offered by reservoir networks. Because the internal connections are fixed, we may use the same network to solve a different problem by just training a different readout. The reservoir can serve as a general-purpose task state representation network layer. Lastly, our results as well as several other studies show that neurons in reservoir networks–even with untrained connections weights–show properties similar to that observed in the real brain [24, 25, 47], suggesting training within the network for specific tasks may not play a role as important as previously thought.
 The fact that the internal connections are fixed in a reservoir network means that the selectivity of the reservoir neurons is also fixed. This may seem at odds with the experimental findings of many OFC neurons shifting their encodings rapidly during reversals [48]. However, these observations may be interpreted differently when we take into account rewards. The neurons that were found to have different responses during reversals might in fact encode a combination of sensory events and rewards. On the other hand, there is evidence that OFC neurons with inflexible encodings during reversals might be more important for flexible behavior [49].
 The choice of a reservoir network as the center piece of task event encoding may appear questionable to some. We do not train the network to learn task event sequences. Instead, we use the dynamic patterns elicited by task event sequences as bases for learning. This approach has obvious weaknesses. One is that the chaotic nature of network dynamics limits how well the task states can be encoded in the network. We have illustrated the network works well for relatively simple tasks. However, when we consider tasks that have many stages or many events, the combination of possible states grows quickly and may exceed the capacity of the network. The fact that we do not train the internal network connections does not help in this regard. However, the purpose of our network model is not to solve very complicated tasks. Instead, we would like to argue this is a more biologically-realistic model than many other recurrent networks. First, it does not depend on supervised learning to learn task event sequences [47, 50]. Second, although the network performance may appear to be limited by task complexity, the real brain, however, also has limited capacity in learning multi-stage tasks [37]. Lastly, we show that a reservoir network can describe OFC neuronal responses during value-based decision making. Several other studies have also shown that reservoir networks may be a useful model of the prefrontal cortex [24, 25].
 Reward input to the reservoir
 One key observation is that reward events must also be provided as inputs to the reservoir layer for the network model to perform well. Including reward events allows the network to establish associations between sensory stimuli and rewards, thus facilitates task structure acquisition. Although reward modulates neural activities almost everywhere in the cortex, the OFC plays a central role in establishing the association between sensory stimuli and rewards [9, 48, 51, 52]. Anatomically, The OFC receives visual sensory inputs from inferior temporal and perirhinal cortex, as well as reward information from the brain areas in the reward circuitry, including the amygdala and ventral striatum, allowing it to have the information for establishing the association between visual information and reward [30–32]. Removing the reward input to the reservoir mimics the situation when animals cannot rely on such an association to learn tasks. In this case, the reservoir is still perfectly functional in terms of encoding task events other than rewards. This is similar to the situation when animals have to depend on their other memory structures in the brain–such as hippocampus or other medial temporal lobe structures–for learning. Consistent with this idea, it has been shown both the OFC and the ventral striatum are important for model-based RL [53]. The importance of the reward input to the reservoir explains the key role that the OFC plays in RL.
 Several recent studies reported that selective lesions in the OFC did not reproduce the behavior deficits in reversal learning previously seen if the fibers passing through or near the OFC were spared [29]. Since these fibers probably carry the reward information from the midbrain areas, these results do not undermine the importance of reward inputs. Presumably, when the lesion is limited to the OFC, the projections that carrying the reward information are still available to or might even be redirected to other neighboring prefrontal structures, including ventromedial prefrontal cortex, which might take over the role of the OFC and contribute to the learning in animals with selective OFC lesions.
 Model-based reinforcement learning
 The acquisition of task structure is a prerequisite for model-based learning. Therefore, it is interesting to ask whether our network model is able to achieve model-based learning. The two-stage task that we model has been used in human literature to study model-based learning [5, 6, 33–36]. Our model, although exhibiting behavior similar to human subjects, can be categorized as the Reward-as-cue agent that was described and categorized as a form of model-free reinforcement learning agent by Akam et al. [37]. Yet, with reward incorporated as part of the task state space, goal-directed behavior can be achieved by searching in the state space for a task event sequence that ends with the desired goal and associating the sequence with appropriate actions. Thus, our network could in theory support model-based learning by providing the task structure to the downstream network layers.
 Extending the network
 The performance of our network depends on several factors. First, it is important that reservoir should be able to distinguish between different task states. The number of possible task states may be only 4 or 8 as in our examples, or may be impossibly large even if the number of inputs increases only modestly. The latter is due to the infamous combinatorial explosion problem. One may alleviate the problem by introducing learning in the reservoir to enhance the representation of relevant stimulus combinations and weed out irrelevant ones. A recent study showed that the selectivity pattern in the prefrontal neurons may be better explained by a random network with Hebbian learning [54]. Second, the dynamics of the reservoir should allow information to be maintained long enough in a decipherable form until the decision is made. The recent developed gated recurrent neural networks may provide a solution with units that may maintain information for long periods [55]. Third, the model exhibits substantial variability between runs, suggesting the initialization may impact its performance. Further investigation is needed to make the model more robust. Last, we show that a reinforcement learning algorithm is capable of solving the relatively simple tasks in this study. However, it has been shown that reinforcement learning is in general not very efficient for extracting information from reservoir networks. Especially, when the task demands the information to be held for an extended period, for example, across different trials, the current learning algorithm fails to extract such relevant information from the reservoir. A possible solution is to introduce additional layers to help with the readout [25].
 Testable predictions
 Our model makes several testable predictions. First, because of the reservoir structure, the inputs from the same source should be represented evenly in the network. For example, in a visual task, different visual stimuli should be represented at roughly the same strength in the OFC, even if their task relevance may be drastically different. Second, we should be able to find neurons encoding all relevant task parameters in the network, even when a particular combination of task parameters is never experienced by the brain. Third, reducing the number of inputs may make the network to be more efficient in certain tasks. This may seem counter-intuitive. But removing inputs reduces the number of states that the network has to encode, thus improves learning efficiency for tasks that do not require those additional states. For example, if we remove the reward input to the SEL, which is essential for learning tasks with volatile rewards, the network should however be more efficient at learning tasks in a more stable environment. Indeed, animals with OFC lesions were found to perform better than control animals when reward history was not important [56].
 Summary
 Our network does not intend to be a complete model of how the OFC works. Instead of creating a complete neural network solution of reinforcement learning or the OFC, which is improbable at the moment, we are aiming at the modest goal of providing a proof of concept that approaches the critical problem of how the brain acquires the task structure with a biologically realistic neural network model. By demonstrating the network’s similarity to the experimental findings in the OFC, our study opens up new possibilities in future investigation."
5,2347850,"""Reinforcement and Reversal Learning in First-Episode      Psychosis""","""10.1093/schbul/sbn078""",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2518639,__sec6,__sec10,"Group demographics and diagnostic information:
The psychosis group and the control group were matched on age, gender, and NARTestimated verbal IQ (Table 1). When all available information was utilized to apply diagnostic categories after 12months, 81 patients were classified as schizophrenia-spectrum psychosis and 31 as affective psychosis, with missing information on 6 cases. PANSS scores from initial assessment were available on 78 patients.
Learning analysis: stages passed and failed
 In the control group, 1 participant failed at the IDS stage, 2 failed at the IDR stage, 5 failed at the EDS stage, and 1 failed at the final reversal stage (see figure 2). In patients, 2 failed at
the compound discrimination stage, 1 at the compound reversal stage, 27 failed at the EDS stage, and 11 failed at the final reversal stage. Thus, in terms of stage failures, significantly more patients failed the EDS stage (? = 16.5, P < .001), and the final reversal
stage (? = 9.6, P < .01), than controls.
 Learning analysis: error analysis
 For a more sensitive analysis, we examined error scores at each stage. We compared all patients (n = 119) vs all controls (n = 107)
on initial discrimination learning (figure 3). By examining the number of errors using the Mann-Whitney U test, we confirmed that psychosis patients made more errors than controls during initial discrimination learning (z = 2.5, P = .01); in contrast, there were
no differences between patients and controls on initial reversal learning, compound discrimination learning, ID set shifting, compound reversal, or IDR. However, there were deficits in ED set shifting (z =
?5.1, P < .001) and final reversal stages (z = ?3.7, P < .001). Next, we examined the total number of reversal errors over the course of the experiment in participants who attempted all stages of the IDED test: ie, those who completed at least the EDS stage and so could attempt the final reversal stage (99 controls and 89 patients, see table 2 and figure 4). Although some psychosis patients showed good performance, as a group patients made more total reversal errors than controls (z = ?2.4, P = .02).
 We next examined whether, within the patient group, total reversal errors could be explained, at least in part, by SD errors. Utilizing Poisson regression, we found that SD errors did not predict total reversal errors (z = 0.7, P = .5). In contrast, SD errors were a significant predictor of EDS errors (z = 5.4, P < .001).
 Having established the differences between first-episode psychosis patients and controls, we proceeded to examine whether patients with schizophrenia-spectrum psychoses differed from patients with affective psychosis. Patients with affective psychosis made fewer ID shifting errors than patients with schizophrenia-spectrum psychosis (z = ?2.3, P = .026), but there were no differences in any other stages of the ID/ED or in total reversal errors (z = ?0.539, P = .6; table 3).

 Finally, we examined whether symptom scores correlated with performance on simple and reversal trials in the 56 first-episode psychosis patients who completed at least the ED stage of the ID/ED test and for whom PANSS scores were available. Poisson regression revealed that total reversal errors were predicted by negative symptoms (z = 3.72, P < .001), but not positive symptoms (z = ?0.6, P = .6), which is consistent with results from a correlational analysis: total reversal errors correlated significantly with negative symptoms (Spearman ? = 0.3, P = .02) but not with positive symptoms (Spearman ? = 0.2, P = .20). There was no association between SD errors and psychopathology either on correlational analysis (positive symptoms Spearman ? = 0.06, P = .6; negative symptoms Spearman ? = 0.02, P = .98) or on Poisson regression (positive symptoms z = 0.52, P = .6, negative symptoms z = 0.35, P = .7).","As expected, we found that psychosis patients had deficits in ED set shifting. This deficit has been previously documented in chronic schizophrenia19,28–30 and in first-episode psychosis,23 although some
studies in first-episode psychosis suggest that there may either be no deficit in this domain31 or that the deficit may be slight.22,32 Given that a number of previous studies have investigated ED set shifting in schizophrenia and early psychosis, here we focus our
interpretation on the results concerning simple reinforcement learning and reversal learning.
 Elliott et al18 and Pantelis et al28 previously demonstrated reversal learning deficits in chronic schizophrenia. Both these groups extracted reversal learning performance from a version of the ID/ED test: the same approach that we employ in this study. Waltz and Gold20 showed profound reversal deficits in 34 patients with chronic schizophrenia using a different method; they employed a probabilistic reversal task adapted from Robbins and colleagues.33,34 Our results demonstrate that reversal learning deficits are also present in many patients near the time of initial presentation to psychiatric services. We note that these deficits were not universal however, and many patients performed at comparable levels to controls (see figure 4).

 In contrast to Waltz and Gold,20 who argued that patients performed adequately on rule acquisition, we were able to detect subtle abnormalities in simple reinforcement learning (SD learning), possibly because of our large sample size. Our study thus provides further support to long-held contentions that there are reinforcement learning abnormalities in psychosis. Discrimination learning has been shown to be impaired by caudate tail lesions35; previous data supports caudate dysfunction in psychosis.13,36,37 Specifically, the tail of the caudate is itself connected to the medial temporal lobe, an area that is strongly implicated in the pathogenesis of psychotic illness38 as well as playing a role in discrimination learning. Research in rhesus monkeys has shown that lesions to the medial temporal lobe rhinal cortex, and to the inferior temporal cortex, result in mild and severe deficits, respectively, in discrimination learning, possibly through an inferior temporal-frontal-thalamic network.39–43 Thus, the SD deficit we note is also consistent with previous evidence for disruptedfrontotemporal connectivity in psychosis.44,45

 Patients with affective and nonaffective psychosis did not differ significantly in reversal learning errors (or indeed in EDS errors). Previous research has identified deficits in reversal learning to be present in bipolar mania,46 consistent with other recent research implicating orbitofrontal cortex dysfunction in mania (including manic psychosis), such as the presence of impairment on the Iowa Gambling Test.47 Interestingly, reversal learning is intact in euthymic bipolar disorder without a history of psychosis, suggesting a state-dependent deficit in nonpsychotic bipolar disorder.48

 Lesion studies in rodents and nonhuman primates have demonstrated a key role for the orbitofrontal cotrex and ventral striatum in reversal learning.49–53 Moreover, this evidence is corroborated from human functional imaging studies33,54,55 and from studies of human patients with orbitofrontal lesions.56,57 These regions are critical for motivational and goal-directed processing10; thus, the present study suggests that there is dysfunction of orbitofrontal/ventral striatal circuitry in psychosis. This contention is consistent with the findings of our correlational analysis in patients, which demonstrates that the greater the reversal impairment, the more severe the negative symptoms (ie, the greater the impairment in motivational and goal-directed behavior). We note that the specificity of this correlation should be viewed with caution because the magnitude of the significant correlation coefficient between negative symptoms and reversal errors (? = 0.3) differed only slightly from the nonsignificant correlation between positive symptoms and reversal errors (? = 0.2).

 Interestingly, we found that the patient group made few errors at the compound discrimination stage, which is in contrast with recent results reported by Jazbec et al.29 They studied 34 patients with chronic schizophrenia and found pronounced deficits in compound discrimination. It is possible that this process may deteriorate with disease progression, though longitudinal research will be required to examine this conjecture.

 Our study does have a number of limitations. Although we found deficits in SD learning, the ID/ED test is not solely or primarily a test of this cognitive domain. Given that the test starts with SD learning, it is conceivable that some psychosis patients might have had trouble adjusting to the task environment in general, leading to an apparent specific deficit in this domain. In addition, there was only  small range in scores in SD learning, which limits the power of correlation and regression analyses to detect associations with clinical variables. For this reason, the failure to detect association between SD errors and clinical variables should not be overinterpreted. SD learning, and its association with clinical variables, merits further investigation in early psychosis in other cognitive paradigms that focus on SD learning in more detail.

 Another limitation of the current study is that the majority of patients were taking second-generation antipsychotic medications. Such medications act on dopaminergic and serotonergic systems, and ascending serotonin and dopamine neurotransmitter systems are known to play a modulatory role in reinforcement learning processes.51,52,58 There are, however, a number of reasons why our current results are unlikely to be secondary to medication effects. First, we note that in a recent functional magnetic resonance imaging study in healthy volunteers, a low dose of the dopamine D2/D3 receptor ntagonist, sulpiride, did not modulate brain activations during reversal learning or impair behavioral reversal performance.59 Secondly, we observed a correlation between the level of negative symptoms and reversal errors, consistent with the theory that both these measures are secondary to one underlying pathological process. Finally, we have, in recent studies, demonstrated behavioral and physiological abnormalities during tests of reinforcement learning and motivational modulations in unmedicated first-episode psychosis patients.13,15 Future studies should examine reversal learning in unmedicated patients with psychosis, its relation to symptoms, and the extent to which reinforcement and reversal learning deficits can be modulated by pharmacological interventions. The relationship between reinforcement learning and reversal deficits and functional impairments also merits investigation."
6,1816166,"""Research on predicting 2D-HP protein folding using reinforcement learning with full state space""","""10.1186/s12859-019-3259-6""",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6929271,Sec14,Sec18,"Comparative experiment between rigid criterion and flexible criterion
According to two different reward settings of rigid and flexible criteria, six paper dataset sequences and ten sequences in the classic Uniref50 database are selected as experimental objects. The known information and test energy information were shown in Table 2. The parameters were set as follows: step-size parameter ??=?0.01, exploration probability ??=?0.5, and learning parameter ??=?0.9.
 In Table 3, the first four sequences were chosen to compare the performance of reinforcement learning with rigid and flexible criteria. In order to avoid contingency, the rigid and flexible criteria experiments were repeated five times. The number of training iterations per round was set to 5 million, and the test was performed once every 10,000 times. In training process, the number of episodes required to converge to the lowest energy was counted as shown in Table ?Table33.
 Combination of Tables 2 and ?and33 showed that reinforcement learning with rigid criterion can stably find the lowest energy conformation faster than reinforcement learning with flexible criterion. For the shorter sequences (1 and 2), the number of training episodes required for agent to achieve convergence conformation by flexible criterion was greater than rigid criterion. Reinforcement learning with rigid criterion sampled an average 30,000 and 210,000 episodes to achieve the robust lowest energy conformation, which was 50 and 63% less than 60,000 and 570,000 episodes required by reinforcement learning with rigid criterion. For the longer sequences (3 and 4), reinforcement learning with flexible criterion could not find the lowest energy conformation. One possible reason was that, although flexibility criterion gave a negative reward (or penalty) for states that caused repetition, the states still had some positive Q values, and the Q values of these repeated states in rigid criterion still had an initial value of 0. Therefore, the probability of the repeated states in flexibility criterion being selected was greater than rigid criterion. And as the length of the sequence increased, the number of states that caused repetition in the full state space was also greater, and it was more difficult to find the lowest energy structure.
 Comparative experiment with greedy algorithm
 Reinforcement learning with full states using rigid criterion was compared with greedy algorithm. The experimental objects were the twelve sequences in the Uniref50 data set. Similarly, in order to avoid accidentality, two methods were trained for five rounds, and the number of training iterations per round was set to 5 million, and the samples were performed once every 10,000 times. We counted the number of times the lowest energy was obtained in the last 100 samples (Table 4).
 It can be seen from Table ?Table22 that reinforcement learning with full states using rigid criterion can find the lowest energy for all 16 sequences, but the greedy algorithm can only find 13 of them. From Table ?Table4,4, the training process with 10 sequences was far superior to the greedy algorithm for the above 12 sequences. And the total number of times that the lowest energy was found was 300, which was greater than 205 for the greedy algorithm.
 Comparative experiment with the reinforcement learning with partial states
 Reinforcement learning with full states using the rigid criterion was compared with reinforcement learning with partial states. The experimental objects and experimental settings were the same for greedy algorithm above.
 In the reinforcement learning with partial states, for an HP sequence of length n, its state space S consists of 1?+?4 (n-1) states. Apart from the first amino acid that had only one state, each of the other amino acids had four different actions (up, down, left, and right) to transfer to four different states, so the number of the entire state set was expressed as 1?+?4 (n-1), so S?=?{s1,?s2,?…,?s1?+?4(n???1)}. For example, the state of the first amino acid is s1. In this state, the four actions of up, down, left, and right were respectively transferred to states s2, s3, s4, s5, which were all possible states of the second amino acid. On the same basis, the four actions of up, down, left and right respectively transferred to the states s6, s7, s8 and s9, which were all possible states of the third amino acid, and so on, to find all the states of the subsequent amino acids.
 In Table ?Table2,2, there were 8 sequences that cannot converge to the lowest energy conformations by the reinforcement learning with partial states, while reinforcement learning with full states successfully folded all sequences to the lowest energy conformations. Table ?Table44 showed that in the last 100 episodes, reinforcement learning with full states hits the lowest energy an average five times, which was 40 and 100% higher than the three and zero times hit by the greedy algorithm and reinforcement learning with partial states, respectively. Reinforcement learning with full states achieved lower energy structures on ten out of twelve sequences than the greedy algorithm.","Analysis of time complexity and space complexity
In this algorithm, for one sequence, many iterations of training are required to get its lowest energy. Therefore, the time complexity of the algorithm is determined by the length of the amino acid sequence (N) and the number of training iterations (I), that is, the time complexity is O(N?×?I). The time complexity of the ant colony algorithm for solving HP two-dimensional structure prediction is O(N?×?(N???1)?×?M?×?I/2), where N is the sequence length, I is the number of iterations, and M is the number of ants. The time complexity of particle swarm optimization is O(N?×?I?×?M), where N is the sequence length, I is the number of iterations, and M is the number of particles. Obviously, the time complexity of the method in this paper is the smallest of the three methods, and the larger the sequence length, the more prominent the time advantage.
 The space complexity is composed of state-transfer function matrix and state-action value matrix. The rows of both matrices represent states, and the columns all represent actions. The number of rows in new state-transfer function matrix is 3N?1?12 and the number of columns is 3. The number of rows in state-action value matrix is 3N?12 and the number of columns is 3. So the space complexity is O3N?1?12×3+3N?12×3.
 Case study
 Sequence 12 is a zinc finger protein 528 (fragment), which is a transcription factor with a finger-like domain and plays an important role in gene regulation. Taking sequence 12 as an example, a series of optimized structures with the lowest energy obtained by the method of this paper under rigid criterion are given, as shown in Fig. 2a-c. The results of the last 100 samples of the method and the greedy algorithm and reinforcement learning with partial states in the training exploration process are given, as shown in Fig. 3a-c. The greedy algorithm itself cannot converge, and the convergence of reinforcement learning with full and partial states in the test process is shown in Fig. 4a, b.
 For reinforcement learning with full states, the agent can be trained to select the better action to obtain a lower energy structure after training for several million times, and then guarantee that the structure obtained after convergence is the optimal structure, and it can be considered that the training effect of reinforcement learning with full states is stable. However, the greedy algorithm is not ideal for training. Only several structures with the lowest energy are trained occasionally, and the accuracy of the lowest energy structure cannot be guaranteed. As a whole, reinforcement learning with full states is better than the greedy algorithm. This is because, for reinforcement learning, the agent can choose better actions based on the previous interaction with the environment during the exploration process. Therefore, as the number of training increases, the agent can select the optimal action more quickly and accurately. Also, because of the setting of the reward function, the agent is more concerned about the overall situation without being trapped in a local optimum. The calculation of each plot in the greedy algorithm is independent, and the previous experience does not help the development of the current plot. As a result, the calculation amount becomes larger and the correct structure cannot be stably obtained.
 From the testing process, it can be found that reinforcement learning with full states can maintain the lowest energy and achieve stable convergence after reaching the minimum energy. In contrast, reinforcement learning with partial states has fluctuations, cannot be stably maintained, and cannot reach the convergence state. This is because each state in the full state space is uniquely determined and can only be transferred by a unique state-action pair, and the process has Markov properties. However, the state in the partial state space can be transferred by different state-action pairs, which has partial uncertainty.
 Full state space compares to partial state space
 The full state space and the partial state space are two different descriptions of the state space in the 2D-HP model under reinforcement learning framework. The same point of the full and partial state spaces is that different states corresponding to each amino acid are set in advance, but they differ in the rules of the state setting. For the full state space, the number of states of subsequent amino acids is always three times the number of previous amino acid states. The state of the subsequent amino acid is obtained by a specific action of the previous amino acid in a specific state. That is to say, each state is transferred by a certain state-action pair, and the whole process has Markov properties. For the partial state space, the number of states for each amino acid except the first amino acid is four. The four states of the subsequent amino acid can be transferred from the four states of the previous amino acid through four different actions, and the whole process does not have Markov properties. The advantage of the full state space is that it can accurately find the lowest energy of the sequence and stabilize the convergence. The disadvantage is that the state space dimension is too high and the memory requirement is high, and the sequence with long length cannot be calculated. The advantage of partial state space is that the required state space is small, and it is possible to calculate a sequence with a long length. The disadvantage is that it cannot converge and cannot find the lowest energy of the sequence.
 Function approximation is especially suitable for solving problems with large state space. The method described above for pre-setting the state-action value matrix and updating the state-action value matrix during the training process takes up a large amount of memory. Function approximation can be used to map the state-action value matrix to an approximation function (such as a parameter approximation function). Updating the parameter values with experimental data during the training process is equivalent to updating the state-action value, and finally a suitable approximation function is obtained. It can save memory space and solve the problem of sequence length limitation."
7,1786347,"""Differential effects of reward and punishment in decision making under uncertainty: a computational study""","""10.3389/fnins.2014.00030""",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3930867,s2,s3,"2.1. Human behavior
In this task, participants were repeatedly shown a series of colored triangles, red or blue, on a screen, and they were required to press one of two buttons in response to each triangle. They received monetary reward or punishment according to whether this response was correct or wrong. To analyze the behavior, we encoded each response according to underlying behavioral types, where type 1 behavior was to press button one when a red triangle was shown, and button two for blue; type 2 was the opposite of type 1. Over groups of consecutive trials, one response type was rewarded on the majority of trials, controlled by two levels of probability (73% and 83%) known as feedback validity (FV), giving expected uncertainty in the environment. For a set of trials in which response type 1 was rewarded the most, we say that the underlying experimental rule was rule 1, likewise for type 2 responses and rule 2. Stimuli were presented in blocks of 120 trials having constant FV and one of two levels of volatility. High volatility blocks had a rule switch every 30 trials, stable blocks had the same underlying rule for all 120 trials. The participants were not given any information about the generation of rewards or the split of the task into blocks. This data has been examined previously, without considering the fit of different learning models to the behavior (Bland and Schaefer, 2011). Further details of the study can be found in the Materials and Methods section or Bland and Schaefer (2011).
 Figure ?Figure11 illustrates the study by showing responses made and feedback given for the first 120 trials of the study for four individual participants. Participants were most likely to switch from one response type to the other after negative feedback, that is a loss of points.
 If the underlying rule can be identified from the pattern of rewards, but the result on individual trials cannot be predicted due to the randomness in reward generation, then to achieve the greatest rewards one should always respond according to the underlying rule, we call this maximizing. So if one knows that type 1 responses are rewarded mostly, then one should make a type 1 response every trial and ignore occasional losses. This does not consider how to identify which response is being rewarded most, or how to identify a rule switch. We quantify participants' behavior by calculating the percentage of trials in which each participant's response is of the type which is associated with the underlying experimental rule. Individual differences in responding to the task gave a range of maximizing behavior from 62% to 89% (mean 74.5%, SD 6%). The overall average feedback validity used in the experiment was 78%, so the average maximizing was just below that. This is in line with many other studies which find that in probabilistic tasks, the average frequency of each response matches the frequency of reward allocation, known as probability matching (see e.g., Vulkan, 2000; Shanks et al., 2002).
 2.2. Computational modeling
 Reinforcement learning models are based on standard reinforcement learning techniques of making trial by trial adjustments to the predicted value of a particular action, which is a prediction of how much reward is expected from that action. This predicted value is adjusted according to the outcome and a learning rate which controls how much influence the latest outcome has in changing the predicted value of an action. We considered three alternative reinforcement learning models, two of these, standard reinforcement learning model (RL) and win loss modified reinforcement learning model (WL), assume that the environment is fully coupled. This assumes that responses can be viewed in terms of the two response types described above, with the red triangle requiring the opposite response from blue, allowing us to ignore the actual color presented on each trial, and also that exactly one of the responses will be rewarded on each trial. This means that, if feedback shows that one response type is incorrect, then the other response type would have been correct on that trial and vice versa, so regardless of which response is made, feedback lets you know how each response type would have fared.
 The assumption that the participants expected the environment to be coupled was motivated by the instructions given to participants, but to validate this assumption, we tested an uncoupled reinforcement learning model (UNC) which considers the colors seen and the button presses to be independent of each other and a separate predicted value is maintained for each combination of button and color. Given the assumption of independence, feedback after making a response does not give you any information about the result of pressing the other button or seeing the other color.
 In our UNC and RL models, one learning rate is assumed for each participant. Our win loss modified reinforcement learning model (WL) allows wins and losses to have different influences on learning by allocating two learning rates to each participant, treating trials following a loss or a win separately.
 Our Bayesian models are based on hidden Markov models which assume that rewards are governed by a hidden environmental state which cannot be directly observed but can be inferred. In our simple hidden Markov model (HMM), as with Hampton et al. (2006), the hidden state has two possible values, which are equivalent to the two experimental rules. Given the structure of the HMM and the outcomes, combination of response type made and feedback received, Bayesian reasoning is used to determine a probability of reward for each response type. Two sets of probabilities define the structure of the HMM, these are taken to be constant parameters for each participant. These probabilities control the chance of a rule switch and the relation between the hidden state and the reward.
 Following the work of Behrens et al. (2007), we created a Bayesian model (VOL) which assumes an additional level of structure to the environment, volatility, or how quickly the environment is changing. As with Behrens et al. (2007), a hidden state relates directly to the probability of reward for a particular response, in our case representing the probability of response type 2 being rewarded, without the assumption in the HMM of only two states. This gives a flexible model which can respond to any change in state including changes in feedback validity. Like Behrens et al. (2007) we have assumed that the process for determining the current state and volatility does not vary between participants.
 In all models, following the calculation of a belief or probability, we apply softmax action selection to determine the probability of making each action on each trial. Softmax action selection assumes that the chosen action depends on the difference between the values associated with each action and on a temperature parameter controlling the amount of randomization of responses on top of underlying beliefs. A low temperature increases the probability of choosing the higher valued action and a high temperature makes the probability of each action more similar. For the RL and UNC models we fit one temperature parameter to each participant's behavior, for the other models we fit two temperature parameters, differentiating trials following wins and losses.
 Given a set of parameters and a model, we calculate a probability for each action on each trial for the outcomes received by the participant. The natural logarithms of the probabilities for the actions actually taken are summed for each participant. For each model, parameters are fit to each participant's behavior by searching possible values to maximize the likelihood of the parameters over all trials, for more details see Materials and Methods.
 2.3. Comparing model fit
 Models with more parameters should be able to show a closer fit to the data so it is customary to penalize models with more free parameters which have been fit to participants' behavior (Mars et al., 2012). To do this, we compare the five models described above by calculating the commonly used Bayesian Information Criterion (BIC) for each model (Lewandowsky and Farrell, 2011).
 As a better model has a lower BIC value, Table ?Table11 shows that the WL model gave the best overall fit to the data. We also examined the BIC for each model calculated separately for each participant. The UNC model was the worst fit to behavior compared to the other models for all participants. The best fit model was the WL model for 24 of the 30 participants, for four participants the best fit was the RL model and for two the HMM. Of the 24 participants for whom the WL model was the best fit, 23 had HMM as the next best fit. The differences in the BIC between the WL model and each other model were statistically significant, p < 0.001 in each case, t(29) = 5.05, 4.25, and 7.48 for comparison of WL to RL, HMM, and VOL models, respectively.
 As described by Lewandowsky and Farrell (2011), we calculated Bayes factors for the difference between the HMM and WL models, we did this using the calculated BIC for each participant. Bayes factors can give an indication of the size of an effect, Lewandowsky and Farrell (2011) report previously proposed guidelines that a Bayes factor above 10 implies strong evidence for one model over the other, and between 3 and 10 implies moderate evidence. Figure ?Figure22 shows the Bayes factors for the WL compared to HMM for all participants.
 The HMM and WL models fit the participants' behavior better than the other models so we now focus on these two models. Having used all trials to determine the best fit parameters for each participant and model, we could now calculate a trial by trial probability of making a type 2 response. Figure ?Figure33 shows these probabilities for the HMM and WL models for three participants for the first 240 trials of the study. In general, the probabilities match closely, but where there is a difference, the WL model is usually closer to the actual response made by the participant. This follows from the use of log likelihood to find the best model, Figure ?Figure33 is an illustration.
 As the WL model was the best fit to behavior, we look at the values of the fit parameters. Figure ?Figure44 shows the fit parameters for the WL model, the temperature was significantly higher after a loss than a win, t(29) = 5.61, p < 0.0001 with means of 0.87 and 0.35 after a loss and a win, respectively. According to this model, participants were more likely to randomize their responses after a loss. The fit learning rates were significantly higher after a win than after a loss, means of 0.77 and 0.52, respectively, t(29) = 4.52, p < 0.0001. A lower learning rate after a loss implies that losses are treated less strongly, which allows behavior to respond more slowly to occasional negative feedback and so take advantage of stable periods by not switching to the opposite response type when occasionally losing points when using the most likely response. Finally, we broke down the BIC scores for the WL model between a win and a loss. We find some indications that the WL model fits best after a loss, but this is at the edge of significance (p = 0.054).
 2.4. Parameter recovery
 If the fit parameters are reliable, we should be able to take simulated data, with known parameters, and accurately estimate those parameters (Lewandowsky and Farrell, 2011). For each model, we chose parameters to represent “typical participants” and used the model's learning rules to generate two sets of simulated responses to each participant's observed outcomes. We used the same process as for the original participant responses to estimate parameters for the simulated responses.
 Figure ?Figure55 shows that the fit parameters for the WL model are clustered around the parameters used for data generation which are shown by crosses suggesting that the parameters are reliable.
 The left of Figure ?Figure66 shows the parameters representing probabilities in the structure of the HMM fit to participant behavior and on the right the parameters fit to data generated using the HMM with parameter values shown by crosses. If the participants had understood the experimental generation of outcomes and were applying that knowledge, we would expect the fit parameters to be close to those approximating the generation of data, shown in the left of Figure ?Figure66 by a cross.
 For the HMM, the spread of fit parameters away from the data generation parameters shows that the parameters are not well recovered. In particular, for several participants the fit value for the error probability was 0.49, that is the probability of losing when using the response type associated with the current rule. Fitting parameters to data generated with this parameter value, the estimated parameter values covered the whole range of feasible values. For the data generated with parameters closer to the actual experimental data, the fit parameters are not so widely spread.
2.5. Model recovery
 We found that the WL model was the best fit to participant data of the models tested. If model fitting is carried out on simulated data, the best fit model should be that which generated the data. Using the simulated data from the parameter recovery testing described above, we compared the fit of each model as in the analysis of participant data. Table ?Table22 shows the percentage of simulations using each model which were best fit by each model. The correct model has been identified in most cases for all of the models.
 The largest incorrect identification was the finding that the RL model was the best fit for 18.8% of the simulations by the HMM. The wrongly identified simulations were those which had the parameter for the error probability set to 0.49, and the probability of a switch set to 0.35. This was also the set of parameters which could not be reliably recovered from the simulated data as described above. A simulation using these parameters always gives probabilities close to 0.5 for each response with slight preference in line with the most recent outcome. Reinforcement learning produces responses in line with the most recent outcome by setting the learning rate to one, and the probabilities remain close to 0.5 by setting a high value for temperature. In this way the same behavior can be achieved by the HMM and RL models. Using BIC to compare models, RL will be preferred as the RL model has two parameters compared to four for the HMM.
 2.6. How well can these learning methods do?
 Human behavior was best fit by the WL model, we now consider how the models compare when carried out by an ideal agent. By ideal agent, we mean an agent which always selects the action which the model suggests is most likely to give a reward, and the model parameters are chosen to give the highest number of rewards for the task. We used the sequence of outcomes received by each participant in the task and then considered the performance of each model on each participant's trials.
 For the RL and WL models, these parameters were found by a grid search over all possible values of the learning rates at intervals of 0.01. For the WL model, a learning rate after a win of 0.48 and after a loss of 0.24 maximized rewards. A learning rate of 0.2 gave maximum rewards for the RL model. The WL model won significantly more rewards than the RL model t(30) = 3.53, p = 0.0014.
 For the HMM, we searched the parameter space in the region of those parameters approximating the generative environment to find the best performance. The generative environment had equal blocks with FV of 83% and 73%, giving an average probability of 22% of losing when using the response associated with the rule, the error probability. For the probability of a rule switch we used a probability of 0.021 based on 5 switches in 240 trials, having switches after 120 or 30 trials. The parameters which maximized rewards were 0.021 for the switch probability and 0.2 for the error probability. There was no significant difference between the performance of the WL and HMM models t(30) = 1, p = 0.33. The HMM was significantly better than the VOL model, t(30) = 4.98, p < 0.0001.
 Figure ?Figure77 shows the maximizing behavior, aligned with the experimental rule, of the ideal WL model in comparison to that of the participants. The percentage of responses in line with the underlying experimental rule were averaged over all participants and the ideal WL model for trials following rule switches, with each of the levels of feedback validity (FV) shown separately. The ideal WL has parameter values which optimize behavior over all trials, not just volatile blocks. The ideal WL model far outperforms the participants and reaches a steady level of maximizing at 100% in the high FV condition.
 As well as being able to outperform humans, the WL model can also closely simulate human behavior. Ten sets of simulated responses were generated using the WL models with the fit parameters and the sequence of outcomes for each individual participant. Figure ?Figure77 shows that the simulations closely replicate the aggregate performance of the participants. Although only volatile blocks are shown, the parameters used in the simulations were those fit to participant behavior across all trials regardless of the experimental conditions. Maximizing behavior of participants and simulations quickly adapts to a rule switch and reaches a plateau which is approximately equal to the level of feedback validity (probability matching). For trials 21–30 following a switch in the high feedback validity (FV = 83%) condition, participants showed maximizing of 82% and the WL simulation 81%. In the low feedback validity condition (FV = 73%), maximizing by participants and the WL model was 73%.","We find that a reinforcement learning model with separate parameters after a win or a loss (WL) gives a significantly better description of human behavior in a two-alternative probabilistic learning task with rule reversals than the other models tested. Our VOL model, implementing the model of Behrens et al. (2007), is flexible and able to adapt to changes in the level of expected uncertainty, or feedback validity (FV), and volatility. However, the WL model is a better fit than the others although it has constant parameters throughout all trials. Behrens et al. (2007), in a broadly similar decision making task, found their model to be a better fit to human behavior than reinforcement learning with constant parameters for each participant or separate parameters for volatile and stable blocks. The difference between the fit of our WL model and standard reinforcement learning (RL) applies even when they are compared using a method that penalizes the WL model. In particular, the advantage of the WL model was observed when the BIC of the models was compared, a method which strongly penalizes models with higher numbers of parameters such as our WL model (Lewandowsky and Farrell, 2011).
 Comparing the performance of ideal agents on our task, we find no significant difference between the HMM and WL models. Ideal agents have parameters which are chosen to maximize rewards given the model and always choose the option given by the model as the most favorable. Bayesian models are constructed to make optimal decisions, providing that the assumptions underlying the models are correct. Although the assumptions of the Bayesian models are based on the experimental structure used to generate rewards, the HMM or VOL models do not outperform the WL model on our task although the WL model does not adjust its learning rate to accommodate different levels of unexpected uncertainty and volatility. There is a small but significant improvement in performance of the WL model compared to the RL model when using ideal agents. All of the models, when used by ideal agents, far outperform human behavior.
 In our WL model, we find that for both the learning rate and temperature parameters, there is a significant difference between the fit parameter values following a win and a loss. These differences are consistent with psychological studies reporting behavioral differences in response to wins and losses and with existing neuroscientific knowledge indicating the existence of different neural pathways linked to the processing of wins and losses (see e.g., Kravitz et al., 2012; Yechiam and Hochman, 2013b).
 In psychology, the concept of loss-aversion, which suggests that behavior changes more in response to losses than to gains of similar magnitude (Kahneman and Tversky, 1984) has prompted much investigation. As an alternative mechanism to loss-aversion, Yechiam and Hochman (2013b) proposed a loss-attention mechanism in which losses cause participants to attend more closely to a task and so losses decrease the amount of randomization.
 These ideas of loss-aversion are often tested in studies of response to risk, that is where participants choose between alternatives with known outcome probabilities. An example (from Kahneman and Tversky, 1984) is a choice between a safe or risky option, where the risky option has an 85% chance of winning $1000 and a 15% chance of winning nothing and the safe option pays out $800 with certainty. People tend to prefer the safe option. In their examination of the loss-attention hypothesis, Yechiam and Hochman (2013a), used several tasks which involved repeated selections between a safe and a risky option where the probabilities had to be learnt from experience. They tested their loss-attention model by fitting a choice sensitivity parameter, the inverse of our temperature parameter, for each task. They found less randomization of responses in tasks in which losses were possible compared to tasks without losses. In our task, unlike that of Yechiam and Hochman (2013a), the participants could not avoid losses as there was no way to predict the outcome on individual trials. We find a higher temperature after individual losses, implying that participants are less likely to follow the underlying belief after a loss. This does not necessarily conflict with the idea of loss-attention, as adding randomness to a response after a loss may be a mechanism for testing an underlying belief without making a large adjustment to that belief.
 In neuroscience, dopamine is related to reward and punishment and separate D1 and D2 dopamine receptors in the basal ganglia have been found to respond to reward and punishment, respectively (see e.g., Gerfen, 1992). This inspired the use of separate pathways to respond to reward and punishment in the computational neural models of reinforcement learning by Frank and colleagues (see e.g., Frank, 2005; Samson et al., 2010). Testing this, Kravitz et al. (2012) found different pathways in the striatum of mice to be involved in processing reward and punishments. Rather than indicating reward and punishment directly, Schultz and colleagues suggested that dopamine signals the difference between an expected reward and that actually received (see e.g., Schultz, 1998). This difference forms the prediction error which is calculated in reinforcement learning.
 Following their neural models with separate pathways for learning after a win and a loss, (see e.g., Frank, 2005; Samson et al., 2010). Frank et al. (2007) use separate learning rate parameters following a win and a loss when using a reinforcement learning model to analyze human behavior in a probabilistic task. Like us, they find that the mean learning rate following a win is higher than that after a loss. They use just one temperature parameter and, as they are looking at associations between genetics and reinforcement learning parameters, they do not compare alternative models of behavior.
 We are aware of only a few studies that have considered separate effects of reward and punishment when comparing alternative computational models of learning from experience, none of which compare Bayesian models which make assumptions about the nature of the environment. These studies are based on different learning tasks to ours, and fit different reward values following a win or a loss (e.g., Ito and Doya, 2009; Guitart-Masip et al., 2012). Guitart-Masip et al. (2012) had four fractal images which signalled whether participants should respond or not to gain rewards or avoid punishments, these associations had to be learnt from experience and there was no switch in associations. Guitart-Masip et al. (2012) fit a number of different reinforcement learning models to behavior, the best fit model did not scale rewards and punishments differently. Analyzing the decisions of rats in two-stage probabilistic decisions, Ito and Doya (2009) found that a reinforcement learning model with different reward values after a win and a loss was a better fit to the rats' behavior than reinforcement learning without differentiation between wins and losses. To maintain the symmetry of the task in which exactly one response is correct on each trial, we have taken a different approach and fit a separate learning rate, rather than reward value, following wins and losses.
 As our Bayesian and reinforcement learning based models make different assumptions about the environment, comparing the fit of different models to human behavior can give insights into the assumptions people make about the environment. Our HMM, as with that of Hampton et al. (2006), as well as assuming that the two outcomes are coupled, assumes that there will be rule switches within probabilistic feedback. Our VOL model not only assumes that there will be rule switches, but also that the frequency of switches depends on the level of volatility in the environment. Hampton et al. (2006) compared a hidden Markov model to a reinforcement learning model that made no assumptions about the structure of the environment. They concluded that participants make assumptions about the structure of the environment. We also found that our hidden Markov model (HMM) was a better fit to behavior than a reinforcement learning model which did not assume that the outcomes were coupled. This uncoupled reinforcement learning model, however, was not as good a fit as our RL and WL models. From this we conclude that participants made some assumptions about the environment but have no evidence that they adjusted their rate of learning to the structure of the environment.
 Bayesian models can optimize the number of rewards received when the assumed structure for the Bayesian inference exactly matches the underlying structure of the task. We examined the performance of our models when, rather than being fit to human behavior, the model parameters were selected to maximize the total reward achieved, we refer to this as an ideal agent using the model. In our task, the rewards obtained by an ideal agent using the WL model were not significantly different to those of the ideal HMM. Our ideal HMM has parameters to closely resemble the generative structure, but assumes that there is a small constant probability of a rule switch. In the experimental data, rule switches only occurred at the ends of blocks of 30 or 120 trials. The HMM also assumes that for each environmental state, there is a constant probability of each outcome. However, the experimental data was generated using two levels of FV, with outcomes randomized to give the correct proportion within a block. We do not believe that these differences between the generative process and the assumptions of the HMM significantly hamper the performance of the HMM. We believe that the ideal agent using the WL model is approaching an optimal level of response in this task. The ideal WL model had a small but significant advantage over the ideal RL model in our task. Our ideal HMM also performed significantly better than our VOL model, our implementation of the model of Behrens et al. (2007). The VOL model is more flexible in the situations in which it can learn.
 The parameters for the ideal WL model, those which gave the best performance in the task, were learning rates of 0.48 after a win and 0.24 after a loss. Our participants also had significantly higher learning rates after a win than a loss, although generally higher than the ideal parameters, with means of 0.76 and 0.52 after a win and a loss, respectively.
 Learning under expected uncertainty with volatility is not simple as indicated by the range of participants' responses. However, our task, having coupled outcomes in which one or the other response is correct, does not require any exploration, or trying the different alternatives to see if things have changed. We expected the participants to know that if the button press was incorrect, then the other button would have been correct. Exploration is an important feature of learning from experience (see e.g., Cohen et al., 2007). Tasks which have more than two options automatically require exploration, as negative feedback does not show what would have been the correct response. It will accordingly be more difficult to learn when there are more alternatives. It has been acknowledged that standard reinforcement algorithms are not suitable in complex situations in which there may be many possible states or actions (e.g., Botvinick et al., 2009). Wilson and Niv (2012) compared optimal performance between a Bayesian and non-Bayesian model in their probabilistic learning task and the Bayesian model clearly had superior performance. Our finding that our ideal WL reinforcement learning model performs as well as our HMM may be restricted to the case of coupled two alternative tasks. Additionally, the level of feedback validity might affect the relative performance of the different styles of responding. These issues remain to be investigated.
 We assumed that whatever decision making processes the participants use to make their responses, these remain constant for the whole task. We have assumed that the task instructions give participants enough information to form a model. Some studies have found that a Bayesian model is a better fit to human behavior only in conditions when participants have been told to expect changes in rule (Payzan-LeNestour and Bossaerts, 2011; Wilson and Niv, 2012). In our study, participants were not given such information. The instructions given to participants can affect behavior in other ways, Taylor et al. (2012) found that probability matching was influenced by whether participants had an explanation for the probabilistic outcomes.
 In summary, we conclude that, with distinctions between learning from a win and a loss, reinforcement learning provides a very good description of the participant responses to repeated trials under expected uncertainty and volatility. It is able to account for individual differences with parameters that remain constant throughout all trials although the feedback validity and volatility varied. Future research should explore whether the differential treatment of a win and a loss would lead to a similar robust performance in other experimental situations."
8,2183937,"""Nucleus accumbens core lesions retard instrumental learning and performance with delayed reinforcement in the rat""","""10.1186/1471-2202-6-9""",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549214,__sec5,__sec17,"In Experiment 1, rats received excitotoxic lesions of the AcbC or sham lesions, and were then tested on an instrumental free-operant acquisition task with delayed reinforcement (Experiment 1A; see Methods) and subsequently a reinforcer magnitude discrimination task (Experiment 1B). In Experiment 2, naïve rats were trained on the free-operant task for delayed reinforcement; AcbC lesions were then made and the rats were retested.
Histology
 In Experiment 1, there were two postoperative deaths. Histological analysis revealed that the lesions were incomplete or encroached significantly on neighbouring structures in four subjects. These subjects were excluded; final group numbers were therefore 8 (sham, 0 s delay), 6 (AcbC, 0 s delay), 8 (sham, 10 s delay), 7 (AcbC, 10 s delay), 8 (sham, 20 s delay), and 7 (AcbC, 20 s delay). In Experiment 2, one rat spontaneously fell ill with a colonic volvulus during preoperative training and was killed, and there were three postoperative deaths. Lesions were incomplete or too extensive in seven subjects; final group numbers were therefore 7 (sham, 0 s delay), 5 (AcbC, 0 s delay), 8 (sham, 10 s delay), 4 (AcbC, 10 s delay), 8 (sham, 20 s delay), and 5 (AcbC, 20 s delay).
 Lesions of the AcbC encompassed most of the core subregion; neuronal loss and associated gliosis extended in an anteroposterior direction from approximately 2.7 mm to 0.5 mm anterior to bregma, and did not extend ventrally or caudally into the ventral pallidum or olfactory tubercle. Damage to the ventromedial caudate-putamen was occasionally seen; damage to AcbSh was restricted to the lateral edge of the dorsal shell. Schematics of the lesions are shown in Figure ?Figure2.2. Photomicrographs of one lesion are shown in Figure ?Figure3,3, and are similar to lesions with identical parameters that have been presented before [29,30].
 Acquisition of instrumental responding (Experiment 1A)
 The imposition of response-reinforcer delays retarded the acquisition of free-operant lever pressing, in sham-operated rats and in AcbC-lesioned rats (Figure ?(Figure4).4). AcbC-lesioned rats responded slightly more than shams on both the active and inactive levers in the absence of response-reinforcers delays, but when such delays were present, AcbC lesions retarded acquisition relative to sham-operated controls (Figure ?(Figure55).
 An overall ANOVA using the model lesion2 × delay3 × (session14 × lever2 × S) revealed multiple significant interactions, including lever × delay × lesion (F2,38 = 5.17, p = .01) and session × lever × delay (F6.0,229.1 = 5.47,  = .464, p < .001), justifying sub-analysis. All six groups learned to respond more on the active lever than the inactive lever (p ? .002, main effect of lever or session × lever interaction for each group alone).
 For sham-operated rats, delays reduced the rate of acquisition of the active lever response and reduced the asymptotic level of responding attained (Figure ?(Figure4a;4a; delay: F2,21 = 11.7, p < .001;  = .276, p < .001; session × delay: F7.2,75.3 = 2.46,  = .276, p = .024). The presence of a delay also increased responding on the inactive lever slightly (delay: F2,21 = 4.06, p = .032), though not systematically (the 10 s group differed from the 0 s group, p = .036, but no other groups differed, p ? .153).
 There was a further, delay-dependent impairment in AcbC-lesioned rats, who responded more than shams at 0 s delay but substantially less than shams at 10 s and 20 s delay. As in the case of sham-operated controls, delays reduced the rate of acquisition and the maximum level of responding attained in AcbC-lesioned rats (Figure ?(Figure4b;4b; delay: F2,17 = 54.6, p < .001; delay × session: F6.9,58.7 = 2.64,  = .266, p = .02). Responding on the inactive lever was not significantly affected by the delays (maximum F15.8,134.2 = 1.65,  = .607, p = .066). At 0 s delay, AcbC-lesioned subjects responded more than shams on the active lever (Figure ?(Figure5a;5a; lesion: F1,12 = 5.30, p = .04) and the inactive lever (lesion: F1,12 = 9.12, p = .011). However, at 10 s delay, AcbC-lesioned rats responded significantly less than shams on the active lever (Figure ?(Figure5b;5b; lesion: F1,13 = 9.04, p = .01); there was no difference in responding on the inactive lever (F < 1, NS). At 20 s delay, again, AcbC-lesioned rats responded significantly less than shams on the active lever (Figure ?(Figure5c;5c; lesion: F1,13 = 9.87, p = .008) and there was no difference in responding on the inactive lever (F < 1, NS).
 Experienced response-delivery and response-collection delays (Experiment 1A)
 For every reinforcer delivered, the active lever response most closely preceding it in time was identified, and the time between that response and delivery of the reinforcer (the 'response-delivery delay') was calculated. This time can therefore be equal to or less than the programmed delay, and is only relevant for subjects experiencing non-zero programmed response-reinforcer delays. The response-to-reinforcer-collection ('response-collection') delays were also calculated: for every reinforcer delivered, the response most closely preceding it and the nosepoke most closely following it were identified, and the time between these two events calculated. This time can be shorter or longer than the programmed delay, and is relevant for all subjects.
 AcbC-lesioned rats experienced the same response-delivery delays as shams when the programmed delay was 10 s, but experienced longer response-delivery delays when the programmed delay was 20 s (Figure ?(Figure6a).6a). Similarly, AcbC-lesioned rats experienced the same response-collection delays as shams when the programmed delay was 0 s, slightly but not significantly longer response-collection delays when the programmed delay was 10 s, and significantly longer response-collection delays when the programmed delay was 20 s (Figure ?(Figure6b).6b). These differences in the mean delay experienced by each rat were reflected in differences in the distribution of response-delivery and response-collection delays when the programmed delay was non-zero (Figure 6c,d). Since AcbC-lesioned rats experienced slightly longer delays than sham-operated rats, it was necessary to take this into account when establishing the effect of delays on learning, as follows.
 Effect of delays on learning (Experiment 1A)
 There was a systematic relationship between the acquisition rate and the programmed delay of reinforcement, and this was altered in AcbC-lesioned rats. Figure ?Figure7a7a replots the rates of responding on the active lever on session 10 of acquisition [1]. Despite the comparatively low power of such an analysis, lever-pressing was analysed for this session only using the model lesion2 × delay3. This revealed a significant lesion × delay interaction (F2,38 = 12.6, p < .001), which was analysed further. Increasing delays significantly reduced the rate of responding in this session for shams (F2,21 = 17.3, p < .001) and AcbC-lesioned rats (F2,17 = 54.4, p < .001). AcbC-lesioned rats responded more than shams at zero delay (F1,12 = 8.52, p = .013) but less than shams at 10 s delay (F1,13 = 4.71, p = .049) and at 20 s delay (F1,13 = 17.3, p = .001).
 Since the AcbC group experienced slightly longer response-delivery and response-collection delays than shams when the programmed delay was non-zero (Figure ?(Figure6),6), it was important to establish whether this effect alone was responsible for the retardation of learning, or whether delays retarded learning in AcbC-lesioned rats over and above any effect to increase the experienced delay. The mean experienced response-collection delay was calculated for each subject, up to and including session 10. The square-root-transformed number of responses on the active lever in session 10 was then analysed using a general linear model of the form lesion2 × experienced delaycov. Unlike a standard analysis of covariance, the factor × covariate interaction term was included in the model. This confirmed that the lesion retarded the acquisition of responding in AcbC-lesioned rats, compared to controls, in a delay-dependent manner, over and above the differences in experienced delay (Figure ?(Figure7b;7b; lesion × experienced delay: F1,40 = 12.4, p = .001).
 Experienced delays and learning on the inactive lever (Experiment 1A)
 No such delay-dependent effects were observed for the inactive lever. Experienced inactive-response-delivery delays (calculated across all sessions in the same manner as for the active lever) were much longer and more variable than corresponding delays for the active lever, because subjects responded on the inactive lever so little. Means ± SEMs were 250 ± 19 s (sham, 0 s), 214 ± 29 s (AcbC, 0 s), 167 ± 23 s (sham, 10 s), 176 ± 33 s (AcbC, 10 s), 229 ± 65 s (sham, 20 s), and 131 ± 37 s (AcbC, 20 s). ANOVA of these data revealed no effects of lesion or programmed delay and no interaction (maximum F1,38 = 1.69, NS). Experienced inactive-response-collection delays were 252 ± 19 s (sham, 0 s), 217 ± 29 s (AcbC, 0 s), 169 ± 23 s (sham, 10 s), 179 ± 33 s (AcbC, 10 s), 231 ± 65 s (sham, 20 s), and 136 ± 37 s (AcbC, 20 s). Again, ANOVA revealed no effects of lesion or programmed delay and no interaction (maximum F1,38 = 1.61, NS). When the square-root-transformed number of responses on the inactive lever in session 10 was analysed with the experienced delays up to that point as a predictor, using the model lesion2 × experienced inactive-response-collection delaycov just as for the active lever analysis, there was no lesion × experienced delay interaction (F < 1, NS).
 Discrimination of relative reinforcer magnitude (Experiment 1B)
 Relative preference for two reinforcers may be inferred from the distribution of responses on concurrent variable interval schedules of reinforcement [31-33]. According to Herrnstein's matching law [31], if subjects respond on two concurrent schedules A and B delivering reinforcement at rates rA and rB respectively, they should allocate their response rates RA and RB such that RA/(RA+RB) = rA/(rA+rB). Overmatching is said to occur if subjects prefer the schedule with the higher reinforcement rate more than predicted by the matching law; undermatching is the opposite. Both sham-operated and AcbC-lesioned rats were sensitive to the distribution of reinforcement that they received on two concurrent random interval (RI) schedules, altering their response allocation accordingly. Subjects preferred the lever on which they received a greater proportion of reinforcement. In general, subjects did not conform to the matching law, but exhibited substantial undermatching; this is common [33]. AcbC-lesioned rats exhibited better matching (less undermatching) than shams (Figure ?(Figure8),8), suggesting that their sensitivity to the relative magnitudes of the two reinforcers was as good as, or better than, shams'.
 To analyse these data, the proportion of pellets delivered by lever A (see Methods), and the proportion of responses allocated to lever A, were calculated for each subject for the last session in each of the three programmed reinforcement distribution contingencies (session 11, programmed reinforcement proportion 0.5; session 19, programmed proportion 0.8; session 27, programmed proportion 0.2; see Table ?Table1).1). The analysis used a model of the form response proportion = lesion2 × (experienced reinforcer distributioncov × S); the factor × covariate term was included in the model. Analysis of sham and AcbC groups separately demonstrated that both groups altered their response allocation according to the distribution of reinforcement, i.e. that both groups discriminated the two reinforcers on the basis of their magnitude (effects of reinforcer distribution; sham: F1,47 = 16.6, p < .001; AcbC: F1,39 = 97.2, p < .001). There was also a significant lesion × reinforcer distribution interaction (F1,86 = 5.5, p = .021), indicating that the two groups' matching behaviour differed, with the AcbC-lesioned rats showing better sensitivity to the relative reinforcer magnitude than the shams (Figure ?(Figure8).8). These statistical conclusions were not altered by including counterbalancing terms accounting for whether lever A was the left or right lever (the left having been the active lever previously in Experiment 1A), or whether a given rat had been trained with 0, 10, or 20 s delays in Experiment 1A.
 Switching behaviour during concurrent schedule performance (Experiment 1B)
 Because switching behaviour has the potential to influence behaviour on concurrent schedules e.g. [34], we also analysed switching probabilities. AcbC-lesioned rats were less likely than shams to switch between levers when responding on two identical concurrent RI schedules with a changeover delay (COD) of 2 s. Responses on the left and right levers were sequenced for sessions 8–11 (concurrent RI-60s schedules, each delivering a one-pellet reinforcer; see Methods and Table ?Table1),1), and the probabilities of switching from one type of response to another, or repeating the same type of response, were calculated. The switch probabilities were analysed by one-way ANOVA; this revealed an effect of lesion (F1,42 = 8.88, p = .005). Mean switch probabilities (± SEMs) were 0.41 ± 0.02 (AcbC) and 0.49 ± 0.01 (sham).
 Effects of AcbC lesions on performance of a previously-learned instrumental response for delayed reinforcement (Experiment 2)
 Due to mechanical faults, data from four subjects in session 10 (preoperative) and data from one subject in session 22 (postoperative) were not collected. Both sessions were removed from analysis completely, and data points for those sessions are plotted using the mean and SEM of the remaining unaffected subjects (but not analysed).
 Preoperatively, the groups remained matched following later histological selection. Analysis of the last 3 preoperative sessions, using the model lesion intent2 × delay3 × (session3 × lever2 × S), indicated that responding was affected by the delays to reinforcement (delay: F2,31 = 5.46, p = .009; delay × lever: F2,31 = 19.5, p < .001), but there were no differences between the groups due to receive AcbC and sham lesions (terms involving lesion intent: maximum F was for session × lever × lesion intent, F2,62 = 1.844, NS). As expected, delays reduced the rate of responding on the active lever (F2,31 = 15.6, p < .001) and increased responding on the inactive lever (F2,31 = 8.12, p = .001) preoperatively.
 AcbC lesions selectively impaired performance of instrumental responding only when there was a response-reinforcer delay. There was no effect of the lesion on responding under the 0 s delay condition, but in the presence of delays, AcbC lesions impaired performance on the active lever (Figure ?(Figure9;9; Figure ?Figure10).10). These conclusions were reached statistically as follows.
 Subjects' responding on the relevant lever in the last preoperative session (session 14) was used as a covariate to increase the power of the analysis [35]. As expected, there were no significant differences in the covariates themselves between groups due to receive AcbC or sham surgery (terms involving lesion intent for the active lever: Fs < 1, NS; for the inactive lever, lesion intent: F1,31 = 2.99, p = .094; lesion intent × delay: F < 1, NS). Analysis of the postoperative sessions, using the model lesion2 × delay3 × (session17 × lever2 × session-14-active-lever-responsescov × S), revealed a near-significant lesion × delay × session × lever interaction (F22.4,335.5 = 1.555,  = .699, p = .054). Furthermore, analysis of postoperative responding on the active lever, using the model lesion2 × delay3 × (session17 × session-14-active-lever-responsescov × S), revealed a session × delay × lesion interaction (F17.3,259.5 = 1.98,  = .541, p = .013) and a delay × lesion interaction (F2,30 = 3.739, p = .036), indicating that the lesion affected responding on the active lever in a delay-dependent manner. In an identical analysis of responding on the inactive lever (using inactive lever responding on session 14 as the covariate), no terms involving lesion were significant (maximum F: lesion, F1,30 = 1.96, p = .172), indicating that the lesion did not affect responding on the inactive lever.
 Postoperatively, response-reinforcer delays continued systematically to decrease responding on the active lever, both in shams (Figure ?(Figure9a;9a; delay: F2,20 = 11.78, p < .001; session × delay: F12.4,124.1 = 2.36,  = .388, p = .008) and in AcbC-lesioned rats (Figure ?(Figure9b;9b; delay: F2,11 = 13.9, p = .001). Shams continued to discriminate between the active and inactive lever at all delays (lever: all groups p ? .002; lever × session: all groups p ? .003). AcbC-lesioned rats continued to discriminate at 0 s and 10 s (lever: p ? .011; lever × session: p ? .036), but AcbC-lesioned subjects in the 20 s condition failed to discriminate between the active and inactive levers postoperatively (lever: F1,4 = 1.866, p = .244; lever × session: F < 1, NS).
 Lesioned subjects responded as much as shams at 0 s delay, but substantially less than shams at 10 s and 20 s delay (Figure ?(Figure10).10). Again, analysis was conducted using responding on the relevant lever in session 14 (the last preoperative session) as a covariate. At 0 s, the lesion did not affect responding on the active lever (lesion: F < 1, NS; lesion × session: F16,144 = 1.34, NS). However, at 10 s, AcbC-lesioned rats responded significantly less than shams on the active lever (lesion: F1,9 = 7.08, p = .026; lesion × session: F15.0,135.3 = 3.04,  = .94, p < .001). Similarly, at 20 s, AcbC-lesioned rats responded less than shams on the active lever (lesion: F1,10 = 6.282, p = .031). There were no differences on responding on the inactive lever at any delay (Fs ? 1.31, NS).
 As in Experiment 1, AcbC-lesioned rats experienced the same response-delivery delays as shams when the programmed delay was 10 s, but experienced longer response-delivery delays when the programmed delay was 20 s (Figure 11a). Similarly, AcbC-lesioned rats experienced the same response-collection delays as shams when the programmed delay was 0 s, slightly but not significantly longer response-collection delays when the programmed delay was 10 s, and significantly longer response-collection delays when the programmed delay was 20 s (Figure 11b).
 Relationship between experienced delays and performance (Experiment 2)
 There was a systematic relationship between the postoperative response rate and the programmed delay of reinforcement, and this was altered in AcbC-lesioned rats. Figure 12a replots the rates of lever-pressing on session 24, the 10th postoperative session (compare Figure ?Figure7).7). An analysis using the model lesion2 × programmed delay3 revealed a significant lesion × delay interaction (F2,31 = 5.09, p = .012). In this session, there was no significant effect of delays on shams' performance (F2,20 = 2.15, p = .143), though there was for AcbC-lesioned rats (F2,11 = 9.01, p = .005). There were no significant differences in responding on this session between shams and AcbC-lesioned rats in the 0 s condition (F1,10 = 3.10, p = .109) or the 10 s condition (F < 1, NS), but AcbC-lesioned rats responded less at 20 s delay (F1,11 = 6.74, p = .025).
 Since the AcbC group experienced slightly longer response-delivery and response-collection delays than shams when the programmed delay was non-zero (Figure ?(Figure11),11), as before, the rate of responding in session 24 was analysed as a function of the delays experienced postoperatively. The mean experienced response-collection delay was calculated for postoperative sessions up to and including session 24; the square-root-transformed number of lever presses in session 24 was then analysed using a general linear model of the form lesion2 × experienced delaycov, with the factor × covariate interaction term included in the model. This confirmed that the lesion affected responding in AcbC-lesioned rats, compared to controls, in a delay-dependent manner, over and above the postoperative differences in experienced delay (Figure 12b; lesion × experienced delay: F1,33 = 6.53, p = .015).
 Locomotor activity and body mass
 AcbC-lesioned animals were hyperactive compared to sham-operated controls, and gained less mass then shams across the experiments (Figure ?(Figure13),13), consistent with previous results [22,29,36].","These results establish that the AcbC contributes to learning of actions when the outcome is delayed. Lesions of the AcbC did not impair instrumental learning when the reinforcer was delivered immediately, but substantially impaired learning with delayed reinforcement, indicating that the AcbC 'bridges' action-outcome delays during learning. Lesions made after learning also impaired performance of the instrumental response in a delay-dependent fashion, indicating that the AcbC also contributes to the performance of actions for delayed reinforcement. Finally, the lesions did not impair the perception of relative reward magnitude as assessed by responding on identical concurrent interval schedules for reinforcers of different magnitude, suggesting that the impulsive choice previously exhibited by AcbC-lesioned rats [22] is attributable to deficits in dealing with delays to reinforcement.
Effect of delays on instrumental learning in normal animals
 Delays have long been known to retard instrumental learning [1,37]. Despite this, normal rats have been shown to acquire free-operant responding with programmed response-reinforcer delays of up to 32 s, or even 64 s if the subjects are pre-exposed to the learning environment [1]. Delays do reduce the asymptotic level of responding [1], though the reason for this phenomenon is not clear. It may be that when subjects learn a response with a substantial response-reinforcer delay, they never succeed in representing the instrumental action-outcome contingency fully. Alternatively, they may value the delayed reinforcer slightly less; finally, the delay may also retard the acquisition of a procedural stimulus-response habit and this might account for the decrease in asymptotic responding. It is not presently known to what degree responses acquired with a response-reinforcer delay are governed by declarative processes (the action-outcome contingency plus a representation of the instrumental incentive value of the outcome) or procedural mechanisms (stimulus-response habits), both of which are known to influence instrumental responding [38,39]; it is similarly not known whether the balance of these two controlling mechanisms differs from that governing responses learned without such a delay.
 Effect of AcbC lesions on instrumental learning and performance with or without delays
 In the absence of response-reinforcer delays, AcbC-lesioned rats acquired an instrumental response normally, responding even more than sham-operated controls. In contrast, blockade of N-methyl-D-aspartate (NMDA) glutamate receptors in the AcbC has been shown to retard instrumental learning for food under a variable-ratio-2 (VR-2) schedule [in which P(reinforcer | response) ? 0.5] [40], as has inhibition or over-stimulation of cyclic-adenosine-monophosphate-dependent protein kinase (protein kinase A; PKA) within the Acb [41]. Concurrent blockade of NMDA and DA D1 receptors in the AcbC synergistically prevents learning of a VR-2 schedule [42]. Once the response has been learned, subsequent performance on this schedule is not impaired by NMDA receptor blockade within the AcbC [40]. Furthermore, infusion of a PKA inhibitor [41] or a protein synthesis inhibitor [43] into the AcbC after instrumental training sessions impairs subsequent performance, implying that PKA activity and protein synthesis in the AcbC contribute to the consolidation of instrumental behaviour. Thus, manipulation of Acb neurotransmission can affect instrumental learning. However, it is also clear that excitotoxic destruction of the AcbC or even the entire Acb does not impair simple instrumental conditioning to any substantial degree. Rats with Acb or AcbC lesions acquire lever-press responses on sequences of random ratio schedules [in which P(reinforcer | response) typically declines from around 1 to 0.05 over training] at near-normal levels [44,45]. In such ratio schedules, where several responses are required to obtain reinforcement, there is no delay between the final response and reinforcement, but there are delays between earlier responses and eventual reinforcement. It is therefore of interest that when differences between AcbC-lesioned rats and shams have been observed, AcbC-lesioned animals have been found to respond somewhat less than shams on such schedules late in training, when the ratio requirement is high [44,45], consistent with our present results. However, lesioned rats are fully sensitive to changes in the instrumental contingency [27,44,45]. Our present results indicate that when AcbC-lesioned rats are exposed to a FR-1 schedule for food [P(reinforcer | response) = 1] in the absence of response-reinforcer delays, they acquire the response at normal rates.
 In contrast, when a delay was imposed between responding and reinforcement, AcbC-lesioned rats were impaired relative to sham-operated controls, in a systematic and delay-dependent fashion. The observation that learning was not affected at zero delay rules out a number of explanations of this effect. For example, it cannot be that AcbC-lesioned rats are in some way less motivated for the food per se, since they responded normally (in fact, more than shams) when the food was not delayed. Thus although the Acb and its dopaminergic innervation are clearly very important in motivating behaviour e.g. [23,46-48], this is not on its own a sufficient explanation for the present results. An explanation in terms of a rate-dependent impairment is also not tenable, since the AcbC-lesioned rats were capable (in the zero-delay condition) of responding at a level greater than they exhibited in the non-zero-delay conditions. Depletion of Acb DA also impairs rats' ability to work on high-effort schedules, where many, or very forceful, responses are required to obtain a given amount of food [47,48]. However, in the present experiments the ratio requirement (one response per reinforcer) and the force required per press were both held constant across delays, so this effect cannot explain the present results. Similarly, although AcbC lesions are known to impair the control over behaviour by Pavlovian conditioned stimuli e.g. [23,29,49-52], there was no Pavlovian stimulus that was differentially associated with delayed as opposed to immediate reinforcement in this task, so this cannot explain the present results.
 Our results also indicated that when there were programmed delays to reinforcement, AcbC-lesioned animals experienced longer response-reinforcer collection delays, partly due to their failure to collect the reinforcer as promptly as shams. These additional experienced delays probably retarded learning. However, in addition to this effect, there was a further deficit exhibited by AcbC-lesioned rats: even allowing for the longer response-collection delays that they experienced, their instrumental learning was impaired more by delays than that of sham-operated controls. Deficits in learning with delayed reinforcement may account for some of the variability in the effect of AcbC lesions or local pharmacological manipulations on instrumental learning across different schedules.
 The fact that pre-exposure to the context improves instrumental learning in normal rats [1] suggests one possible mechanism by which AcbC lesions might retard learning when delays are present. When a reinforcer arrives, it may be associated either with a preceding response, or with the context. Therefore, in normal animals, pre-exposure to the context may retard the formation of context-reinforcer associations by latent inhibition, or it might serve to retard the formation of associations between irrelevant behaviours and reinforcement. Similarly, non-reinforced exposure to the context forces the subjects to experience a zero-response, zero-reinforcer situation, i.e. P(outcome | no action) = 0. When they are then exposed to the instrumental contingency, such that P(outcome | action) > 0, this prior experience may enhance their ability to detect the instrumental contingency ?P = P(outcome | action) - P(outcome | no action). In one aversive Pavlovian conditioning procedure in which a conditioned stimulus (CS) was paired with electric shock, AcbC lesions have been shown to impair conditioning to discrete CSs, but simultaneously to enhance conditioning to contextual (background) CSs [53], though not all behavioural paradigms show this effect [54,55]. It is therefore possible that enhanced formation of context-reinforcer associations may explain the retardation of response-reinforcer learning in AcbC-lesioned rats in the presence of delays.
 The instrumental task used requires animals either to associate their response with the delayed food outcome (an action-outcome association that can be used for goal-directed behaviour), or to strengthen a stimulus-response association (habit) when the reinforcer eventually arrives [38,39]. Both mechanisms require the animal to maintain a representation of their past action so it can be reinforced (as a habit) or associated with food when the food finally arrives. This mnemonic requirement is not obviated even if the animal learns to predict the arrival of food using discriminative stimuli, and uses these stimuli to reinforce its responding (conditioned reinforcement): in either case, since the action precedes reinforcement, some trace of past actions or stimuli must persist to be affected by the eventual delivery of food.
 A delay-dependent impairment was also seen when AcbC lesions were made after training. This indicates that the AcbC does not only contribute to the learning of a response when there is an action-outcome delay: it also contributes to the performance of a previously-learned response. Again, AcbC-lesioned rats were only impaired when that previously-learned response was for delayed (and not immediate) reinforcement. Of course, learning of an instrumental response depends upon the animal being able to perform that response; preventing an animal from pressing a lever (a performance deficit) would clearly impair its ability to learn an instrumental response on that lever to obtain food. In the present set of experiments, it is clear that AcbC-lesioned rats were just as able to perform the response itself (to press the active lever and to discriminate it physically from the inactive lever) as controls, as shown by their normal performance in the zero-delay condition, so it is not clear whether the delay-dependent impairments in learning and performance can be attributed to the same process. Again, since responding was unaffected in the zero-delay condition, many alternative interpretations (such as a lack of motivation to work for the food) are ruled out. It may be that AcbC-lesioned rats are impaired at representing a declarative instrumental action-outcome contingency when the outcome is delayed, or in forming or executing a procedural stimulus-response habit when the reinforcing event does not follow the response immediately. It may also be that they represent the action-outcome contingency normally but value the food less because it is delayed, and that this affects responding in a free-operant situation even though there is no alternative reinforcer available.
 Excitotoxic lesions of the whole Acb do not prevent rats from detecting changes in reward value (induced either by altering the concentration of a sucrose reward or by changing the deprivational state of the subject) [27]. Such lesions also do not impair rats' ability to respond faster when environmental cues predict the availability of larger rewards [28], and nor does inactivation of the Acb with local anaesthetic or blockade of AMPA glutamate receptors in the Acb [56]; the effects of intra-Acb NMDA receptor antagonists have varied [57,58]. AcbC-lesioned rats can still discriminate large from small rewards [24,25]. Similarly, DA depletion of the Acb does not affect the ability to discriminate large from small reinforcers [59-61], and systemic DA antagonists do not affect the perceived quantity of food as assessed in a psychophysical procedure [62]. Our study extends these findings by demonstrating that excitotoxic AcbC lesions do not impair rats' ability to allocate their responses across two schedules in proportion to the experienced reinforcement rate, even when the two schedules are identical except in the magnitude of the reinforcements they provide, thus demonstrating their sensitivity to reinforcer magnitude is quantitatively no worse than shams'. In this experiment, there was substantial undermatching, but this is common [33,63] see also [64,65]; differential cues signalling the two rewards might have improved matching but were not used in the present experiments since it is known that AcbC lesions can themselves affect rats' sensitivity to cues signalling reinforcement [23,29,49-52]. Given that AcbC-lesioned subjects showed a reduced probability of switching between two identical RI schedules, it may be the case that an enhanced sensitivity to the COD accounts for the better matching exhibited by the AcbC-lesioned rats [34]. Alternatively, the lesion may have enhanced reinforcer magnitude discrimination or improved the process by which behaviour allocation is matched to environmental contingencies. In summary, the present results suggest that AcbC damage leads to pathological impulsive choice (preferring a small, immediate reinforcer to a large, delayed reinforcer) [22] not through any relative lack of value of large reinforcers, but through a specific deficit in responding for delayed reinforcement.
 Contribution of the AcbC to reinforcement learning
 The term 'reinforcement learning' simply means learning to act on the basis of reinforcement received; it is a term used in artificial intelligence research [66] that does not specify the mechanism of such learning [67,68]. Our present results indicate that the AcbC is a reinforcement learning structure that is critical for instrumental conditioning when outcomes are delayed, consistent with electrophysiological and functional neuroimaging evidence indicating that the ventral striatum responds to recent past actions [10,15] and to predicted future rewards [8-15], and with computational models suggesting a role for the striatum in predicting future primary reinforcement [20,21]. However, when reward is certain and delivered immediately, the AcbC is not necessary for the acquisition of instrumental responding. The delay-dependent role of the AcbC indicates that it plays a role in allowing actions to be reinforced by bridging action-outcome delays through a representation of past acts or future rewards. Acb lesions have also produced delay-dependent impairments in a delayed-matching-to-position task [69,70]; their effects on the delayed-matching-to-sample paradigm have also been studied, but a more profound and delay-independent deficit was observed, likely due to differences in the specific task used [71]. Finally, the AcbC is not alone in containing neurons that respond to past actions and future rewards. The dorsal striatum is another such structure [10,15,72,73]; expression of stimulus-response habits requires the dorsal striatum [74,75], and the rate at which rats learn an arbitrary response that delivers electrical stimulation to the substantia nigra is correlated with the degree of potentiation of synapses made by cortical afferents onto striatal neurons, a potentiation that requires DA receptors [76,77]. The prelimbic area of rat prefrontal cortex is important for the detection of instrumental contingencies and contributes to goal-directed, rather than habitual, action [78,79]. Similarly, the orbitofrontal cortex and basolateral amygdala encode reinforcement information and project to the AcbC, and lesions of these structures can produce impulsive choice see [24,80-82]. It is not yet known whether lesions of these structures also impair learning with delayed reinforcement."
9,908754,"""Proximity of Substantia Nigra Microstimulation to Putative GABAergic Neurons Predicts Modulation of Human Reinforcement Learning""","""10.3389/fnhum.2017.00200""",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5422436,s3,s4,"We applied intra-operative microstimulation in the SN of eleven patients undergoing DBS for the treatment of PD as they performed a reinforcement learning task (Table ?(Table1).1). Subjects selected between a red and blue card deck by pressing buttons on hand-held controllers and subsequently received positive or negative feedback (Figure ?(Figure1A).1A). The reward probabilities associated with each card deck stochastically fluctuated throughout the intra-operative session to encourage learning (Figure ?(Figure1B,1B, see Section 2).
 Subjects demonstrated clear evidence of learning on the task. Both during stage 1 and stage 2, subjects showed an increased probability of repeating the same action after receiving positive feedback [“win-stay,” 0.5 expected by chance; t(10) > 5.8, p's < 0.001, Figure ?Figure1C].1C]. Subjects also showed an increased probability of making a high reward probability choice (“accuracy”) during the last 10 trials of a particular reward probability regime, as compared to the first 10 trials after a regime switch [t(10) = 4.35, p = 0.001, Figure ?Figure1D1D].
 To assess the importance of SN neuronal activity for learning, we applied SN microstimulation following approximately half the reward trials during stage 2 of each subject's intra-operative session. To assess whether SN stimulation had an effect on learning, we compared subjects' win-stay probabilities following reward trials that were accompanied by stimulation (“stim trials”) and stage 2 reward trials during which stimulation was not applied (“control trials”). Across 11 subjects, we observed a trend toward decreased win-stay following stimulation trials compared to control trials [t(10) = 2.03, p = 0.068, Figure ?Figure22].
 Our main hypothesis was that stimulation-related changes in learning would vary based on the functional properties of neurons near the electrode tip. To assess whether this was the case, we extracted various physiological parameters from neural activity recorded during stage 1 of each subject's intra-operative session (see Section 2). We assessed whether there was a correlation between stimulation-related changes in learning and mean spike rate of units recorded on each channel, and observed a significant negative correlation such that the greatest impairments in learning were observed when the electrode was positioned near neurons with relatively high spike rates (r = ?0.64, p = 0.045, Figure ?Figure3A).3A). Based on the the established finding that high spike rates and narrow waveforms are properties of GABAergic neurons (Ungless and Grace, 2012), we also assessed for a correlation between stimulation-related changes in learning and mean waveform duration. We observed a positive correlation between stimulation-related changes in learning and waveform duration, such that the strongest impairments occurred near neurons with narrow waveforms (r = 0.64, p = 0.044, Figure ?Figure3B).3B). We did not observe a significant relation between stimulation-related changes in learning and phasic post-reward changes in activity (p > 0.5), and generally did not observe post-reward phasic changes in activity (z-score range: ?0.1:0.36). Two example neurons are shown in Figure ?Figure3C3C.","We applied microstimulation in SN of patients undergoing DBS for the treatment of PD as they performed a reinforcement learning task. We found that microstimulation applied during the 500-ms post-reward interval impaired learning. These results demonstrate a causal relation between post-reward SN firing and human reinforcement learning as microstimulation is known to acutely enhance local neural firing (Histed et al., 2009). We hypothesized that the effect of SN microstimulation on learning would vary based on their relative proximity to dopaminergic (DA) neurons that guide reinforcement learning (Glimcher, 2011) or GABAergic neurons that exert inhibitory control on DA neurons (Damier et al., 1999a; Lobb et al., 2011; Ramayya et al., 2014b). As hypothesized, we observed the largest stimulation-related impairments in learning when the electrode was positioned near neurons with relatively high firing rates and narrow waveforms, properties characteristic of GABA neurons (Joshua et al., 2009; Matsumoto and Hikosaka, 2009; Ungless and Grace, 2012). Thus, our results suggest that microstimulation near GABA neurons impairs reinforcement learning.
 This finding provides direct evidence relating phasic SN neural firing to human reinforcement learning. It goes beyond animal electrophysiology studies that may not generalize to human learning because they typically involve long periods of intense training. It also goes beyond prior human studies of reinforcement learning; functional neuroimaging studies cannot test a causal role for SN neural activity (Montgomery et al., 2009), and pharmacological manipulations of DA in patients with PD (Frank et al., 2004; Rutledge et al., 2009) cannot distinguish phasic neural activity from tonic changes in DA throughout the brain (Niv et al., 2007). Ramayya et al. (2014a) also showed a stimulation-related decrease in performance. However, because rewards in that study were contingent on stimuli, but independent of actions, the observed stimulation-related decrease in performance could either be attributed to an impairment of learning or a selective strengthening of action-reward associations that competed with stimulus-reward learning. Our current study overcame this limitation by using an experimental design with consistent stimulus-response mapping, such that stimulus-reward and action-reward associations were always correlated. Thus, our finding of a stimulation-related impairment in performance suggests decreased learning.
 In Ramayya et al. (2014a), stimulation-related decreases in performance were correlated with an increased propensity to repeat the same action following reward, particularly when the electrode was positioned near putative DA neurons, suggesting that microstimulation near SN DA neurons enhanced action-reward learning. The current finding that stimulation near putative GABA neurons produced impairments in reinforcement suggests opposing roles of DA and GABA neurons during reinforcement learning. Specifically, if phasic bursts of SN DA neurons encode reward prediction errors that result in subsequent learning (Glimcher, 2011), and SN GABA neurons provide inhibitory inputs to local DA neurons (Tepper et al., 1995; Luscher and Ungless, 2006; Lobb et al., 2011; Henny et al., 2012; Pan et al., 2013), then one would observe enhanced learning when stimulating DA neurons (Ramayya et al., 2014a), but impaired reinforcement learning following microstimulation of SN GABA neurons. This explanation is also supported by our observation of opposing post-reward firing responses from putative DA and GABA neurons in the human (Ramayya et al., 2014b).
 It is difficult to interpret whether the observed changes reinforcement learning were related to changes in stimulus-reward and/or action-reward learning because these forms of learning were perfectly correlated in the current experimental design. That we did not observe robust stimulation-related changes in learning near putative DA sites is difficult to interpret when considering our previous finding that microstimulation near putative DA neurons enhances action-reward learning (de Berker and Rutledge, 2014; Ramayya et al., 2014a). It is possible that we did not sample from a functional population of DA neurons in this study, as suggested by the absence of phasic post-reward bursts in activity from putative DA neurons in this study, unlike our previous studies (Ramayya et al., 2014a,b). Alternatively, it is possible that stimulation near SN DA neurons has a specific effect on action-reward learning that was not evident in this study because it was masked by simultaneous stimulus-reward learning.
 An alternative explanation for how microstimulation of SN GABA neurons might have resulted in impaired learning is that stimulation may have caused a behavioral change during the post-reward interval that impaired subjects' learning during those trials. Several studies have linked the firing of SN GABA neurons in the pars reticulata subregion (that contains the majority of SN GABA neurons; Nair-Roberts et al., 2008) to regulation of downstream movement and saccade-generating structures (e.g., superior colliculus; Carpenter et al., 1976; DeLong et al., 1983; Hikosaka and Wurtz, 1983). If microstimulation of SN GABA neurons suppressed orienting saccades that likely occurred in response to the presentation of salient reward stimuli (in this case, a silver dollar and the sound of cash register; Hikosaka and Wurtz, 1983), then reward stimuli presented during stimulation trials might be associated with diminished salience and result in reduced learning. However, this is unlikely to be the case because non-human primate studies have shown that SN microstimulation has a limited influence on visually-guided saccades (Mahamed et al., 2011).
 We note several limitations to our study. First, we are unable to provide direct histochemical evidence that electrophysiological parameters (spike rate and waveform duration) indicate distinct neuronal populations, however, a large body of evidence from animal studies suggest that these electrophysiological criteria may be used to identify distinct midbrain neuronal populations (Ungless and Grace, 2012). Second, we did not observe stimulation-related changes in learning near putative DA neurons in this study, whereas we observed such changes in our previous microstimulation study (Ramayya et al., 2014a). This likely reflects reduced sampling of DA neurons during this experiment, which is consistent with the fact that we did not observe post-reward bursts of activity in this study (a marker of DA activity), in contrast to Ramayya et al. (2014a). Finally, the population we studied—patients undergoing DBS surgery for PD—is known to have degeneration of DA neurons in SN. Even though this poses the challenge of interpreting findings concerning the functional role of SN neurons in patients who have degenerative disease, histological studies in PD patients (Damier et al., 1999b), and electrophysiological studies in rat models of PD (Hollerman and Grace, 1990; Zigmond et al., 1990), and humans (Zaghloul et al., 2009; Ramayya et al., 2014b) indicate that a significant population of viable neurons remain in the parkinsonian SN. Taken together with the clear evidence of learning that subjects demonstrated during the task, we suggest that the neural processes we describe reflect the subpopulation of healthy neurons that remain in the SN."
