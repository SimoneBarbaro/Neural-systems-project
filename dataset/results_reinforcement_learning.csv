,doc_id,result_id,text
0,639480,0,Behavioural analyses
1,639480,1,"Experiment 1 \n The experiment was designed to compare the learning and retention of visuomotor rotations under error-based feedback and two types of reinforcement feedback. Figure 2 A shows the time series of reach angles for the different groups. All groups could reach within the target zone during the baseline phase. During the perturbation phase all groups learned and on average were within the final target zone centred on 15°. The open- and closed-loop groups were able to match the desired duration time window for most trials and did not have a significantly different proportion of trials outside the range (invalid trials, 5.7% and 3.7%, respectively, P = 0.18). During retention, when both the cursor and outcome feedback were removed, each group initially maintained their learned reach angle, though the error-based group showed some decay over this period. \n A mixed model ANOVA of reach angle showed no group effect, but a significant effect of experimental phase [ F (4,228) = 79.176, P <  0.001; Greenhouse-Geisser corrected, F (2.267,129.198) = 79.176, P <  \n 0.001] and a significant group — phase interaction [ F (2,228) = 6.849, P <  0.001]. Thus, the three groups responded differently across phase. We used post hoc comparisons to examine the key features of learning and retention. We found differences at baseline with the open-loop group having a significantly more negative reach angle compared to the error-based group ( P <  0.05). In addition the closed-loop group showed significantly more learning in the early perturbation phase compared to the error-based and open-loop groups (both P <  0.001). All groups showed significant learning from baseline to the late perturbation phase (all P <  0.001) and similar reach angles early in retention, but in late retention the error-based group showed significant decay from late perturbation compared to the other groups ( P <  0.001). This suggests that learning with error feedback yielded reduced retention of the learned movement relative to reinforcement feedback. \n Figure 2 B shows total learning (difference between Baseline and End Perturbation) and the per cent early and late retention (measured as the ratio of Early and Late Retention angle to End Perturbation). One-way ANOVA on total learning revealed a significant main effect of group [ F (2,57) = 5.446, P <  0.01], driven by a significant difference between the error-based and open-loop group. This effect was preserved in Brown-Forsythe testing of mean differences, which accounts for unequal variances within groups [ F (2,38.439) = 5.446, P <  0.01]. \n One-way ANOVA showed similar early retention (mean of first 40 retention trials) for all groups [F(2,57) = 2.833,P = 0.067] but a significant difference in late retention [mean of last 40 retention trials F (2,57) = 5.056, P <  0.05] and post hoc test showed this was driven by the decay in the error-based group compared to the open-loop group ( P <  0.05). To examine differences in retention between groups when equated for total learning (and therefore discounting potential effects of the number of valid trials), we selected 10 subjects (of 20) from each of the open-loop and closed-loop groups so as to match their total learning mean and variance to the error-based group. To do this we computed the mean and variance of all possible subgroups of 10 subjects in the open-loop and closed-loop groups, respectively. We chose the subgroup that minimized the Jensen-Shannon divergence between that subgroup and the error-based group’s total learning distributions. Figure 2 C shows the time series of reach angles for these matched subgroups and the error-based group. Group means for total learning were not significantly different following matching, [ F (2,37) = 3.215, P = 0.052; Brown-Forsythe, F (2,17.182) = 2.645, P =  0.10; Fig. 2 D]. Analysis of both early and late retention in the matched subgroups revealed a significant main effect of group [early retention, F(2,37) = 6.573, P <  0.01; late retention,F (2,37) = 8.916, P <  0.01;Fig. 2 D]. The main effect for late retention was preserved following Brown-Forsythe tests [ F (2,19.406) = 9.097, P <  0.01] indicating that the result was robust to unequal variance across groups. In both early and late retention, main effects were driven by significantly reduced retention in the error-based group compared with the two reinforcement-learning groups. Thus, learning with reinforcement feedback yielded enhanced retention of the learned reach angle compared with online error feedback. \n To examine variability in performance we analysed within subject standard deviations in the reach angles of the overall groups over the 40 baseline trials. One-way ANOVA revealed a significant main effect of group [ F (2,57) = 7.251, P <  0.01], which was repeated following Brown-Forsythe testing [ F (2,42.044) = 7.251, P <  0.05]. Post hoc comparisons revealed that the effect resulted from significantly greater within subject reach angle variability in the two reinforcement groups compared to the error-based group (both P <  0.01). The main effect of group was replicated when within subject reach angle standard deviation was analysed in the resampled reinforcement groups matched to the performance of the error-based group [ F (2,37) = 7.496, P < 0.01; Brown-Forsythe, F (2,20.587) = 5.695, P <  0.05]. Overall, these findings suggest that online error feedback reduces behavioural variability compared with binary reinforcement feedback."
2,639480,2," Experiment 2 \n This experiment was designed to compare reinforcement learning and error-based learning in individuals with cerebellar damage and age-matched, healthy controls (older controls). Both groups could, on average, reach within the target zone during the baseline phase of both tasks ( Fig. 3 A and B). By the late perturbation phase, control subjects were on average within the final target zone (centred on 15°) in both tasks. However, the cerebellar group on average only reached the final target in the error-based task. Both groups maintained their learned reach angle throughout retention in the closed-loop task. In the error-based learning task, older control subjects initially maintained their learned reach angle, but their performance decayed over the retention period. In contrast, the cerebellar subjects showed no retention as their performance immediately returned to close to baseline. \n We used a mixed-model ANOVA to examine differences in reach angle between the groups, tasks and phases. We found significant main effects of group [ F (1,21) = 8.6, P <  0.01], task [ F (1,21) = 6.3,P <  .05] and phase [ F (4,84) = 50.1,P < 0.001; Greenhouse-Geisser corrected,F(2.4,49.8) = 50.1,P < 0.001]. The interaction among these factors was also significant [F(4,84) = 4.73,P < 0.01].Post hocanalysis showed that both groups learned from baseline to the late perturbation phase in both tasks (for the cerebellar group in the closed-loop task,P < 0.05, all others, P < 0.001). Total learning was greater for the error-based task compared to the reinforcement task [F(1,21) = 24.7,P < 0.001,Fig. 3C]. Although there were differences in the mean learned reach angle between older control group and cerebellar group in the closed-loop task (P < 0.01), total learning was not significantly different across the groups in either task (P = 0.235). \n In the closed-loop condition, the cerebellar and older control group did not have a significantly different proportion of invalid trials (10.9 and 11.5%, respectively, P = 0.90) whereas the young controls had only 1.7% invalid trials. We used a bootstrap analysis to compare reach angles at the end of the perturbation phase as well total learning, both adjusted for the number of valid trials (see ‘Materials and methods’ section). This showed that the cerebellar group had a significantly smaller reach angle at the end of the perturbation phase compared to the older and younger groups (P < 0.001), but the older group was not significantly different from the younger group (P = 0.24). However, for total learning all groups showed significantly different amounts of learning with the older learning more than the cerebellar group (P = 0.026) and the younger learning more than the older group (P < 0.001). \n In the error-based task, both groups showed a significant decay from late perturbation to early and late retention (older control:  P <  0.001, P < 0.05, respectively; cerebellar: bothP < 0.001). However, the decay in early retention was much greater for the cerebellar group as they returned to near baseline performance ( P < 0.001). \n To take into account the final learning level, we examined the per cent early and late retention ( Fig. 3 D and E). Mixed model ANOVA of per cent early retention revealed a significant main effect of task [ \n F (1,21) = 54.8, P <  0.001] and a significant interaction among factors group and task [ F (1,21) = 15.2,P < 0.01, Fig. 3 D]. Post hoc analysis showed that the cerebellar group had reduced retention in the error-based task relative to the closed-loop task ( P <  0.001) and reduced retention in the error-based task compared to controls ( P < 0.01). ANOVA of per cent late retention revealed a significant main effect of task [ F (1,21) = 11.3, P < 0.01, Fig. 3 E], which was driven by reduced retention in both groups the error-based task compared to the closed-loop task. Together these results suggest that cerebellar patients retained their learning in the closed-loop task similarly to age-matched controls. Conversely, while control participants showed some early retention of the adaptation in the error-based task, the cerebellar group showed no such retention. \n We were surprised that the cerebellar group showed almost complete adaptation in the error-based task, despite no retention. This discrepancy could arise if the cerebellar group relied on online visual feedback to control their reaches. In support of this, examination of the initial portion of the trajectory in the error-based group appears more directed toward the visual location of the target late in learning ( Fig. 4 , middle) and only curves towards the target zone as the movement progresses. In contrast the initial trajectory in the closed-loop task (in which visual feedback cannot be used) is aimed towards the target zone. In early retention ( Fig. 4 , right) the error-based group make movements almost straight to the target suggesting that the curvature seen in the later perturbation movements are driven by visual feedback. In the closed-loop task, however, the cerebellar group maintains some of their initial trajectory deviation in the early retention period. This suggests that the cerebellar group used online feedback to correct their movements in the error-based task resulting in the appearance of complete adaptation, but without any true updating of feedforward models of the correct movement. \n To examine movement variability we analysed baseline within subject reach angle standard deviation for each subject in the two tasks. Mixed model ANOVA revealed a significant main effect of group that was the result of greater reach angle variability in the cerebellar group compared to controls [ F (1,21) = 14.0, P <  0.01]. The group — task interaction was also significant [ F (1,21) = 8.3, P < 0.01]. Within the older control group, reach angle variability was greater in the closed-loop task compared to the error-based task ( P <  0.01). Baseline reach angle standard deviation in the cerebellar group was related to their clinical impairment. The kinetic function sub-score of the International Cooperative Ataxia Rating Scale (ICARS), which is the component of the scale directly related to arm ataxia, was correlated with baseline variability in both the error-based ( r = 0.8, P <  0.01, Fig. 5 A) and closed-loop tasks ( r = 0.6, P <  0.05, Fig. 5 B). No other significant correlations were found between dependent variables and ataxia rating scores."
3,639480,3,"Modelling \n  Many of the cerebellar subjects showed significant learning and retention in the reinforcement condition. Further, we saw individual differences in reinforcement learning within the cerebellar and control groups with some subjects showing substantial learning and others showing little learning. In an attempt to understand these differences, we developed a simple mechanistic model of the reinforcement learning task and fit each subject’s closed-loop data. The model considered the reach angle executed on a given trial to be the result of an internal estimate of the ideal reach angle (i.e. to counter the current rotation applied) with the addition of two sources of variability: motor noise and exploration variability. The important difference between these two sources of variability is that we assume participants are unaware of their motor noise, but have full awareness of their exploration variability. When an action is rewarded, the subject updates their internal estimate of reach angle based solely on the contribution of exploration variability. When an action is unrewarded, the reach angle is not updated. The model has two parameters, the standard deviations of the Gaussian distributions that generate the motor noise and exploration variability. We fit the model to each individual subject’s data using maximum likelihood estimation. \n Three typical subject’s data (a cerebellar, old and young control) are shown along with a typical simulation for each ( \n Fig. 6 \n A–C and parameter estimates, see ‘Materials and methods’ section). The cerebellar subject had the largest motor noise, followed by the older control with the young control having the smallest amount of motor noise. In contrast, the younger control had the largest exploration variability. These parameters led to slower learning for the cerebellar and older control and faster learning for the younger control. Figure 6 D shows that group means match reasonably well to the mean of the simulations (averages of 10 000 simulations with each subjects fitted parameter). \n The model predicts that the amount of learning and reward depends on both the motor noise and exploration variability. Figure 7 A shows the proportion of rewarded trials in late learning from the model for different settings of the motor noise and exploration variability. This shows that performance tends to decrease with motor noise and intermediate values of exploration variability lead to the greatest reward. Therefore for a given level of motor noise there is an optimal level of exploration variability that will lead to maximal adaptation. Achieving a good level of adaptation thus requires a balance between exploration and motor noise. \n Examining the fits across the subjects (Fig. 7 B parameters with 95% confidence intervals) shows that there is wide variation of the parameters but that they tend to cluster by group. In addition there is variability in the per cent learning in the late perturbation period across subjects (percentage learning is shown as fill of the circular symbols in Fig. 7 B). The reason for this is that the model is probabilistic in nature and therefore even with identical parameters for exploration variability and motor noise the actual time series of reaches and adaptation will vary each time the model is run. For example, sometimes the model will be lucky and draw random samples of motor noise with less variability and sometimes the model will be unlucky and draw samples with more variability. We examined this by looking at the distribution of adaptation expected from the model when we performed many thousands of simulations and then determined where a participant’s actual adaptation lies within this distribution. Across all 34 participants we find on average that participants’ adaptation was ranked at 58 (where 1 is the model’s best and 100 the model’s worst performance) and that this rank was not significantly different from the expected value of 50 (P =  0.11). Importantly, even though there was variation in performance predicted by the model, the way in which a participant changed their reach angle as a function of current reach angle and reward (or lack thereof) allowed us to extract model parameters with reasonably narrow confidence limits. \n Analysis of mean parameter values yielded significant differences between groups. One-way ANOVA of motor noise revealed a main effect of group [? \n m \n ; \n F(2,31) = 7.6,P < 0.01; Brown-Forsythe,F(2,19.9) = 7.2,P < 0.01;Fig. 7C] that resulted from significantly greater motor noise in the cerebellar group compared to both young (P < 0.05) and age-matched controls (P < 0.01). One-way ANOVA of exploration variability also revealed an effect of group [?e;F(2,31) = 6.4,P < 0.01; Brown-Forsythe,F(2,15.2) = 5.7,P < 0.05,Fig. 7C], which resulted from significantly greater exploration in young controls compared to both the cerebellar group (P < 0.05) and older controls (P < 0.01). Importantly, one-way ANOVA of the proportion of rewarded trials over the perturbation phase of the experiment revealed a main effect of group [ F (2,31) = 8.232, P <  0.01] where the reward rate was significantly lower for the cerebellar group (0.57 ± 0.04) than both the older (0.73 ± 0.04) and young control (0.76 ± 0.03) groups (both P < .01). \n Although we have phrased the model as motor noise and exploration variability, a mathematically similar way of expressing this is that there is a total variability in a participant’s reach direction and that he or she is only aware of a proportion of this variability and corrects for this proportion when rewarded (see ‘Materials and methods’ section). We replot the parameter estimates as total variance and the proportion corrected for in Fig. 7D. One-way ANOVA of total variance revealed a main effect of group [F(2,31) = 9.9,P < 0.001; Brown-Forsythe,F(2,21.1724) = 10.0,P < 0.01] where both cerebellar (P < 0.001) and young control (P < 0.05) groups showed significantly greater variability than the older control group, but were not different from each other. Analysis of rho showed a significant main effect of group [F(2,31) = 4.8,P < 0.05; Brown-Forsythe,F(2,11.2) = 4.1,P < 0.05] where the cerebellar group was aware of a smaller proportion of their total variability compared with both older and young controls. This comparison was significant between the cerebellar and young control group (P < 0.05)."
4,2436073,0,"Effects of type of reinforcement and exposure to stressors unrelated to the learning task \n In the absence of stressors unrelated to the learning task, the learning performance (number of trials to reach stage 1 and number of stages reached) did not differ between the two types of reinforcement (Kruskal-Wallis trials to reach stage 1: K = 7.07, P = 0.07, Kruskal-Wallis trials to reach stage 1: K = 8.68, P = 0.03, Dunn test PR vs. NR, Ptrials to reach stage 1 = 0.80, Pstages reached = 0.78, Fig 3A and 3B). However, when stressors unrelated to the learning task were added, learning performance significantly decreased with positive reinforcement and tended to decrease with negative reinforcement (Dunn test PR vs. PR+ES: Ptrials to reach stage 1 = 0.03, Pstages reached = 0.04; Dunn test NR vs. NR+ES: Ptrials to reach stage 1 = 0.06, Pstages reached = NS; Fig 3A and 3B). \n  In the absence of stressors unrelated to the learning task, the observed behaviours did not differ between reinforcement types (Table 2, comparison between PR and NR). However, when stressors unrelated to the task were added, horses exhibited significantly more alert postures, startle reactions and glancing at the audience horse under both reinforcements contingencies; in the NR group, they also showed more ears pointing backward and blowing (Table 2, comparison between PR vs. PR+ES or NR vs. NR+ES). No significant differences were observed between the four groups in the number of times the horses sniffed or nibbled the experimenter, explored the environment, defecated or crossed the wooden bar. \n  In the absence of stressors unrelated to the task, the increase in salivary cortisol concentrations did not differ between groups, neither in session 1 nor 3. When stressors unrelated to the task were added, the increase in concentration was significantly higher but only in the case of positive reinforcement (Dunn test PR vs. PR+ES: PSession 1 < 0.05, PSession 3 < 0.01; Table 3)."
5,2436073,1," Relationships between personality and learning performance \n  Horses learning with negative reinforcement without stressors unrelated to the task (NR) that required the shortest time to reach the stage 1 criterion (i.e. the best performers among the NR horses) were the most fearful and active (correlation between the number of trials to reach stage 1 and a) intensity of the startle response during the surprise test: rs = -0.57, P = 0.03; and b) number of areas crossed: rs = -0.61, P = 0.02). \n  Horses learning with negative reinforcement and exposure to extrinsic stress (NR+ES) that reached the highest number of stages (i.e. the best performers among the NR+ES horses) were the least fearful and active (correlations between the number of stages reached and a) number of glancing at the novel area: rs = 0.54, P = 0.04; b) total number of blowing: rs = 0.55, P = 0.03; c) latency before eating during the novel area test: rs = 0.48, P = 0.07 (tendency); d) number of contacts with the novel object: rs = -0.45, P = 0.09 (tendency); and e) number of areas crossed: rs = -0.54, P = 0.04). In addition, the fastest horses to reach the stage 1 criterion in the NR+ES group (the best performers) were also the least fearful, the closest to humans and the most active (correlations between the number of trials to reach the stage 1 criterion and a) number of contacts with the novel object: rs = -0.52, P = 0.04; b) number of contacts with the passive human: rs = -0.57, P = 0.03; and c) number of areas crossed: rs = -0.51, P = 0.05). \n  By contrast, only one negative correlation between a variable related to fearfulness and the level of learning performance was found in each group of horses with positive reinforcement (PR and PR+ES). This correlation indicated that horses reaching the highest number of stages (i.e. the best performers among PR and PR+ES horses respectively) were the least fearful in both groups (correlations between the number of stages reached and a) the number of glancing at the novel area in PR group: rs = -0.51, P = 0.05; and b) the number of glancing at the novel object in PR+ES group: rs = -0.57, P = 0.03)."
6,1649970,0,"Suppression of gene expression by dsRNA injections of OA1, Dop1 and Dop2 in the cricket \n We first studied the effects of double-stranded RNA (dsRNA) injection of the OA1, Dop1 or Dop2 gene on the expression levels of target gene mRNA in the cricket head. As a control, the amount of OA1, Dop1 or Dop2 mRNA in the brains of DsRed2 RNAi crickets was also studied. Groups of crickets were injected with 20?pmol OA1, Dop1, Dop2 or DsRed2 dsRNA into the head hemolymph. Target sequences of RNAi for OA1, Dop1 and Dop2 are shown in Methods and in Fig. 1. The amounts of mRNA in the heads of dsRNA-injected crickets were measured by quantitative real-time PCR (qPCR) at 48?hr after injection, and the relative amount of mRNA transcribed from the target gene was compared to that in intact crickets. The levels of OA1, Dop1 and Dop2 mRNA in DsRed2 RNAi crickets were similar to those in intact crickets (Fig. 2). In contrast, expression levels of OA1, Dop1 and Dop2 mRNA in the heads of OA1, Dop1 and Dop2 RNAi crickets were, on average, 31%, 29% and 25% of those in intact crickets. Statistical comparisons showed that the levels of OA1, Dop1 and Dop2 mRNA in OA1, Dop1 and Dop2 RNAi crickets were significantly lower than those in DsRed2 RNAi crickets (OA1: t?=?3.68, df?=?4.60, p?=?0.017; Dop1: t?=?3.09, df?=?8.26, p?=?0.014; Dop2: t?=?2.31, df?=?15.28, p?=?0.035, two sample t-test with Welch’s correction). All RNAi crickets exhibited normal viability, and their locomotor activities and feeding behaviors were not distinguishable from those of intact animals. Evidence suggesting intact sensory and motor functions of those RNAi crickets is described below. \n Effects of the suppression of OA1, Dop1 and Dop2 genes on acquisition and retention in appetitive conditioning \n  We then studied the effects of gene silencing by RNAi on acquisition and retention performances in appetitive olfactory conditioning. We used conditioning of maxillary palpi extension response (MER) for evaluation of the conditioning effect8. We have reported that presentation of peppermint or apple odor to the antennae of crickets rarely (in less than 20% of crickets) induces MER and that pairing of each odor with water leads to an increase in the probability of MER8. \n  We first studied acquisition performance in appetitive conditioning in groups of crickets injected with dsRNA. Four groups of crickets were injected with 20?pmol dsRNA of OA1, Dop1, Dop2 or DsRed2. Two days after the injection, they were subjected to 4-trial conditioning to associate an odor with water reward with inter-trial intervals of 5?min. The absence or presence of MER to 3-sec odor (CS) presentation prior to presentation of water (US) was recorded (Fig. 3a). \n  All groups exhibited significant increases in the percentages of MER to the CS with increasing the number of conditioning trials (Fig. 4a, Cochran’s Q test: OA1: ?2?=?9.8, p?=?0.02; Dop1: ?2?=?32.3, p?=?0.00000046; Dop2: ?2?=?32.2, p?=?0.00000048; DsRed2: ?2?=?31.9, df ?=?3, p?=?0.00000056). Between-group comparison, however, showed that the percentage of MER to the CS in the OA1 RNAi group was significantly lower than that in the control (DsRed2 RNAi) group in the 4th trial (i.e., after the third training, see Fig. 3) (Fisher’s exact test, adjusted by Holm’s method: p?=?0.012, sample numbers shown in the figure). In contrast, the percentages of MER to the CS in the Dop1 and Dop2 RNAi groups did not significantly differ from that in the DsRed2 RNAi group (Fisher’s exact test, adjusted by Holm’s method: Dop1: p?=?0.781, Dop2: p?=?0.189). The results indicate that silencing of OA1 significantly reduces acquisition in appetitive learning, but that of Dop1 or Dop2 does not. Impairment of appetitive learning by silencing of OA1 is not due to impairment of the perception of water US, because the crickets exhibited MER to a drop of water applied to the mouth or palpi. Moreover, perception of CS and behavioral responses to the CS are intact in the OA1 RNAi crickets, as evidenced by their intact aversive learning described below. \n  Next, retention performance was tested at 30?min after 4-trial appetitive conditioning. The Dop1, Dop2 and DsRed2 RNAi groups exhibited high percentages (more than 70%) of MER to the odor paired with the US (paired odor or CS), and the percentages were significantly greater than those of MER to the odor not used in training (novel odor) (Fig. 4b, McNemar’s test: Dop1: ?2?=?11.1, df?=?1, p?=?0.0000013; Dop2: ?2?=?9.09, p?=?0.0026; DsRed2: ?2?=?19, p?=?0.0000013). In the OA1 RNAi group, on the other hand, percentage of MER to the CS did not significantly differ from that to the novel odor (?2?=?0, p?=?1). Thus, the Dop1 and Dop2 RNAi groups exhibited normal retention of CS-specific appetitive memory, but the OA1 RNAi group did not."
7,1649970,1," Effects of the suppression of OA1, Dop1 and Dop2 genes on acquisition and retention in aversive conditioning \n  In aversive conditioning experiment, we used vanilla or maple odor, to which crickets exhibited high percentages (more than 70%) of MER8. We observed that repeated presentation of these odors alone led to a decrease of MER percentages8, obviously due to habituation. In order to discriminate pairing-specific decrement in percentages of MER due to aversive conditioning from this non-associative effect, we used a differential conditioning procedure in which one of the two odors was paired with 20% sodium chloride solution (paired odor) and the other was presented alone (unpaired control odor) (Fig. 3b)8, and the percentages of MER to the paired and unpaired odors were statistically compared. \n  Four groups of crickets were injected with dsRNA of OA1, Dop1, Dop2 or DsRed2, and two days later they received 4-trial differential aversive conditioning training with 5-min inter-trial intervals (Fig. 3). The percentages of MER to the unpaired odor significantly decreased with the increase of the trial number in the OA1, Dop1 and Dop2 RNAi groups (Cochran’s Q test: OA1: ?2?=?22.6, p?=?0.000050; Dop1: ?2?=?11.6, p?=?0.0088; Dop2: ?2?=?26.6, p?=?0.0000071). DsRed2 RNAi group also showed a decrease of percentages of MER to the unpaired control odor, but the decrease was not statistically significant (?2?=?3.5, df?=?3, p?=?0.317). This was obviously due to a slightly lower percentage of MER in the first trial, which appeared to reflect incidental data variation due to small sample size. \n  Percentages of MER to the paired odor significantly decreased with progress of training in all groups (Fig. 5, Cochran’s Q test: OA1: ?2?=?59.9, p?=?0.00000000000062; Dop1; ?2?=?13.7, p?=?0.0034; Dop2: ?2?=?57.5, p?=?0.0000000000020; DsRed2: ?2?=?24.1, df?=?3, p?=?0.000023). In OA1, Dop2 and DsRed RNAi groups, percentages of MER to the paired odor were significantly lower than those to the unpaired odor in the 3rd trial (McNemar’s test: OA1: ?2?=?4.57, p?=?0.033; Dop2: ?2?=?9.31, p?=?0.0023, DsRed2: ?2?=?4.5, df?=?1, p?=?0.033) and 4th trial (OA1: ?2?=?5.4, p?=?0.020; Dop2: ?2?=?12, p?=?0.00053; DsRed2: ?2?=?6.4, p?=?0.011), indicating that aversive conditioning is successful. In the Dop1 RNAi group, on the other hand, the percentages of MER to the paired odor did not significantly differ from those to the unpaired odor in the 3rd trial (McNemar’s test: ?2?=?0.29, p?=?0.59) and 4th trial (?2?=?0.25, p?=?0.62). The results suggest that silencing of Dop1, but not that of OA1 or Dop2, impairs acquisition in aversive conditioning. \n  Retention performance was tested at 30?min after 4-trial differential aversive conditioning. The OA1, Dop2 and DsRed2 RNAi groups exhibited significantly lower percentages of MER to the CS (paired odor) than to the unpaired odor (Fig. 5, McNemar’s test: OA1: ?2?=?4.5, p?=?0.034; Dop2: ?2?=?7,14, p?=?0.0075; DsRed2: ?2?=?9, df?=?1, p?=?0.0027). In the Dop1 RNAi group, on the other hand, the percentages of MER to the paired odor did not significantly differ from those to the unpaired odor (McNemar’s test, ?2?=?0.29, p?=?0.59). Thus, the OA1 and Dop2 RNAi groups exhibited normal retention of CS-specific aversive memory, but the Dop1 RNAi group did not."
8,1962556,0,"Experiment 1 \n The first experiment examined how adding noise affects reinforcement motor learning in healthy subjects. Figure 3a shows the group mean time series for all three experimental conditions in which no noise, or low or high levels of noise were added (green, yellow, and red curves, respectively). Participants’ average reaches during the baseline phase of each condition, both before and after the addition of noise, were very similar (Fig 3b). This suggests that simply adding noise did not impair their baseline ability to perform the task. \n  When subjects performed the control condition (i.e., with no noise added), they showed strong learning to the initial rotation, followed by a return to baseline, and finally strong learning of the second opposite rotation. In this condition, subjects hit a mean reach angle of 12º (in the direction countering the rotation) across the final 40 trials of each rotation phase (Fig 3b, green). Similar levels of adaptation (?10º) were seen when subjects had a low noise level added (Fig 3b, yellow), although they had a significantly lower reward rate (61% vs 77%). However, in the high-noise condition (Fig. 3b, red), subjects showed significantly reduced adaptation (?5º) and a lower reward rate (46%). \n  We performed a repeated-measures ANOVA for average hand reach angle in the final 40 trials of each block (Fig 3a, BL1, BL2, R1, and R2) and condition. This showed a significant main effect of block (F(2,20) = 47.950, p < 0.001; Greenhouse–Geisser corrected, F(1.248,12.480) = 47.950, p < 0.001) and condition (F(2,20) = 10.402, p = 0.001) as well as a significant block by condition interaction (F(4,40) = 12.349, p < 0.001; Fig. 3b). Post hoc analysis showed no significant differences between the first and second baseline blocks in any of the experimental conditions (all p > 0.05). However, in all three conditions, there was significant adaptation during both the first rotation block (all p < 0.001) and the second rotation block (control and low noise, p < 0.001; high noise, p = 0.028) compared with their respective baselines. In the rotation block, reach angle adaptation in the high-noise condition was significantly reduced compared with both the control (p = 0.002) and low-noise (p = 0.006) conditions. No significant difference was found in learning between the control and low-noise conditions (p = 1.00). Overall, these results suggest that participants were able to use reinforcement feedback to significantly alter their reach angle in all three experimental conditions, but this was reduced in the high-noise condition. \n Figure 3c shows that the reinforcement rate was highest in the control condition and dropped in the low-noise and high-noise conditions (72%, 61%, and 46%, respectively; F(2,30) = 37.405, p < 0.001). Note that the reinforcement rate showed a statistically significant decrease with each level of added noise (all post hoc tests, p < 0.001). \n  To parse the influence of reinforcement feedback on subjects’ behavior in the noise conditions, we computed the absolute change in reach angle as a function of whether a trial was rewarded or not (Pekny et al., 2015) and whether the true reach was in or out of the reward zone. Figure 4 shows that subjects changed their reach angle more following unrewarded trials compared with rewarded trials, regardless of whether they were in or out of zone. The main effect of reward for the low-noise and high-noise conditions were F(1,10) = 29.179, p < 0.001 and F(1,10) = 29.179, p < 0.001, respectively. In the low-noise condition, there was also a main effect of reach accuracy (F(1,10) = 9.299, p = 0.012). Finally, there were significant reward by accuracy interactions in both the low-noise condition (F(1,10) = 7.006, p = 0.024; Fig. 4a) and the high-noise condition (F(1,10) = 14.270, p = 0.004; Fig. 4b)."
9,1962556,1," Experiment 2 \n  The second experiment examined whether slower learning in the high-noise condition of experiment 1 resulted from reinforcing errors by clamping the reinforcement rate at 33.8% (clamp) in a new group of participants. This was done to match the proportion of the trials reinforced in the high-noise task when the hand reach angle was correct. The high-noise and clamp tasks were now similar, except that the latter was not reinforced on any error trial (12% of trials in the high-noise task). To ensure that the reinforcement rate was indeed held at a fixed value throughout the clamp condition, we analyzed the group mean reinforcement rate at the following four phases of the task: the final 40 trials of the second baseline phase; the final 40 trials of the first rotation; the final 40 trials of the return to baseline; and the final 40 trials of the second rotation. Repeated-measures ANOVA showed no significant differences across the phases (F(3,27) = 1.637, p = 0.204). \n  A paired-samples t test showed that the difference in reinforcement rate between the control and clamp conditions was significant (t(9) = 14.1524, p < 0.001; Fig. 5b). Figure 5a shows the group mean time series for the two conditions. The average reach angle during the baseline phase of the clamp condition was similar to that during the control condition, indicating that the reduction in reinforcement rate did not impair participants’ baseline ability to perform the task. We also found that the lower reinforcement rate of the clamp condition did not affect the learning of the initial rotation, return to baseline, or the learning of the second opposite rotation. Repeated-measures ANOVA showed only a significant main effect of block (F(2,18) = 30.734, p < 0.001; Greenhouse–Geisser corrected: F(1.286,11.572) = 30.734, p < 0.001; Fig. 5c). Post hoc analysis showed that the main effect was driven by significant differences between the two baseline blocks and the rotation block (both p < 0.001). There was no significant difference between the first and second baseline block (p = 1.00). Within both the control and clamp conditions, there was significant adaptation in both the first and second rotations (all p < 0.001) compared with both baseline blocks. This suggests that the reduced learning rate in the high-noise group from experiment 1 was due to the 12% of erroneous reaches that were rewarded when ?hand was outside the reward zone. \n  As in experiment 1, we analyzed the absolute change in reach angle as a function of reinforcement feedback. Repeated-measures ANOVA showed only a significant main effect of reward (F(1,9) = 12.490, p = 0.006; Fig. 5d). That is, participants showed a greater change in reach angle following unrewarded trials compared with rewarded trials. There was no effect of condition (F(1,9) = 0.416, p = 0.535) or reward by condition interaction (F(1,9) = 0.469, p = 0.511). This suggests that although participants altered their behavior following unrewarded trials more than following rewarded trials, they did not change their behavior from the control and clamp conditions."
10,1962556,2," Comparison with patients with cerebellar damage \n  We compared subjects’ learning of the first rotation in experiment 1 to the performance of a group of individuals with cerebellar degeneration who learned a single 15º rotation using the same closed-loop reinforcement feedback as part of a previous study (published in Therrien et al., 2016). Figure 6 shows the time series data comparing the performance of the group with cerebellar damage to the groups in control (Fig. 6a), low-noise (Fig. 6b), and high-noise (Fig. 6c) conditions of experiment 1. The group of patients with cerebellar damage was able to reach within the target zone at baseline, but were unable to reach to the new target zone in the rotation block. This differed from the control and low-noise conditions of experiment 1, where participants were able to learn to rotate their reach angles to the new target zone. Similar to the patient group, participants were unable to reach the final target zone in the high-noise condition of experiment 1. However, participants in the high-noise condition still showed a faster learning rate than patients with cerebellar damage. \n  Using independent-samples t tests, we compared the mean values of the group with cerebellar damage for early learning rate, total learning, and reinforcement rate for each condition of experiment 1 (Fig. 6d–f). Participants in the control condition showed significantly greater total learning (t(21) = ?3.2648, p = 0.004), early learning rate (t(21) = 4.3910, p < 0.001), and reinforcement rate (t(21) = ?3.8324, p < 0.001) compared with the group of patients with cerebellar damage. Participants also showed significantly greater total learning (t(21) = ?3.9907, p < 0.001) and early learning rate (t(21) = 5.3784, p < 0.001) in the low-noise condition compared with the patients with cerebellar damage. The difference in reinforcement rate between the low-noise condition and the group of patients with cerebellar damage was not significant (t(21) = ?1.1919, p = 0.247). \n  Finally, comparing the high-noise condition to the patients with cerebellar damage, we found that total learning was not significantly different between groups (t(21) = ?1.861, p = 0.249). However, there were significant differences in early learning rate (t(21) = 3.6563, p = 0.002) and reinforcement rate (t(21) = 2.4292, p = 0.024). The early learning rate was greater for participants in the high-noise condition compared with patients with cerebellar damage. However, patients with cerebellar damage showed a greater reinforcement rate than participants in the high-noise condition. Overall, these results suggest that adding noise to the reaches of neurologically healthy participants’ impaired reinforcement learning in our task, but did not completely replicate the behavior of the patient group."
11,1962556,3," Modeling results \n  The objective of experiment 1 was to test the hypothesis that increased variability from motor noise would impair reinforcement learning in a similar way to patients with cerebellar degeneration. While behavioral data showed similar total learning between the high-noise and patient groups, the high-noise group showed a faster early learning rate. To determine the source of this discrepancy, we modeled subjects learning in the three conditions of experiment 1 using a simple mechanistic model of the reinforcement learning task. The reach angle executed on each trial is modeled as the sum of an internal estimate of the correct reach angle and two sources of behavioral variability, as follows: exploration variability and motor noise. The model assumes that participants have access to the variability from exploration, but do not have access to motor noise. As a result, if a reach is reinforced, the model only updates the internal estimate of the correct movement by the draw of exploration variability on that trial. When a reach is not reinforced, the internal estimate is not updated. The model has three parameters: the SDs of the Gaussian distributions that generate motor noise and exploration variability following rewarded and unrewarded trials. We fit the model to each subject’s data in each condition of experiment 1 as well as each subject’s data in each condition of experiment 2. We also fit the data for each patient with cerebellar damage. The model was fit using maximum-likelihood estimation. \n  Comparing mean parameter values between subjects in experiment 1 and the group of patients with cerebellar damage revealed significant differences in motor noise (Fig. 7a). Patients with cerebellar damage had significantly greater motor noise than participants in the control (t(21) = 3.5707, p = 0.002) and low-noise (t(21) = 3.1929, p = 0.004) conditions of experiment 1. The difference between patients with cerebellar damage and participants in the high-noise condition was not significant (t(21) = 1.3225, p = 0.200), suggesting that this condition effectively matched the motor noise of patients with cerebellar damage. There were no differences between groups across the conditions of experiment 1 for exploration variability following rewarded trials (control condition: t(21) = ?0.6409, p = 0.529; low-noise condition: t(21) = 1.0964, p = 0.285; high-noise condition: t(21) = 1.2381, p = 0.229; Fig. 7b). However, patients with cerebellar damage showed significantly smaller exploration following unrewarded trials compared with participants in all three conditions of experiment 1 (control condition: t(21) = ?2.5702, p = 0.018; low-noise condition: t(21) = ?4.5549, p < 0.001; high-noise condition: t(21) = ?2.8990, p = 0.009; Fig. 7c). Thus, adding noise in experiment 1 did not reduce participants’ exploration following errors relative to the control condition. This left them greater variability from which to learn compared with patients with cerebellar damage. \n  There were no differences in fitted parameter values between the control and clamp conditions of experiment 2 (Fig. 7d: ?m, t(9) = ?1.7678, p = 0.111; Fig. 7e: ?e after rewarded trial, t(9) = ?0.2686, p = 0.794; Fig. 7f: ?e after unrewarded trial, t(9) = ?0.4655, p = 0.653). This suggests that withholding reinforcement of correct movements did not change participants’ motor noise or exploration variability relative to the control condition. \n  To determine the goodness of fit of our model, we compared the mean reaching behavior over subjects in each condition of experiments 1 and 2 to the mean of the model simulations. This resulted in R 2 values of 0.95, 0.91, and 0.66 for the control, low-noise, and high-noise conditions of experiment 1. For experiment 2, R 2 values were 0.87 and 0.76 for the control and clamp conditions, respectively. \n  Finally, to examine the importance of each model parameter we compared the full three-parameter model to two reduced models (one with no motor noise and one with exploration variability) that did not depend on whether the previous trial was rewarded or not (Therrien et al., 2016). We did not examine a model with motor noise only because some exploration variability is needed to show learning. Model comparisons using the BIC showed that the three-parameter model best fit the data for experiments 1 and 2, while the reduced model with exploration that was independent of reward on the previous trial best fit the data for patients with cerebellar damage (Table 1)."
12,440047,0,"We describe our results in three parts. We start with using our network to model a classical reversal learning task. We take advantage of the simplicity of the task to explain the principal ideas behind the network model and why we think the network resembles the OFC. Then we show such a network may be applied to a more complex scenario, both in the task structure and in the temporal dynamics, in which the OFC has been shown to play important roles. Finally, to further illustrate the similarities between our network model and the OFC, we demonstrate how the selectivity of the neurons in the network may resemble experimental findings in the OFC during value-based decision making."
13,440047,1,"Reversal learning \n  In a classical reversal learning task, the animals have to keep track of the reward contingency of two choice options that may be reversed during a test session [9, 28]. Normal animals were found to learn reversals faster and faster, which has been used as an indication of their ability of learning the structure of the task [7]. Such behavior was however found to be impaired in animals with OFC lesions and/or with lesions that contained fibers passing near the OFC [9, 29]. These animals were not able to learn reversals faster and faster when they were repeatedly tested. The learning impairments could be explained by a deficit in acquiring and representing the task structure [7]. \n  Our neural network model consists of a state encoding layer (SEL), which is a reservoir network. It receives three inputs and generates two outputs (Fig 1A). The three inputs from the input layer (IL) to the SEL are the two choice options A and B, together with a reward input that indicates whether the choice yields a reward or not in the current trial. The outputs units in the decision-making output layer (DML) represent choice actions A and B for the next trial. The inputs are provided concurrently and the neural activity of the SEL at the end of the trial is used to determine the SEL’s output (Fig 1B). The connections from the IL to the SEL and the connections within the SEL are fixed. Only the connection weights from the SEL to the DML are modified during the training with a reward dependent Hebbian rule, in which the weight changes are proportional to the reward prediction error and the pre- and post-synaptic neuronal activities. \n  The network is able to reproduce animals’ behavior. The number of the error trials that takes for the network to achieve the performance threshold, which is set at 93% in the initial learning and at 80% in the subsequent reversals, decreases as the network goes through more and more reversals (Fig 1C). Interestingly, a learning deficit similar to that found in OFC-lesion animals is observed if we remove the reward input to the SEL (Fig 1C). As the OFC and its neighboring brain areas such as the ventromedial prefrontal cortex (vmPFC) are known to receive both the sensory inputs and reward inputs from sensory and reward circuitry in the brain [30–32], removing the reward input from our model mimics the situation where the brain has to learn without functional structures in or near the OFC. \n  Neurons in the SEL, as expected from a typical reservoir network, show highly heterogeneous response patterns. Some neurons are found to encode the stimulus identity, some neurons encode reward, and others show mixed tuning (Fig 2A). A principal component analysis (PCA) based on the population activity shows that the network can distinguish all four possible task states: choice A rewarded, choice A not rewarded, choice B rewarded, and choice B not rewarded (Fig 2B and S1 Fig). The first three principal components capture 92.0% variance of the population activity. \n  The ability to distinguish these states is essential for learning. To understand the task acquisition behavior exhibited by our model, we study how neurons with different selectivity contribute to the learning (Fig 2C and S2 Fig). We find that readout weights of the SEL neurons that are selective to the combination of stimulus and reward inputs (e.g. AR and BR) are mostly affected by the learning. The difference between the weights of their connections to the outputs A and B keeps evolving despite repeated reversals. In contrast, the weights of the output connections of pure stimulus-selective neurons (e.g. A and B) only wiggle around the baseline between reversals. Once the network is trained, the expected rewards from AR/BN and BR/AN inputs are exactly the opposite (S3 Fig). \n  The difference between these two groups of neurons explains why our network achieves flexible learning behavior only when the reward input is available. Let us first consider the AR neurons, which are selective for the situation when choice A leads to reward. In these A-rewarded blocks, the connections between the AR neurons and the DML neuron of choice A are strengthened. When the reward contingency is reversed and now choice A leads to no reward, the connections between the AR neurons and choice A are not affected very much. That is because the group of AN and then BR neurons instead of the AR neurons are activated in the blocks when choice A is not rewarded. As the result, the connections between the AN neurons and the DML neuron of choice B are strengthened and the connections between the AN neurons and the DML neuron of choice A are weakened. When the reward contingency is flipped again, the connections between the AR neurons and the DML neuron of choice A are strengthened further. This way, the learning is never erased by the reversals, and the network learns faster and faster. In comparison, let us now consider the A neurons, which encode only the sensory inputs and are activated whenever input A is present. In the A-rewarded blocks, the connections between the A neurons and the DML neuron of choice A are strengthened. In B-rewarded blocks, the connections between the A neurons and the DML neuron of choice A are however weakened when the network chooses A and gets no reward, and the learning in the previous block is reversed. Thus, the output connections of A neurons only fluctuate around the baseline with the reversals. They do not contribute much to the learning, and the overall behavior of the network is mostly driven by neurons that are activated by the combination of reward input and sensory inputs. Removing R deactivates these neurons and leads to the structure agnostic behavior. \n  The importance of the neurons that are selective for the combination of stimulus and reward inputs can be further illustrated by a simulated lesion experiment. After the network is well-trained, we stop the training and test the network’s performance with a proportion of neurons randomly removed at the time of decision (Fig 2D). The neurons that are removed are either 50 randomly chosen neurons, 50 A neurons, or 50 AR neurons. This inactivation happens only at the time of decision making, therefore the state encoding in the reservoir is not affected. The inactivation of AR neurons produces the largest impairment in the network’s performance. Compared to the network with random inactivation, the network with AR-specific inactivation cannot reach the criterion we set previously within a block in more than 50% of the blocks and makes significantly more errors to reach the criterion in the blocks that it does. Inactivation of A-selective neurons produces much smaller performance deficits. \n  It is important to note that although the reinforcement learning algorithm employs the same small learning rate for both the intact network and the “OFC-lesion” network, the former only requires a few number of trials to acquire a reversal in the later stage of training, indicating the reversal behavior may not have to be slow with a small learning rate. In fact, once the network is trained, learning is no longer necessary for the reversal behavior. The network takes very few trials to adapt to reversals without learning (Fig 2E). That is because the association between input AR/BN and decision A and the association between input BR/AN and decision B have been established in the network."
14,440047,2," Two-stage Markov decision task \n  We further test our network with a two-stage decision making task. The task is similar to the Markov decision task used previously in several human fMRI studies and used to study the model-based reinforcement learning behavior in humans [6, 33–36]. In this task, the subjects have to choose between two options A1 and A2. Their choices then lead to two intermediate outcomes B1 and B2 at different but fixed probabilities. The choice of A1 more likely leads to B1, and the choice of A2 is more likely followed by B2. Importantly, the final reward is contingent only on these intermediate outcomes, and the contingency is reversed across blocks (Fig 3A). Thus, the probability of getting a reward is higher for B1 in one block and becomes lower in the next block. The probabilistic association between the initial choices and the intermediate outcomes never changes. The subjects are not informed of the structure of the task, and they have to figure out the best option by tracking not only the reward outcomes but also the intermediate outcomes. \n  We keep our network model mostly the same as in the previous task. Here, we have two additional input units that reflect the intermediate outcomes (Fig 3B). To demonstrate our network model’s capability of encoding sequential events, the input units are activated sequentially in our simulations as they are in the real experiment (Fig 3C). We also add a non-reward input unit whose activity is set to 1 when a reward is not obtained at the end of a trial. The additional non-reward input facilitates learning but does not change the results qualitatively. \n  For a simple temporal difference learning strategy without using any knowledge of task structure, the probability of repeating the previous choice only depends on its reward outcome. The probability of repeating the previous choice is higher when a reward is obtained than when no reward is obtained. The intermediate outcome is ignored. However, this is no longer the case when the task structure is taken into account. For example, consider the situation when the subject initially chooses A1, the intermediate outcome happens to be B2, and a reward is obtained. If the subject understands B2 is an unlikely outcome of choice A1 (rare), but a likely outcome of choice A2 (common), a reward obtained after the rare event B2 should actually motivate the subject to switch from the previous choice A1 and choose A2 the next time. The subject should always choose the option that is more likely to lead to the intermediate outcome that is currently associated with the better reward. \n  To quantify the learning behavior, we first evaluate the impact of the previous trial’s outcome on the current trial. We classify all trial outcomes into four categories: common-rewarded (CR), common-unrewarded (CN), rare-rewarded (RR) and rare-unrewarded (RN). Here, common and rare indicate whether the intermediate outcome is the more likely outcome of the chosen option or not. Glascher et al [6] showed that the model based learning led to a higher probability of repeating the previous choice in the CR and RN conditions. This is also what we observe in our network model’s behavior (Fig 4A). \n  To illustrate how the network acquires the task structure, we define the task-structure index, which represents the tendency of employing task structure information (see the Method). The task-structure index grows larger as the training goes on (Fig 4B). It indicates that the network learns the structure of the task gradually and transits to a more efficient behavior from an initially task-agnostic behavior. \n  Similar to our findings in the first task, the network without the reward input in the SEL behaves in a task-agnostic manner. It does not show the transition that indicates the learning of the task structure (Fig 4B). We further quantify the contribution of task structure information to the network behavior using a model fitting procedure previously described by Glascher et al. [6], and the network without the reward input shows a significantly smaller weight for the usage of task structure, suggesting it is worse at picking up the task structure (Fig 4C and S4 Fig). When the network time constant is sufficiently long, the task-structure dependent behavior is not because the intermediate outcomes occur after the first stage outcomes so that the former having a stronger representation in the network at the time of decision (S5 Fig). \n  Again, a PCA on the SEL population activity shows that the SEL distinguishes different task states (Fig 4D). The first three principal components explain 83.97% variance of the population activity. Because the structure of the task in which the contingency between the first stage options and the intermediate outcomes is fixed, the network only needs to find out the current reward contingency of the intermediate outcomes. We found that the learning picks out the most relevant neurons that encode the contingency between the intermediate outcomes and the reward outcomes (B1R, B2R, etc.). Their connection weights to the DML neurons show better and better differentiation of the two choices throughout the training (Fig 4E). In contrast, the connection weights of the neurons that encode the association between the first stage options and the reward outcomes (A1R, A2R, etc.) are less differentiated. \n  These results suggest that the network acquires the task structure. It understands that the contingency between intermediate outcomes and reward outcomes is the key to the performance. Thus, its choice only depends on the interaction between the intermediate outcome and the reward outcome of the last trial, but not on the other factors (Fig 4F and 4G). The network behavior is similar to the Reward-as-cue agent described by Akam et al. [37]."
15,440047,3," Value representation by the OFC \n  Previous electrophysiology studies have shown that OFC neurons encode value during economic choices [11, 13]. In a series of studies carried out by Padoa-Schioppa and his colleagues, monkeys were required to make choices between two types of juice in different amounts. The monkeys’ choices depended on both their juice preference and the reward magnitude. Recordings in the OFC revealed multiple classes of neurons encoding a variety of information, including the value of individual offers (offer value), the value of the chosen option (chosen value), and the identity of the chosen option (chosen identity) [38, 39]. \n  Here we show that our network model may explain this apparent heterogeneous value encoding in the OFC. We model the two-alternative economic choice task by providing two inputs to the SEL, representing the reward magnitude of each option with range adaption (Fig 5A). The input dynamics are similar to that of the sensory neurons [40]. The network model reproduces the choice behavior of monkeys (Fig 5C)[11]. \n  Then we study the selectivity of the SEL neurons. Just as in the previous experimental findings in the OFC, we find not only neurons that encode the value of each option (offer value neurons, middle panel in Fig 6A), but also neurons that encode the value of the chosen option (chosen value neurons, left panel in Fig 6A). Furthermore, a proportion of neurons show the selectivity for the choice as previously reported (chosen identity neurons, right panel in Fig 6A). We classify the neurons in the reservoir network into 10 categories as described in Padoa-Schioppa and Assad [11]. Interestingly, we are able to find neurons in the reservoir in 9 of the 10 previous described categories (Fig 6B and 6C). The only missing category (neurons encoding other/chosen value) was also rarely found in the experimental data. Although the proportions of neurons encoding each category are not an exact copy of the experimental data, but the similarity is apparent. This is surprising given that we do not tune the internal connections of the SEL to the task. The results are robust across different input connection gains, noise levels in the SEL, and dynamics of the input profiles (S6 Fig). The heterogeneity that is naturally expected from a reservoir network takes much more effort to explain with recurrent network models that have a well-defined structure [40, 41]."
16,2347850,0,"Group demographics and diagnostic information: \n The psychosis group and the control group were matched on age, gender, and NARTestimated verbal IQ (Table 1). When all available information was utilized to apply diagnostic categories after 12months, 81 patients were classified as schizophrenia-spectrum psychosis and 31 as affective psychosis, with missing information on 6 cases. PANSS scores from initial assessment were available on 78 patients."
17,2347850,1,"Learning analysis: stages passed and failed \n  In the control group, 1 participant failed at the IDS stage, 2 failed at the IDR stage, 5 failed at the EDS stage, and 1 failed at the final reversal stage (see figure 2). In patients, 2 failed at \n the compound discrimination stage, 1 at the compound reversal stage, 27 failed at the EDS stage, and 11 failed at the final reversal stage. Thus, in terms of stage failures, significantly more patients failed the EDS stage (? = 16.5, P < .001), and the final reversal \n stage (? = 9.6, P < .01), than controls."
18,2347850,2," Learning analysis: error analysis \n  For a more sensitive analysis, we examined error scores at each stage. We compared all patients (n = 119) vs all controls (n = 107) \n on initial discrimination learning (figure 3). By examining the number of errors using the Mann-Whitney U test, we confirmed that psychosis patients made more errors than controls during initial discrimination learning (z = 2.5, P = .01); in contrast, there were \n no differences between patients and controls on initial reversal learning, compound discrimination learning, ID set shifting, compound reversal, or IDR. However, there were deficits in ED set shifting (z = \n ?5.1, P < .001) and final reversal stages (z = ?3.7, P < .001). Next, we examined the total number of reversal errors over the course of the experiment in participants who attempted all stages of the IDED test: ie, those who completed at least the EDS stage and so could attempt the final reversal stage (99 controls and 89 patients, see table 2 and figure 4). Although some psychosis patients showed good performance, as a group patients made more total reversal errors than controls (z = ?2.4, P = .02). \n  We next examined whether, within the patient group, total reversal errors could be explained, at least in part, by SD errors. Utilizing Poisson regression, we found that SD errors did not predict total reversal errors (z = 0.7, P = .5). In contrast, SD errors were a significant predictor of EDS errors (z = 5.4, P < .001). \n  Having established the differences between first-episode psychosis patients and controls, we proceeded to examine whether patients with schizophrenia-spectrum psychoses differed from patients with affective psychosis. Patients with affective psychosis made fewer ID shifting errors than patients with schizophrenia-spectrum psychosis (z = ?2.3, P = .026), but there were no differences in any other stages of the ID/ED or in total reversal errors (z = ?0.539, P = .6; table 3). \n  Finally, we examined whether symptom scores correlated with performance on simple and reversal trials in the 56 first-episode psychosis patients who completed at least the ED stage of the ID/ED test and for whom PANSS scores were available. Poisson regression revealed that total reversal errors were predicted by negative symptoms (z = 3.72, P < .001), but not positive symptoms (z = ?0.6, P = .6), which is consistent with results from a correlational analysis: total reversal errors correlated significantly with negative symptoms (Spearman ? = 0.3, P = .02) but not with positive symptoms (Spearman ? = 0.2, P = .20). There was no association between SD errors and psychopathology either on correlational analysis (positive symptoms Spearman ? = 0.06, P = .6; negative symptoms Spearman ? = 0.02, P = .98) or on Poisson regression (positive symptoms z = 0.52, P = .6, negative symptoms z = 0.35, P = .7)."
19,1816166,0,"Comparative experiment between rigid criterion and flexible criterion \n According to two different reward settings of rigid and flexible criteria, six paper dataset sequences and ten sequences in the classic Uniref50 database are selected as experimental objects. The known information and test energy information were shown in Table 2. The parameters were set as follows: step-size parameter ??=?0.01, exploration probability ??=?0.5, and learning parameter ??=?0.9. \n  In Table 3, the first four sequences were chosen to compare the performance of reinforcement learning with rigid and flexible criteria. In order to avoid contingency, the rigid and flexible criteria experiments were repeated five times. The number of training iterations per round was set to 5 million, and the test was performed once every 10,000 times. In training process, the number of episodes required to converge to the lowest energy was counted as shown in Table ?Table33. \n  Combination of Tables 2 and ?and33 showed that reinforcement learning with rigid criterion can stably find the lowest energy conformation faster than reinforcement learning with flexible criterion. For the shorter sequences (1 and 2), the number of training episodes required for agent to achieve convergence conformation by flexible criterion was greater than rigid criterion. Reinforcement learning with rigid criterion sampled an average 30,000 and 210,000 episodes to achieve the robust lowest energy conformation, which was 50 and 63% less than 60,000 and 570,000 episodes required by reinforcement learning with rigid criterion. For the longer sequences (3 and 4), reinforcement learning with flexible criterion could not find the lowest energy conformation. One possible reason was that, although flexibility criterion gave a negative reward (or penalty) for states that caused repetition, the states still had some positive Q values, and the Q values of these repeated states in rigid criterion still had an initial value of 0. Therefore, the probability of the repeated states in flexibility criterion being selected was greater than rigid criterion. And as the length of the sequence increased, the number of states that caused repetition in the full state space was also greater, and it was more difficult to find the lowest energy structure."
20,1816166,1," Comparative experiment with greedy algorithm \n  Reinforcement learning with full states using rigid criterion was compared with greedy algorithm. The experimental objects were the twelve sequences in the Uniref50 data set. Similarly, in order to avoid accidentality, two methods were trained for five rounds, and the number of training iterations per round was set to 5 million, and the samples were performed once every 10,000 times. We counted the number of times the lowest energy was obtained in the last 100 samples (Table 4). \n  It can be seen from Table ?Table22 that reinforcement learning with full states using rigid criterion can find the lowest energy for all 16 sequences, but the greedy algorithm can only find 13 of them. From Table ?Table4,4, the training process with 10 sequences was far superior to the greedy algorithm for the above 12 sequences. And the total number of times that the lowest energy was found was 300, which was greater than 205 for the greedy algorithm."
21,1816166,2," Comparative experiment with the reinforcement learning with partial states \n  Reinforcement learning with full states using the rigid criterion was compared with reinforcement learning with partial states. The experimental objects and experimental settings were the same for greedy algorithm above. \n  In the reinforcement learning with partial states, for an HP sequence of length n, its state space S consists of 1?+?4 (n-1) states. Apart from the first amino acid that had only one state, each of the other amino acids had four different actions (up, down, left, and right) to transfer to four different states, so the number of the entire state set was expressed as 1?+?4 (n-1), so S?=?{s1,?s2,?…,?s1?+?4(n???1)}. For example, the state of the first amino acid is s1. In this state, the four actions of up, down, left, and right were respectively transferred to states s2, s3, s4, s5, which were all possible states of the second amino acid. On the same basis, the four actions of up, down, left and right respectively transferred to the states s6, s7, s8 and s9, which were all possible states of the third amino acid, and so on, to find all the states of the subsequent amino acids. \n  In Table ?Table2,2, there were 8 sequences that cannot converge to the lowest energy conformations by the reinforcement learning with partial states, while reinforcement learning with full states successfully folded all sequences to the lowest energy conformations. Table ?Table44 showed that in the last 100 episodes, reinforcement learning with full states hits the lowest energy an average five times, which was 40 and 100% higher than the three and zero times hit by the greedy algorithm and reinforcement learning with partial states, respectively. Reinforcement learning with full states achieved lower energy structures on ten out of twelve sequences than the greedy algorithm."
22,1786347,0,"2.1. Human behavior \n In this task, participants were repeatedly shown a series of colored triangles, red or blue, on a screen, and they were required to press one of two buttons in response to each triangle. They received monetary reward or punishment according to whether this response was correct or wrong. To analyze the behavior, we encoded each response according to underlying behavioral types, where type 1 behavior was to press button one when a red triangle was shown, and button two for blue; type 2 was the opposite of type 1. Over groups of consecutive trials, one response type was rewarded on the majority of trials, controlled by two levels of probability (73% and 83%) known as feedback validity (FV), giving expected uncertainty in the environment. For a set of trials in which response type 1 was rewarded the most, we say that the underlying experimental rule was rule 1, likewise for type 2 responses and rule 2. Stimuli were presented in blocks of 120 trials having constant FV and one of two levels of volatility. High volatility blocks had a rule switch every 30 trials, stable blocks had the same underlying rule for all 120 trials. The participants were not given any information about the generation of rewards or the split of the task into blocks. This data has been examined previously, without considering the fit of different learning models to the behavior (Bland and Schaefer, 2011). Further details of the study can be found in the Materials and Methods section or Bland and Schaefer (2011). \n  Figure ?Figure11 illustrates the study by showing responses made and feedback given for the first 120 trials of the study for four individual participants. Participants were most likely to switch from one response type to the other after negative feedback, that is a loss of points. \n  If the underlying rule can be identified from the pattern of rewards, but the result on individual trials cannot be predicted due to the randomness in reward generation, then to achieve the greatest rewards one should always respond according to the underlying rule, we call this maximizing. So if one knows that type 1 responses are rewarded mostly, then one should make a type 1 response every trial and ignore occasional losses. This does not consider how to identify which response is being rewarded most, or how to identify a rule switch. We quantify participants' behavior by calculating the percentage of trials in which each participant's response is of the type which is associated with the underlying experimental rule. Individual differences in responding to the task gave a range of maximizing behavior from 62% to 89% (mean 74.5%, SD 6%). The overall average feedback validity used in the experiment was 78%, so the average maximizing was just below that. This is in line with many other studies which find that in probabilistic tasks, the average frequency of each response matches the frequency of reward allocation, known as probability matching (see e.g., Vulkan, 2000; Shanks et al., 2002)."
23,1786347,1," 2.2. Computational modeling \n  Reinforcement learning models are based on standard reinforcement learning techniques of making trial by trial adjustments to the predicted value of a particular action, which is a prediction of how much reward is expected from that action. This predicted value is adjusted according to the outcome and a learning rate which controls how much influence the latest outcome has in changing the predicted value of an action. We considered three alternative reinforcement learning models, two of these, standard reinforcement learning model (RL) and win loss modified reinforcement learning model (WL), assume that the environment is fully coupled. This assumes that responses can be viewed in terms of the two response types described above, with the red triangle requiring the opposite response from blue, allowing us to ignore the actual color presented on each trial, and also that exactly one of the responses will be rewarded on each trial. This means that, if feedback shows that one response type is incorrect, then the other response type would have been correct on that trial and vice versa, so regardless of which response is made, feedback lets you know how each response type would have fared. \n  The assumption that the participants expected the environment to be coupled was motivated by the instructions given to participants, but to validate this assumption, we tested an uncoupled reinforcement learning model (UNC) which considers the colors seen and the button presses to be independent of each other and a separate predicted value is maintained for each combination of button and color. Given the assumption of independence, feedback after making a response does not give you any information about the result of pressing the other button or seeing the other color. \n  In our UNC and RL models, one learning rate is assumed for each participant. Our win loss modified reinforcement learning model (WL) allows wins and losses to have different influences on learning by allocating two learning rates to each participant, treating trials following a loss or a win separately. \n  Our Bayesian models are based on hidden Markov models which assume that rewards are governed by a hidden environmental state which cannot be directly observed but can be inferred. In our simple hidden Markov model (HMM), as with Hampton et al. (2006), the hidden state has two possible values, which are equivalent to the two experimental rules. Given the structure of the HMM and the outcomes, combination of response type made and feedback received, Bayesian reasoning is used to determine a probability of reward for each response type. Two sets of probabilities define the structure of the HMM, these are taken to be constant parameters for each participant. These probabilities control the chance of a rule switch and the relation between the hidden state and the reward. \n  Following the work of Behrens et al. (2007), we created a Bayesian model (VOL) which assumes an additional level of structure to the environment, volatility, or how quickly the environment is changing. As with Behrens et al. (2007), a hidden state relates directly to the probability of reward for a particular response, in our case representing the probability of response type 2 being rewarded, without the assumption in the HMM of only two states. This gives a flexible model which can respond to any change in state including changes in feedback validity. Like Behrens et al. (2007) we have assumed that the process for determining the current state and volatility does not vary between participants. \n  In all models, following the calculation of a belief or probability, we apply softmax action selection to determine the probability of making each action on each trial. Softmax action selection assumes that the chosen action depends on the difference between the values associated with each action and on a temperature parameter controlling the amount of randomization of responses on top of underlying beliefs. A low temperature increases the probability of choosing the higher valued action and a high temperature makes the probability of each action more similar. For the RL and UNC models we fit one temperature parameter to each participant's behavior, for the other models we fit two temperature parameters, differentiating trials following wins and losses. \n  Given a set of parameters and a model, we calculate a probability for each action on each trial for the outcomes received by the participant. The natural logarithms of the probabilities for the actions actually taken are summed for each participant. For each model, parameters are fit to each participant's behavior by searching possible values to maximize the likelihood of the parameters over all trials, for more details see Materials and Methods."
24,1786347,2," 2.3. Comparing model fit \n  Models with more parameters should be able to show a closer fit to the data so it is customary to penalize models with more free parameters which have been fit to participants' behavior (Mars et al., 2012). To do this, we compare the five models described above by calculating the commonly used Bayesian Information Criterion (BIC) for each model (Lewandowsky and Farrell, 2011). \n  As a better model has a lower BIC value, Table ?Table11 shows that the WL model gave the best overall fit to the data. We also examined the BIC for each model calculated separately for each participant. The UNC model was the worst fit to behavior compared to the other models for all participants. The best fit model was the WL model for 24 of the 30 participants, for four participants the best fit was the RL model and for two the HMM. Of the 24 participants for whom the WL model was the best fit, 23 had HMM as the next best fit. The differences in the BIC between the WL model and each other model were statistically significant, p < 0.001 in each case, t(29) = 5.05, 4.25, and 7.48 for comparison of WL to RL, HMM, and VOL models, respectively. \n  As described by Lewandowsky and Farrell (2011), we calculated Bayes factors for the difference between the HMM and WL models, we did this using the calculated BIC for each participant. Bayes factors can give an indication of the size of an effect, Lewandowsky and Farrell (2011) report previously proposed guidelines that a Bayes factor above 10 implies strong evidence for one model over the other, and between 3 and 10 implies moderate evidence. Figure ?Figure22 shows the Bayes factors for the WL compared to HMM for all participants. \n  The HMM and WL models fit the participants' behavior better than the other models so we now focus on these two models. Having used all trials to determine the best fit parameters for each participant and model, we could now calculate a trial by trial probability of making a type 2 response. Figure ?Figure33 shows these probabilities for the HMM and WL models for three participants for the first 240 trials of the study. In general, the probabilities match closely, but where there is a difference, the WL model is usually closer to the actual response made by the participant. This follows from the use of log likelihood to find the best model, Figure ?Figure33 is an illustration. \n  As the WL model was the best fit to behavior, we look at the values of the fit parameters. Figure ?Figure44 shows the fit parameters for the WL model, the temperature was significantly higher after a loss than a win, t(29) = 5.61, p < 0.0001 with means of 0.87 and 0.35 after a loss and a win, respectively. According to this model, participants were more likely to randomize their responses after a loss. The fit learning rates were significantly higher after a win than after a loss, means of 0.77 and 0.52, respectively, t(29) = 4.52, p < 0.0001. A lower learning rate after a loss implies that losses are treated less strongly, which allows behavior to respond more slowly to occasional negative feedback and so take advantage of stable periods by not switching to the opposite response type when occasionally losing points when using the most likely response. Finally, we broke down the BIC scores for the WL model between a win and a loss. We find some indications that the WL model fits best after a loss, but this is at the edge of significance (p = 0.054)."
25,1786347,3," 2.4. Parameter recovery \n  If the fit parameters are reliable, we should be able to take simulated data, with known parameters, and accurately estimate those parameters (Lewandowsky and Farrell, 2011). For each model, we chose parameters to represent “typical participants” and used the model's learning rules to generate two sets of simulated responses to each participant's observed outcomes. We used the same process as for the original participant responses to estimate parameters for the simulated responses. \n  Figure ?Figure55 shows that the fit parameters for the WL model are clustered around the parameters used for data generation which are shown by crosses suggesting that the parameters are reliable. \n  The left of Figure ?Figure66 shows the parameters representing probabilities in the structure of the HMM fit to participant behavior and on the right the parameters fit to data generated using the HMM with parameter values shown by crosses. If the participants had understood the experimental generation of outcomes and were applying that knowledge, we would expect the fit parameters to be close to those approximating the generation of data, shown in the left of Figure ?Figure66 by a cross. \n  For the HMM, the spread of fit parameters away from the data generation parameters shows that the parameters are not well recovered. In particular, for several participants the fit value for the error probability was 0.49, that is the probability of losing when using the response type associated with the current rule. Fitting parameters to data generated with this parameter value, the estimated parameter values covered the whole range of feasible values. For the data generated with parameters closer to the actual experimental data, the fit parameters are not so widely spread."
26,1786347,4,"2.5. Model recovery \n  We found that the WL model was the best fit to participant data of the models tested. If model fitting is carried out on simulated data, the best fit model should be that which generated the data. Using the simulated data from the parameter recovery testing described above, we compared the fit of each model as in the analysis of participant data. Table ?Table22 shows the percentage of simulations using each model which were best fit by each model. The correct model has been identified in most cases for all of the models. \n  The largest incorrect identification was the finding that the RL model was the best fit for 18.8% of the simulations by the HMM. The wrongly identified simulations were those which had the parameter for the error probability set to 0.49, and the probability of a switch set to 0.35. This was also the set of parameters which could not be reliably recovered from the simulated data as described above. A simulation using these parameters always gives probabilities close to 0.5 for each response with slight preference in line with the most recent outcome. Reinforcement learning produces responses in line with the most recent outcome by setting the learning rate to one, and the probabilities remain close to 0.5 by setting a high value for temperature. In this way the same behavior can be achieved by the HMM and RL models. Using BIC to compare models, RL will be preferred as the RL model has two parameters compared to four for the HMM."
27,1786347,5," 2.6. How well can these learning methods do? \n  Human behavior was best fit by the WL model, we now consider how the models compare when carried out by an ideal agent. By ideal agent, we mean an agent which always selects the action which the model suggests is most likely to give a reward, and the model parameters are chosen to give the highest number of rewards for the task. We used the sequence of outcomes received by each participant in the task and then considered the performance of each model on each participant's trials. \n  For the RL and WL models, these parameters were found by a grid search over all possible values of the learning rates at intervals of 0.01. For the WL model, a learning rate after a win of 0.48 and after a loss of 0.24 maximized rewards. A learning rate of 0.2 gave maximum rewards for the RL model. The WL model won significantly more rewards than the RL model t(30) = 3.53, p = 0.0014. \n  For the HMM, we searched the parameter space in the region of those parameters approximating the generative environment to find the best performance. The generative environment had equal blocks with FV of 83% and 73%, giving an average probability of 22% of losing when using the response associated with the rule, the error probability. For the probability of a rule switch we used a probability of 0.021 based on 5 switches in 240 trials, having switches after 120 or 30 trials. The parameters which maximized rewards were 0.021 for the switch probability and 0.2 for the error probability. There was no significant difference between the performance of the WL and HMM models t(30) = 1, p = 0.33. The HMM was significantly better than the VOL model, t(30) = 4.98, p < 0.0001. \n  Figure ?Figure77 shows the maximizing behavior, aligned with the experimental rule, of the ideal WL model in comparison to that of the participants. The percentage of responses in line with the underlying experimental rule were averaged over all participants and the ideal WL model for trials following rule switches, with each of the levels of feedback validity (FV) shown separately. The ideal WL has parameter values which optimize behavior over all trials, not just volatile blocks. The ideal WL model far outperforms the participants and reaches a steady level of maximizing at 100% in the high FV condition. \n  As well as being able to outperform humans, the WL model can also closely simulate human behavior. Ten sets of simulated responses were generated using the WL models with the fit parameters and the sequence of outcomes for each individual participant. Figure ?Figure77 shows that the simulations closely replicate the aggregate performance of the participants. Although only volatile blocks are shown, the parameters used in the simulations were those fit to participant behavior across all trials regardless of the experimental conditions. Maximizing behavior of participants and simulations quickly adapts to a rule switch and reaches a plateau which is approximately equal to the level of feedback validity (probability matching). For trials 21–30 following a switch in the high feedback validity (FV = 83%) condition, participants showed maximizing of 82% and the WL simulation 81%. In the low feedback validity condition (FV = 73%), maximizing by participants and the WL model was 73%."
28,2183937,0,"In Experiment 1, rats received excitotoxic lesions of the AcbC or sham lesions, and were then tested on an instrumental free-operant acquisition task with delayed reinforcement (Experiment 1A; see Methods) and subsequently a reinforcer magnitude discrimination task (Experiment 1B). In Experiment 2, na¯ve rats were trained on the free-operant task for delayed reinforcement; AcbC lesions were then made and the rats were retested."
29,2183937,1,"Histology \n  In Experiment 1, there were two postoperative deaths. Histological analysis revealed that the lesions were incomplete or encroached significantly on neighbouring structures in four subjects. These subjects were excluded; final group numbers were therefore 8 (sham, 0 s delay), 6 (AcbC, 0 s delay), 8 (sham, 10 s delay), 7 (AcbC, 10 s delay), 8 (sham, 20 s delay), and 7 (AcbC, 20 s delay). In Experiment 2, one rat spontaneously fell ill with a colonic volvulus during preoperative training and was killed, and there were three postoperative deaths. Lesions were incomplete or too extensive in seven subjects; final group numbers were therefore 7 (sham, 0 s delay), 5 (AcbC, 0 s delay), 8 (sham, 10 s delay), 4 (AcbC, 10 s delay), 8 (sham, 20 s delay), and 5 (AcbC, 20 s delay). \n  Lesions of the AcbC encompassed most of the core subregion; neuronal loss and associated gliosis extended in an anteroposterior direction from approximately 2.7 mm to 0.5 mm anterior to bregma, and did not extend ventrally or caudally into the ventral pallidum or olfactory tubercle. Damage to the ventromedial caudate-putamen was occasionally seen; damage to AcbSh was restricted to the lateral edge of the dorsal shell. Schematics of the lesions are shown in Figure ?Figure2.2. Photomicrographs of one lesion are shown in Figure ?Figure3,3, and are similar to lesions with identical parameters that have been presented before [29,30]."
30,2183937,2," Acquisition of instrumental responding (Experiment 1A) \n  The imposition of response-reinforcer delays retarded the acquisition of free-operant lever pressing, in sham-operated rats and in AcbC-lesioned rats (Figure ?(Figure4).4). AcbC-lesioned rats responded slightly more than shams on both the active and inactive levers in the absence of response-reinforcers delays, but when such delays were present, AcbC lesions retarded acquisition relative to sham-operated controls (Figure ?(Figure55). \n  An overall ANOVA using the model lesion2 — delay3 — (session14 — lever2 — S) revealed multiple significant interactions, including lever — delay — lesion (F2,38 = 5.17, p = .01) and session — lever — delay (F6.0,229.1 = 5.47,  = .464, p < .001), justifying sub-analysis. All six groups learned to respond more on the active lever than the inactive lever (p ? .002, main effect of lever or session — lever interaction for each group alone). \n  For sham-operated rats, delays reduced the rate of acquisition of the active lever response and reduced the asymptotic level of responding attained (Figure ?(Figure4a;4a; delay: F2,21 = 11.7, p < .001;  = .276, p < .001; session — delay: F7.2,75.3 = 2.46,  = .276, p = .024). The presence of a delay also increased responding on the inactive lever slightly (delay: F2,21 = 4.06, p = .032), though not systematically (the 10 s group differed from the 0 s group, p = .036, but no other groups differed, p ? .153). \n  There was a further, delay-dependent impairment in AcbC-lesioned rats, who responded more than shams at 0 s delay but substantially less than shams at 10 s and 20 s delay. As in the case of sham-operated controls, delays reduced the rate of acquisition and the maximum level of responding attained in AcbC-lesioned rats (Figure ?(Figure4b;4b; delay: F2,17 = 54.6, p < .001; delay — session: F6.9,58.7 = 2.64,  = .266, p = .02). Responding on the inactive lever was not significantly affected by the delays (maximum F15.8,134.2 = 1.65,  = .607, p = .066). At 0 s delay, AcbC-lesioned subjects responded more than shams on the active lever (Figure ?(Figure5a;5a; lesion: F1,12 = 5.30, p = .04) and the inactive lever (lesion: F1,12 = 9.12, p = .011). However, at 10 s delay, AcbC-lesioned rats responded significantly less than shams on the active lever (Figure ?(Figure5b;5b; lesion: F1,13 = 9.04, p = .01); there was no difference in responding on the inactive lever (F < 1, NS). At 20 s delay, again, AcbC-lesioned rats responded significantly less than shams on the active lever (Figure ?(Figure5c;5c; lesion: F1,13 = 9.87, p = .008) and there was no difference in responding on the inactive lever (F < 1, NS)."
31,2183937,3," Experienced response-delivery and response-collection delays (Experiment 1A) \n  For every reinforcer delivered, the active lever response most closely preceding it in time was identified, and the time between that response and delivery of the reinforcer (the 'response-delivery delay') was calculated. This time can therefore be equal to or less than the programmed delay, and is only relevant for subjects experiencing non-zero programmed response-reinforcer delays. The response-to-reinforcer-collection ('response-collection') delays were also calculated: for every reinforcer delivered, the response most closely preceding it and the nosepoke most closely following it were identified, and the time between these two events calculated. This time can be shorter or longer than the programmed delay, and is relevant for all subjects. \n  AcbC-lesioned rats experienced the same response-delivery delays as shams when the programmed delay was 10 s, but experienced longer response-delivery delays when the programmed delay was 20 s (Figure ?(Figure6a).6a). Similarly, AcbC-lesioned rats experienced the same response-collection delays as shams when the programmed delay was 0 s, slightly but not significantly longer response-collection delays when the programmed delay was 10 s, and significantly longer response-collection delays when the programmed delay was 20 s (Figure ?(Figure6b).6b). These differences in the mean delay experienced by each rat were reflected in differences in the distribution of response-delivery and response-collection delays when the programmed delay was non-zero (Figure 6c,d). Since AcbC-lesioned rats experienced slightly longer delays than sham-operated rats, it was necessary to take this into account when establishing the effect of delays on learning, as follows."
32,2183937,4," Effect of delays on learning (Experiment 1A) \n  There was a systematic relationship between the acquisition rate and the programmed delay of reinforcement, and this was altered in AcbC-lesioned rats. Figure ?Figure7a7a replots the rates of responding on the active lever on session 10 of acquisition [1]. Despite the comparatively low power of such an analysis, lever-pressing was analysed for this session only using the model lesion2 — delay3. This revealed a significant lesion — delay interaction (F2,38 = 12.6, p < .001), which was analysed further. Increasing delays significantly reduced the rate of responding in this session for shams (F2,21 = 17.3, p < .001) and AcbC-lesioned rats (F2,17 = 54.4, p < .001). AcbC-lesioned rats responded more than shams at zero delay (F1,12 = 8.52, p = .013) but less than shams at 10 s delay (F1,13 = 4.71, p = .049) and at 20 s delay (F1,13 = 17.3, p = .001). \n  Since the AcbC group experienced slightly longer response-delivery and response-collection delays than shams when the programmed delay was non-zero (Figure ?(Figure6),6), it was important to establish whether this effect alone was responsible for the retardation of learning, or whether delays retarded learning in AcbC-lesioned rats over and above any effect to increase the experienced delay. The mean experienced response-collection delay was calculated for each subject, up to and including session 10. The square-root-transformed number of responses on the active lever in session 10 was then analysed using a general linear model of the form lesion2 — experienced delaycov. Unlike a standard analysis of covariance, the factor — covariate interaction term was included in the model. This confirmed that the lesion retarded the acquisition of responding in AcbC-lesioned rats, compared to controls, in a delay-dependent manner, over and above the differences in experienced delay (Figure ?(Figure7b;7b; lesion — experienced delay: F1,40 = 12.4, p = .001)."
33,2183937,5," Experienced delays and learning on the inactive lever (Experiment 1A) \n  No such delay-dependent effects were observed for the inactive lever. Experienced inactive-response-delivery delays (calculated across all sessions in the same manner as for the active lever) were much longer and more variable than corresponding delays for the active lever, because subjects responded on the inactive lever so little. Means ± SEMs were 250 ± 19 s (sham, 0 s), 214 ± 29 s (AcbC, 0 s), 167 ± 23 s (sham, 10 s), 176 ± 33 s (AcbC, 10 s), 229 ± 65 s (sham, 20 s), and 131 ± 37 s (AcbC, 20 s). ANOVA of these data revealed no effects of lesion or programmed delay and no interaction (maximum F1,38 = 1.69, NS). Experienced inactive-response-collection delays were 252 ± 19 s (sham, 0 s), 217 ± 29 s (AcbC, 0 s), 169 ± 23 s (sham, 10 s), 179 ± 33 s (AcbC, 10 s), 231 ± 65 s (sham, 20 s), and 136 ± 37 s (AcbC, 20 s). Again, ANOVA revealed no effects of lesion or programmed delay and no interaction (maximum F1,38 = 1.61, NS). When the square-root-transformed number of responses on the inactive lever in session 10 was analysed with the experienced delays up to that point as a predictor, using the model lesion2 — experienced inactive-response-collection delaycov just as for the active lever analysis, there was no lesion — experienced delay interaction (F < 1, NS)."
34,2183937,6," Discrimination of relative reinforcer magnitude (Experiment 1B) \n  Relative preference for two reinforcers may be inferred from the distribution of responses on concurrent variable interval schedules of reinforcement [31-33]. According to Herrnstein's matching law [31], if subjects respond on two concurrent schedules A and B delivering reinforcement at rates rA and rB respectively, they should allocate their response rates RA and RB such that RA/(RA+RB) = rA/(rA+rB). Overmatching is said to occur if subjects prefer the schedule with the higher reinforcement rate more than predicted by the matching law; undermatching is the opposite. Both sham-operated and AcbC-lesioned rats were sensitive to the distribution of reinforcement that they received on two concurrent random interval (RI) schedules, altering their response allocation accordingly. Subjects preferred the lever on which they received a greater proportion of reinforcement. In general, subjects did not conform to the matching law, but exhibited substantial undermatching; this is common [33]. AcbC-lesioned rats exhibited better matching (less undermatching) than shams (Figure ?(Figure8),8), suggesting that their sensitivity to the relative magnitudes of the two reinforcers was as good as, or better than, shams'. \n  To analyse these data, the proportion of pellets delivered by lever A (see Methods), and the proportion of responses allocated to lever A, were calculated for each subject for the last session in each of the three programmed reinforcement distribution contingencies (session 11, programmed reinforcement proportion 0.5; session 19, programmed proportion 0.8; session 27, programmed proportion 0.2; see Table ?Table1).1). The analysis used a model of the form response proportion = lesion2 — (experienced reinforcer distributioncov — S); the factor — covariate term was included in the model. Analysis of sham and AcbC groups separately demonstrated that both groups altered their response allocation according to the distribution of reinforcement, i.e. that both groups discriminated the two reinforcers on the basis of their magnitude (effects of reinforcer distribution; sham: F1,47 = 16.6, p < .001; AcbC: F1,39 = 97.2, p < .001). There was also a significant lesion — reinforcer distribution interaction (F1,86 = 5.5, p = .021), indicating that the two groups' matching behaviour differed, with the AcbC-lesioned rats showing better sensitivity to the relative reinforcer magnitude than the shams (Figure ?(Figure8).8). These statistical conclusions were not altered by including counterbalancing terms accounting for whether lever A was the left or right lever (the left having been the active lever previously in Experiment 1A), or whether a given rat had been trained with 0, 10, or 20 s delays in Experiment 1A."
35,2183937,7," Switching behaviour during concurrent schedule performance (Experiment 1B) \n  Because switching behaviour has the potential to influence behaviour on concurrent schedules e.g. [34], we also analysed switching probabilities. AcbC-lesioned rats were less likely than shams to switch between levers when responding on two identical concurrent RI schedules with a changeover delay (COD) of 2 s. Responses on the left and right levers were sequenced for sessions 8–11 (concurrent RI-60s schedules, each delivering a one-pellet reinforcer; see Methods and Table ?Table1),1), and the probabilities of switching from one type of response to another, or repeating the same type of response, were calculated. The switch probabilities were analysed by one-way ANOVA; this revealed an effect of lesion (F1,42 = 8.88, p = .005). Mean switch probabilities (± SEMs) were 0.41 ± 0.02 (AcbC) and 0.49 ± 0.01 (sham)."
36,2183937,8," Effects of AcbC lesions on performance of a previously-learned instrumental response for delayed reinforcement (Experiment 2) \n  Due to mechanical faults, data from four subjects in session 10 (preoperative) and data from one subject in session 22 (postoperative) were not collected. Both sessions were removed from analysis completely, and data points for those sessions are plotted using the mean and SEM of the remaining unaffected subjects (but not analysed). \n  Preoperatively, the groups remained matched following later histological selection. Analysis of the last 3 preoperative sessions, using the model lesion intent2 — delay3 — (session3 — lever2 — S), indicated that responding was affected by the delays to reinforcement (delay: F2,31 = 5.46, p = .009; delay — lever: F2,31 = 19.5, p < .001), but there were no differences between the groups due to receive AcbC and sham lesions (terms involving lesion intent: maximum F was for session — lever — lesion intent, F2,62 = 1.844, NS). As expected, delays reduced the rate of responding on the active lever (F2,31 = 15.6, p < .001) and increased responding on the inactive lever (F2,31 = 8.12, p = .001) preoperatively. \n  AcbC lesions selectively impaired performance of instrumental responding only when there was a response-reinforcer delay. There was no effect of the lesion on responding under the 0 s delay condition, but in the presence of delays, AcbC lesions impaired performance on the active lever (Figure ?(Figure9;9; Figure ?Figure10).10). These conclusions were reached statistically as follows. \n  Subjects' responding on the relevant lever in the last preoperative session (session 14) was used as a covariate to increase the power of the analysis [35]. As expected, there were no significant differences in the covariates themselves between groups due to receive AcbC or sham surgery (terms involving lesion intent for the active lever: Fs < 1, NS; for the inactive lever, lesion intent: F1,31 = 2.99, p = .094; lesion intent — delay: F < 1, NS). Analysis of the postoperative sessions, using the model lesion2 — delay3 — (session17 — lever2 — session-14-active-lever-responsescov — S), revealed a near-significant lesion — delay — session — lever interaction (F22.4,335.5 = 1.555,  = .699, p = .054). Furthermore, analysis of postoperative responding on the active lever, using the model lesion2 — delay3 — (session17 — session-14-active-lever-responsescov — S), revealed a session — delay — lesion interaction (F17.3,259.5 = 1.98,  = .541, p = .013) and a delay — lesion interaction (F2,30 = 3.739, p = .036), indicating that the lesion affected responding on the active lever in a delay-dependent manner. In an identical analysis of responding on the inactive lever (using inactive lever responding on session 14 as the covariate), no terms involving lesion were significant (maximum F: lesion, F1,30 = 1.96, p = .172), indicating that the lesion did not affect responding on the inactive lever. \n  Postoperatively, response-reinforcer delays continued systematically to decrease responding on the active lever, both in shams (Figure ?(Figure9a;9a; delay: F2,20 = 11.78, p < .001; session — delay: F12.4,124.1 = 2.36,  = .388, p = .008) and in AcbC-lesioned rats (Figure ?(Figure9b;9b; delay: F2,11 = 13.9, p = .001). Shams continued to discriminate between the active and inactive lever at all delays (lever: all groups p ? .002; lever — session: all groups p ? .003). AcbC-lesioned rats continued to discriminate at 0 s and 10 s (lever: p ? .011; lever — session: p ? .036), but AcbC-lesioned subjects in the 20 s condition failed to discriminate between the active and inactive levers postoperatively (lever: F1,4 = 1.866, p = .244; lever — session: F < 1, NS). \n  Lesioned subjects responded as much as shams at 0 s delay, but substantially less than shams at 10 s and 20 s delay (Figure ?(Figure10).10). Again, analysis was conducted using responding on the relevant lever in session 14 (the last preoperative session) as a covariate. At 0 s, the lesion did not affect responding on the active lever (lesion: F < 1, NS; lesion — session: F16,144 = 1.34, NS). However, at 10 s, AcbC-lesioned rats responded significantly less than shams on the active lever (lesion: F1,9 = 7.08, p = .026; lesion — session: F15.0,135.3 = 3.04,  = .94, p < .001). Similarly, at 20 s, AcbC-lesioned rats responded less than shams on the active lever (lesion: F1,10 = 6.282, p = .031). There were no differences on responding on the inactive lever at any delay (Fs ? 1.31, NS). \n  As in Experiment 1, AcbC-lesioned rats experienced the same response-delivery delays as shams when the programmed delay was 10 s, but experienced longer response-delivery delays when the programmed delay was 20 s (Figure 11a). Similarly, AcbC-lesioned rats experienced the same response-collection delays as shams when the programmed delay was 0 s, slightly but not significantly longer response-collection delays when the programmed delay was 10 s, and significantly longer response-collection delays when the programmed delay was 20 s (Figure 11b)."
37,2183937,9," Relationship between experienced delays and performance (Experiment 2) \n  There was a systematic relationship between the postoperative response rate and the programmed delay of reinforcement, and this was altered in AcbC-lesioned rats. Figure 12a replots the rates of lever-pressing on session 24, the 10th postoperative session (compare Figure ?Figure7).7). An analysis using the model lesion2 — programmed delay3 revealed a significant lesion — delay interaction (F2,31 = 5.09, p = .012). In this session, there was no significant effect of delays on shams' performance (F2,20 = 2.15, p = .143), though there was for AcbC-lesioned rats (F2,11 = 9.01, p = .005). There were no significant differences in responding on this session between shams and AcbC-lesioned rats in the 0 s condition (F1,10 = 3.10, p = .109) or the 10 s condition (F < 1, NS), but AcbC-lesioned rats responded less at 20 s delay (F1,11 = 6.74, p = .025). \n  Since the AcbC group experienced slightly longer response-delivery and response-collection delays than shams when the programmed delay was non-zero (Figure ?(Figure11),11), as before, the rate of responding in session 24 was analysed as a function of the delays experienced postoperatively. The mean experienced response-collection delay was calculated for postoperative sessions up to and including session 24; the square-root-transformed number of lever presses in session 24 was then analysed using a general linear model of the form lesion2 — experienced delaycov, with the factor — covariate interaction term included in the model. This confirmed that the lesion affected responding in AcbC-lesioned rats, compared to controls, in a delay-dependent manner, over and above the postoperative differences in experienced delay (Figure 12b; lesion — experienced delay: F1,33 = 6.53, p = .015)."
38,2183937,10," Locomotor activity and body mass \n  AcbC-lesioned animals were hyperactive compared to sham-operated controls, and gained less mass then shams across the experiments (Figure ?(Figure13),13), consistent with previous results [22,29,36]."
39,908754,0,"We applied intra-operative microstimulation in the SN of eleven patients undergoing DBS for the treatment of PD as they performed a reinforcement learning task (Table ?(Table1).1). Subjects selected between a red and blue card deck by pressing buttons on hand-held controllers and subsequently received positive or negative feedback (Figure ?(Figure1A).1A). The reward probabilities associated with each card deck stochastically fluctuated throughout the intra-operative session to encourage learning (Figure ?(Figure1B,1B, see Section 2)."
40,908754,1," Subjects demonstrated clear evidence of learning on the task. Both during stage 1 and stage 2, subjects showed an increased probability of repeating the same action after receiving positive feedback [“win-stay,” 0.5 expected by chance; t(10) > 5.8, p's < 0.001, Figure ?Figure1C].1C]. Subjects also showed an increased probability of making a high reward probability choice (“accuracy”) during the last 10 trials of a particular reward probability regime, as compared to the first 10 trials after a regime switch [t(10) = 4.35, p = 0.001, Figure ?Figure1D1D]."
41,908754,2," To assess the importance of SN neuronal activity for learning, we applied SN microstimulation following approximately half the reward trials during stage 2 of each subject's intra-operative session. To assess whether SN stimulation had an effect on learning, we compared subjects' win-stay probabilities following reward trials that were accompanied by stimulation (“stim trials”) and stage 2 reward trials during which stimulation was not applied (“control trials”). Across 11 subjects, we observed a trend toward decreased win-stay following stimulation trials compared to control trials [t(10) = 2.03, p = 0.068, Figure ?Figure22]."
42,908754,3," Our main hypothesis was that stimulation-related changes in learning would vary based on the functional properties of neurons near the electrode tip. To assess whether this was the case, we extracted various physiological parameters from neural activity recorded during stage 1 of each subject's intra-operative session (see Section 2). We assessed whether there was a correlation between stimulation-related changes in learning and mean spike rate of units recorded on each channel, and observed a significant negative correlation such that the greatest impairments in learning were observed when the electrode was positioned near neurons with relatively high spike rates (r = ?0.64, p = 0.045, Figure ?Figure3A).3A). Based on the the established finding that high spike rates and narrow waveforms are properties of GABAergic neurons (Ungless and Grace, 2012), we also assessed for a correlation between stimulation-related changes in learning and mean waveform duration. We observed a positive correlation between stimulation-related changes in learning and waveform duration, such that the strongest impairments occurred near neurons with narrow waveforms (r = 0.64, p = 0.044, Figure ?Figure3B).3B). We did not observe a significant relation between stimulation-related changes in learning and phasic post-reward changes in activity (p > 0.5), and generally did not observe post-reward phasic changes in activity (z-score range: ?0.1:0.36). Two example neurons are shown in Figure ?Figure3C3C."
