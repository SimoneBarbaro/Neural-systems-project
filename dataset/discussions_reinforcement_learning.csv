,doc_id,discussion_id,text
0,639480,0,"We examined the learning and retention of a visuomotor rotation using error-based and reinforcement feedback, and whether these mechanisms depend on the integrity of cerebellar function. Reinforcement schedules produced better retention compared with error-based learning. Moreover, using a closed-loop reinforcement schedule, where the reward was contingent on prior performance, produced rapid learning. Cerebellar patients could learn under the closed-loop reinforcement schedule and retained much more of the learned reaching pattern compared to when they performed error-based learning. However, cerebellar patients varied in their learning ability in the reinforcement condition, with some showing only partial learning of the rotation. We developed a computational model of the reinforcement condition and found that learning was dependent on the balance between motor noise and exploration variability, with the patient group having greater motor noise and hence learning less. Our results suggest that cerebellar damage may indirectly impair reinforcement learning by increasing motor noise, but does not interfere with the reinforcement mechanism itself."
1,639480,1,"We based the open-loop task on prior work showing binary reinforcement could drive visuomotor learning in controls (Izawa and Shadmehr, 2011). However, in open-loop paradigms, subjects sometimes lag behind the perturbation to such an extent that they end up receiving no reward and no longer adjust their reaches or explore sufficiently to reacquire the target zone. We, therefore, included a closed-loop condition to mitigate this problem and to ensure that the reward schedule was set as close to 50% in the early period of learning. This was done by rewarding any reach that exceeded the average of the last 10 reaches in the desired direction (i.e. countering the rotation). This closed-loop paradigm led to more rapid learning than the open-loop reinforcement paradigm, with similar final levels and retention across our subjects."
2,639480,2,"While we designed our study to assess reinforcement and error-based learning in isolation, in general these mechanisms will work together in real-world situations. In the reinforcement task participants clearly do not have access to error information, whereas in the error-based task they have both error and task success (reinforcement) information. Nevertheless, our results show clear differences between the two paradigms, which suggests that we are largely studying distinct processes—one that is driven primarily by error that is not well retained and another clearly driven by reinforcement that is well retained. This is consistent with previous studies examining interactions between the two learning mechanisms, showing enhanced retention of an error-based learning process when reinforcement is also provided (Shmuelofet al., 2012;Galeaet al., 2015). In addition, combining error-based and reinforcement learning has been shown to speed up learning (Nikooyan and Ahmed 2015)."
3,639480,3,"Cerebellar patients have known deficits in error-based learning(Martinet al., 1996;Maschkeet al., 2004;Richteret al., 2004;Morton and Bastian, 2006). Yet, the patients in our study were able to follow the gradual perturbation in the error-based condition. Analysis of their reach trajectories suggests that this ability relied on the patients using online visual feedback of the cursor to steer their reaching movements toward the target. As reach angles were calculated using the endpoint of each movement (this was necessary to compare the error-based task to the reinforcement tasks where reaches were rewarded based on movement endpoint), this feedback-dependent compensation made them appear to be learning. However, when visual feedback was removed to assess retention, the ability to correct the movement online was immediately lost (i.e. no retention). Thus, cerebellar patients did not truly adapt to the visuomotor rotation in the error-based task. Consistent with this, is that cerebellar patients cannot follow a gradual rotation when only endpoint cursor feedback is provided (Schlerfet al., 2013)."
4,639480,4,"Reinforcement learning has been posited as a spared mechanism of motor learning following cerebellar damage. Consistent with this,Izawaet al.(2012)showed that cerebellar subjects could counter a gradual visuomotor rotation and generalize the new reach pattern to other targets when they learned with concurrent online visual cursor feedback and binary reinforcement feedback. They suggested that the cerebellar patients were relying on a reinforcement mechanism because they did not change the perceived location of their hand in a proprioceptive recalibration test, whereas control subjects did. Such proprioceptive recalibration is thought to be a hallmark of error-based adaptation (Synofziket al., 2008). Here we specifically show that cerebellar patients can use binary reinforcement feedback alone to alter their reaching movements, although some patients were able to learn more than others. All patients, regardless of how much they learned, showed almost complete retention in the reinforcement task. This is in in stark contrast to the same patients showing a complete lack of retention in the error-based paradigm."
5,639480,5,"The failure of some cerebellar patients to learn in the reinforcement task could be due to either deficits in the reinforcement learning mechanism itself, or deficits in other mechanisms which might limit the usefulness of reinforcement learning (or a combination of the two). Cerebellar damage causes reaching ataxia, indicated by curved and variable reaching movements (Bastianet al., 1996). Patients with cerebellar damage also have proprioceptive impairments during active movements that are consistent with disrupted movement prediction (Bhanpuri et al., 2013). Together these studies suggest that cerebellar damage increases motor noise and/or reduces the sensitivity with which they can locate their own arm. In other words, this leads to variability in reaching that cannot be estimated by the brain. We suspected that such variability (which we consider a form of motor noise) might interfere with reinforcement learning."
6,639480,6,"To test this hypothesis we used a simple model of the reinforcement task in which each subject was characterized by two sources of variability—one that they were unaware of, which we call motor noise, and one that they were aware of, which we term exploration variability. We chose to use a simpler model than the one used byIzawa and Shadmehr (2011), in which reward learning was based on a temporal difference learning algorithm. This algorithm requires specification of a range of parameters a priori (e.g. motor noise, discount rates, motor costs). In the temporal difference rule the current reward is compared to the expected reward to drive learning. However, due to the closed-loop nature of our paradigm, which set the perturbation so that the expected reward rate was always close to 50%, getting a reward was in general better than expected and failing to get a reward worse than expected. Therefore, we simplified the rule to update the motor command only for success and maintained the motor command for failure. This allowed us to avoid setting any parameters a priori and we fit two parameters to characterize subjects’ learning. Moreover, rather than fit squared error we were able to use a full probabilistic model using maximum likelihood, which allowed us to test whether our model was adequate to explain the subjects’ data. The model provided a good fit for the vast majority of subjects and showed that the patients’ had increased motor noise, but similar exploration variability compared to the matched controls. In other words, the majority of variability contributing to cerebellar patients’ behaviour could not be used by the motor system for learning. When reinforcement was received on successful trials, updating of internal estimates of the correct action (i.e. reach angle) was impaired—estimates could only be updated based on a small proportion of the movement variability (corresponding to the exploration component) resulting in less learning. The younger group had similar motor noise to the older control group but had higher exploration variability, which led to improved learning."
7,639480,7,"Previous work has noted that increased motor variability in the rewarded dimension of a reinforcement learning task is associated with more successful performance (Wuet al., 2014). These results suggested that behavioural variability might be a deliberate output of the motor system that is necessary during learning to explore the task space, find the optimal response and yield maximal reward. Our results are in general agreement with these findings as in a reinforcement learning task, exploration variability is essential. In general, for reinforcement learning the optimal amount of exploration variability will depend on the level of motor noise. Therefore, for a fixed level of motor noise subjects should ideally learn to set their exploration variability so as to have maximum adaptation. Although we have phrased the model as exploration variability and motor noise (Fig. 7C), a mathematically similar way of expressing this is that there is a total variability in a participant’s reach direction, but he or she is only aware of a proportion of this variability or only corrects for a proportion when rewarded (Fig. 7D). Under this interpretation of the model, the cerebellar subjects have higher overall variability and correct for less of this variability when they are successful."
8,639480,8," In summary, we have shown that cerebellar patients are able to use binary reinforcement feedback alone to learn and retain a visuomotor reaching task. However, their motor noise interferes with this process. Future work is needed to determine if motor noise can be reduced or if increasing exploration variability can benefit these patients."
9,2436073,0,"Our study shows that exposure to stressors before testing has a negative effect on learning performance, mainly under conditions of positive reinforcement. In addition, learning performance appears to be differentially related to personality according to the type of reinforcement and the presence of extrinsic stress. In the absence of stressors unrelated to the task, the most fearful horses were the best performers when they learned with negative reinforcement but the worst when they learned with positive reinforcement. When stressors unrelated to the task were applied, the most fearful horses were consistently the worst performers, particularly with negative reinforcement learning."
10,2436073,1,"Stressors unrelated to the task impair learning performance according to the type of reinforcement \n  In the absence of stressors unrelated to the task, we did not observe any effect of reinforcement type on learning performance and behavioural or physiological parameters. However, in the presence of stressors unrelated to the task, learning performances decreased or tended to decrease in both groups (NR+ES and PR+ES). Impairment of attention induced by the stressors could be involved [1]. Horses exposed to stressors unrelated to the task displayed more startled reactions and alert behaviours oriented outside of the learning task and directed at the audience horse than horses not exposed to stressors unrelated to the task, suggesting a decrease in attention toward the learning task itself, as observed previously [13]. Interestingly, this learning deficit appeared to be smaller in horses learning with negative reinforcement than in horses learning with positive reinforcement. This result could be explained by the fact that negative reinforcement in itself could be considered a stressor directly related to the cognitive task, known to alleviate learning performance by focusing attention toward the learning task ([1,3], e.g. [25]). This focus of attention toward the task could have counterbalanced the impact of the stressors unrelated to the task in the NR group. In addition, acute stress is known to decrease food motivation [26–28]. An adaptative response to stress consists for an organism to prepare itself to react to this threat, generally based on a fight or flight response. Escaping becomes then the priority other eating. Within a behaviour analytic framework, fear could serve as a potential establishing operation [29], increasing the effectiveness of escaping as a reinforcer. In contrast, that fear could serve as an abolishing operation for food as a reinforcer. Therefore, an impaired motivation for food rewards may also explain why learning performance was less impaired with negative than with positive reinforcement (i.e. food reward). \n  Different patterns of behavioural and physiological responses between PR+ES and NR+ES horses were observed but did not provide an explanation for which group was more stressed. Elevated salivary cortisol levels indicate that the stress level might have been higher in horses learning with positive reinforcement, since PR+ES horses showed increased cortisol levels. However, NR+ES horses but not PR+ES horses showed more ears pointing backward and blowing, which might both indicate discomfort or stress [30–32]. One reason that may explain those differences is the history with escaping a stressor that changes according to the reinforcement type. Indeed, during each learning session, NR+ES experienced several times the avoidance of a negative outcome (i.e. tactile stimulation) by going actively into a safe compartment (i.e. pointed compartment). Experiencing this active escaping repeatedly may help reduce cortisol, whereas PR+ES horses did not experience such active process to cope with stress. However, even though the present results suggest that different patterns of stress response may emerge, they do not allow establishing clearly which reinforcement type was more stressful, and further investigations are needed to improve our knowledge of discomfort and stress measurements in horses."
11,2436073,2," Importance of personality in relationship between stress and learning performance \n  Our study reveals the existence of relationships between personality and learning performance. However, because personality is only one factor among others of predisposition of the learning performances, these relationships are tenuous. That explains why, at a statistical level of 5% and with only 15 animals per group, only a few variables of personality are significantly correlated with learning performance. However, what is interesting is that that these relationships vary with the presence of stressors unrelated to the task and type of reinforcement, especially for the dimension of fearfulness. In the absence of stressors unrelated to the task, the most fearful horses appear to be the best performers with negative reinforcement learning (NR group) but the worst when they had to learn with positive reinforcement learning (PR group). The fact that fearfulness might be advantageous when animals learn with negative reinforcement but disadvantageous when they learn with positive reinforcement is in agreement with previous experiments (active avoidance in guppies [33], instrumental task in horses [22,34,35], instrumental task of discrimination in ravens [36], instrumental cooperative task in rooks [37]). It is possible that the positive effect of negative reinforcement (considered as stressor) on performance, through the focus of attention toward the task, might have been accentuated in fearful individuals and could explain their improved performance in the absence of stressors unrelated to the task. By contrast, there were no stressors related to the learning task in the PR group, and the fearful horses might have been more easily distracted by the external environment. When stressors unrelated to the task were added, learning performance of the most fearful horses appeared to be consistently impaired, independent from the type of reinforcement (PR+ES and NR+ES groups). The disruption of cognitive and attentional processes caused by the stressors unrelated to the task is likely to be more pronounced in fearful individuals who react more strongly to various forms of stressors (e.g. [37], synthesis: [38,39]). \n  Finally, our study indicates that other dimensions of personality may also influence learning performance. Locomotor activity was positively related to performance in negative reinforcement learning, independent from exposure to stressors unrelated to the task. We hypothesize that locomotor activity may broadly reflect a tendency to initiate actions, since we previously observed such a positive effect with learning tasks requiring a displacement of body position similar to the present task [22], but also when requiring different types of movement (touching an object with the nose: [13]). In addition, reactivity to humans was negatively correlated with learning performance in the NR+ES group only. The horses less frightened by humans might have been less affected by the stressors unrelated to the task because they were more attentive to human cues. \n  We have to mention again that with 15 animals per group, only a few significant correlations between variables of personality and learning performance were revealed. To reveal the whole influence of personality on learning abilities, it would be ideal to perform this kind of studies on a larger amount of subjects, maybe several hundreds. Unfortunately, it is quite impossible to test this number of subject, within the equine species. However, we have to underline the fact that, taken together, all the experiments carried out on horses reveal consistent links between personality and learning abilities [10,13,14,22,40]. It would mean that these correlations are not obtained by chance, but more likely reflect a real influence of personality."
12,2436073,3," Conclusion \n  The present study contributes to a better understanding of the influence of stress on learning performance by showing the importance of the nature of the stress (related or unrelated to the task) and personality. Our results also provide important clues to more personalized training according to each animal’s needs. Indeed, the present study shows that positive and negative reinforcement may lead to equivalent learning performance in the absence of stressors unrelated to the task. Considering that previous studies highlighted long-lasting promising effects of positive reinforcement on training, welfare and relationships with humans [32,41–46], our results provide additional evidence for promoting the use of positive reinforcement in horse training, while the use of negative reinforcement still dominates traditional training methods [47,48]. However, the present results also show that the use of food reward as reinforcement may not be adequate in stressful conditions. Indeed, the loss of food motivation induced by stress could render the reinforcement inefficient and constitute an additional source of stress. In addition, our analyses including personality suggest that stress is a key factor in understanding how animals differ in learning performance according to their personality. Predicting how fearful animals react when they face a learning challenge therefore not only requires an evaluation of stress level but also of the nature of the stress (related or unrelated to the task) and their possible interactions. Future investigations are needed to define more precisely when the switch from favourable to unfavourable conditions for learning occurs according to the type and the intensity of a stressor and how it relates to personality."
13,1649970,0,"There has been a discrepancy in the results of studies on neurotransmitters mediating reinforcement signals in crickets and fruit-flies, and the purpose in this study was to fully resolve the discrepancy. Results of studies using transgenic fruit-flies have suggested that different types of dopamine neurons mediate both appetitive and aversive reinforcement signals via the Dop1 receptor12,13,14,15,16,34,35. On the other hand, results of our pharmacological studies have suggested that octopamine and dopamine mediate appetitive and aversive reinforcement signals in crickets3,4,5,6,7,8,36. In accordance with this, moreover, our recent study using Dop1-knockout crickets showed that they are defective in aversive learning but not in appetitive learning19. The results of that study, however, were not conclusive, since the aversive learning defects may be due to defects in the development of brain circuitry."
14,1649970,1," In this study, we observed that OA1, Dop1 and Dop2 RNAi crickets exhibited ca. 70–75% reduction in expression levels of these genes. These results suggest that silencing of the expression of these genes is successful, though we could not determine amounts of these receptor proteins in the brains of dsRNA-injected crickets because antibodies against these receptors were not available. We observed that Dop1-silenced crickets exhibited impairment of aversive learning but not appetitive learning, whereas OA1-silenced crickets exhibited impairment of appetitive learning but not aversive learning. The impairments in RNAi crickets were not due to defects of odor (CS) or water (US) perception or motor function necessary for performance of MER, because the Dop1-silenced crickets exhibited normal appetitive MER conditioning and the OA1-silenced crickets exhibited normal aversive conditioning. It can be argued that different experimental procedures for appetitive and aversive conditioning (absolute or differential conditioning and the use of different odors) might be the reason for the different effects of Dop1 and OA1 gene silencing. However, this is unlikely because we previously showed that effects of dopamine and octopamine receptor antagonists are conserved among experiments with an absolute or differential conditioning procedure and among different kinds of odors used as the CS (see discussion in ref. 8). The consistency between the results of the present gene silencing study with those of the pharmacological and Dop1 gene knockout study using CRISPR/Cas9 (cited above) strongly suggests that dopamine mediates aversive reinforcement via Dop1 receptors and octopamine mediates appetite reinforcement via OA1 receptors in associative learning in crickets."
15,1649970,2," Our observation that silencing of OA1 but not that of Dop1 or Dop2 impairs appetitive learning suggests that impairment of appetitive learning by administration of octopamine receptor antagonists observed in our previous studies3,4,5,6 is due to their effect on OA1 receptor. We observed that epinastine and mianserin impair appetitive learning but not aversive learning and, since these drugs are known as potent antagonists of insect octopamine receptors37,38, we suggested that octopamine mediates appetitive reinforcement but not aversive reinforcement. A recent study in honey bees, however, suggested that these drugs antagonize not only OA1 but also Dop239, which raised the possibility that the impairment might be mediated by blockade of the Dop2 receptor, instead of or in addition to the OA1 receptor. The finding in the present study that silencing of Dop2 does not impair appetitive learning refutes this possibility."
16,1649970,3," Our suggestion that OA1 but not Dop1 mediates appetitive reinforcement with water reward in crickets fundamentally differs from the suggestion based on results of studies using transgenic fruit-flies that Dop1 mediates appetitive reinforcement with sucrose or water reward in addition to aversive reinforcement12,13,14,15,16,34,35. The different conclusions regarding neurotransmitters mediating appetitive reinforcement obtained in studies on crickets and fruit-flies cannot be ascribed to a slight difference in conditioning and testing procedures (see discussion in ref. 8). We thus conclude that neurotransmitters mediating appetitive reinforcement indeed differ in crickets and fruit flies, whereas those for aversive reinforcement are the same."
17,1649970,4," The results of our studies in crickets raise a question about the diversity and evolution of reinforcement systems for associative learning among animals. There is evidence suggesting that dopamine neurons mediate appetitive reinforcement in mammals1,2, mollusks40 and fruit-flies14,15, whereas octopamine neurons have been suggested to mediate appetitive reinforcement in honey bees9,10. The diversity of neurotransmitters mediating appetitive reinforcement among invertebrates and vertebrates should emerge as a fascinating research subject."
18,1649970,5," The fact that appetitively reinforcing neurons in mammals and crickets use different neurotransmitters may indicate that they mediate different information for reinforcement. Results of our recent study, however, suggested that the signals that these neurons mediate are conserved between mammals and crickets. In mammals, it has been shown that whether appetitive learning occurs is determined by the discrepancy, or error, between the predicted reward and the actual reward41 and that certain classes of midbrain dopamine neurons mediate reward prediction errors1,2. In crickets, our behavioral and pharmacological studies suggest that octopamine neurons mediate reward prediction error33. We are currently investigating whether dopamine neurons mediate punishment prediction error in aversive learning in crickets."
19,1649970,6," We found no impairment in appetitive or aversive learning in Dop2 RNAi crickets. Since we did not evaluate to what extent the level of Dop2 protein is reduced in Dop2 RNAi crickets and since Dop2 is known to be expressed in some Kenyon cells in crickets24, more studies are needed to examine the possibility that Dop2 plays some roles in olfactory learning and memory. Insects possess several types of octopamine and dopamine receptors other than Dop1, Dop2 and OA125,26,27. Expression of these genes in the cricket brain and the possible participation of these receptors in learning and memory remain as subjects of our future study."
20,1649970,7," There is dense expression of Dop1 mRNA in Kenyon cells of crickets24. A high expression level of OA1 mRNA in Kenyon cells of the mushroom body has been reported in the fruit-fly31 and honey bee32, and there is an urgent need to confirm this in crickets. The mushroom body is known to play critical roles in olfactory learning in fruit-flies29, honey bees30, cockroaches42 and crickets43. Anatomical and physiological characterization of dopaminergic and octopaminergic neurons that make synapses with Kenyon cells is the next step for elucidation of the neural mechanisms of appetitive and aversive learning in crickets."
21,1649970,8," We conclude that the neurotransmitter and the receptor mediating appetitive reinforcement signals differ in crickets and fruit-flies. This urges us to examine neurotransmitter mechanisms for associative learning in different species of insects to evaluate the diversity, and evolution, of reinforcing mechanisms in insects. Moreover, consideration of the ubiquity and diversity of reinforcing mechanisms for associative learning in vertebrates and invertebrates should become an important research subject in neuroscience."
22,1962556,0,"We examined whether perturbing neurologically healthy individuals by adding noise to their reach endpoints would impair reinforcement learning in a manner similar to what has been observed in individuals with cerebellar damage (Therrien et al., 2016). Adding a low level of noise, to increase participants’ baseline variability by 50%, did not impair learning relative to a control condition where no noise was added. However, adding a high level of noise (to increase baseline variability by 150%) significantly impaired learning. Increasing variability affects the mapping of hand location to reward. That is, in the presence of noise it is possible for the hand to be within the reward zone yet not be rewarded or, conversely, be rewarded when outside it. To assess whether reinforcing errors could account for impaired learning with high noise, we performed an additional experiment in which we artificially reduced (clamped) the reinforcement rate to match the reinforcement corresponding to reaches where both the hand and noisy locations were in the reward zone. In contrast to the noise conditions, in this additional task participants were never rewarded when the hand location was outside the reward zone. Reducing reward yielded learning similar to that in a control condition. Together, these results suggest that the reduced learning in the high-noise condition was driven by the reinforcement of incorrect behavior, rather than not reinforcing correct behavior. Finally, comparing performance in the high-noise condition to that of a group of patients with cerebellar damage showed similar total learning between the groups, but faster early learning in the high-noise condition."
23,1962556,1," Similar to previous work (Pekny et al., 2015), we found a larger change in reach angle following unrewarded trials, suggesting that participants tend to explore more following errors than after successful movements. However, when noise was added, this exploratory behavior was modulated by whether outcome feedback matched the true hand position. That is, participants showed greater change following unrewarded trials when the hand reach angle was outside the reward zone (i.e., an appropriate withholding of reward) compared with when the hand reach angle was actually correct (i.e., a false withholding of reward). In experiment 2, the change in reach angle was also greater for the unrewarded versus rewarded trials. However, there was no difference between control and clamp conditions. This is in contrast to the results of Pekny et al. (2015), who found that clamping reinforcement at a lower level increased variability following unrewarded trials. This discrepancy may have been the result of methodological differences between the two tasks. In their study, Pekny et al. (2015) clamped the reinforcement rate during a prolonged period where no rotation perturbation was applied. Thus, many subjects would have reached a plateau in performance before experiencing the clamp. A sudden reduction in the reward rate under these conditions may have prompted subjects to change their behavior to search for a new solution to the task. In our study, however, the clamp was applied during the rotation phase. Here, subjects would naturally experience changes in the reinforcement rate as the task solution changed with each rotation. As a result, subjects in our study may have been less likely to change their behavior, relative to the control condition, on the introduction of clamp."
24,1962556,2," Adding a high level of noise to reaches of healthy participants matched the total learning of a group of patients with cerebellar damage. However, healthy participants still showed a faster early learning rate than the patient group. To describe how variability from noise influenced learning in our task, we expanded a model developed in our previous work (Therrien et al. (2016). The simple mechanistic model assumes that trial-to-trial variability in subjects’ reach angles stems from two broad sources termed “exploration variability” and “motor noise.” The important distinction between these sources of variability is that the sensorimotor system has access to the amount of exploration on any trial, but it does not have access to the motor noise on that trial. Although the model is framed in terms of motor noise and exploration variability, it is equally valid to view the motor noise as proprioceptive noise (or a combination of both motor and sensory noise), so that this noise limits the ability to localize the limb. As a result, when a reach is reinforced, the motor system can only learn from the magnitude of exploration that contributed to it. Thus, high motor noise may decrease the efficiency of learning by altering the mapping of the reach angle to the reinforcement signal. Here, we allowed exploration to vary depending on whether the previous trial was rewarded or not. Fitting the model to an individual participant’s task performance revealed that added noise increased the fitted motor noise in healthy participants to match that found in patients, but there were group differences in exploration variability. While patients with cerebellar damage showed similar exploration following rewarded trials compared with healthy control subjects with and without added noise, their exploration following unrewarded trials was reduced. This suggests that the patient group was less able to modify their behavior following errors than healthy participants, even when the level of noise was matched between groups."
25,1962556,3," A discrepancy in error sensitivity between our high-noise condition and the patients with cerebellar damage could have arisen for a number of reasons. Studies of visuomotor adaptation have shown that healthy individuals are able to detect false or variable feedback and explicitly alter their behavior so as to learn normally (Bond and Taylor, 2017; Morehead et al., 2017). Added noise in the present study was akin to providing participants with false feedback. Given that they had normal proprioceptive precision, it is possible they were aware of a discrepancy between the movements performed and the feedback received, which may have reduced their sense of agency over feedback about performance errors (Parvin et al., 2018). Furthermore, healthy participants may have been able to use an estimate of the discrepancy to adjust their response to achieve more rewarding feedback. In contrast, pathological motor variability from cerebellar damage is considered to be the product of faulty predictions of limb states (Miall et al., 2007; Miall and King, 2008), which result in poor compensation for limb dynamics and interjoint interaction torques during movement (Bastian et al., 1996; Bhanpuri et al., 2014). Therefore, in patients with cerebellar damage, noise may increase uncertainty about the movement performed—that is, decrease proprioceptive precision (Bhanpuri et al., 2013; Weeks et al., 2017a,b). While the feedback resulting from such movements can also be viewed as false, patients with cerebellar damage are likely to be less able to detect and estimate the discrepancy, making it difficult to detect the source of errors."
26,1962556,4," Previous work has addressed how motor noise can alter learning in a variety of motor tasks. There are several studies of error-based learning that have artificially added noise into various sensorimotor tasks. These have shown that, although performance degrades, participants change their behavior so as to be close to optimal in performance given the noise (Baddeley et al., 2003; TrommershÃ¤user et al., 2005; Chu et al., 2013). Our finding that motor noise can impair motor learning is in agreement with a recent study of reinforcement learning by Chen et al. (2017). The purpose of that study was to understand the similarities between motor reinforcement and decision-making using tasks that were designed to have similar structures. They found that the decision-making task was learned faster and suspected that this was due to the motor noise present in the motor reinforcement task. In a separate experiment, they measured the level of motor noise outside of the reinforcement learning task and showed that the level of noise was inversely related to learning. That is, participants with more noise learned slower. However, they were able to equilibrate performance by artificially adding noise into the decision-making task. This suggested, as in our experiment, that variability from noise limits the ability to learn from reinforcement feedback."
27,1962556,5," In conclusion, we have shown that adding external noise to the movements of neurologically healthy individuals alters reinforcement learning in a motor task. Our findings suggest that high levels of noise primarily impair learning through the attribution of reinforcement to incorrect behavior. Not reinforcing correct behavior did not impair learning in our task, suggesting that it is less detrimental to the motor system. Additionally, adding noise to healthy individuals’ reaches reduced total learning to a level similar to that of a group of patients with cerebellar damage. However, healthy participants showed a faster initial learning rate. We suggest that this may result from a discrepancy between the form of noise in the present study and the source of noise in the patients with cerebellar damage. That is, the added noise in our experiment did not disrupt participants’ estimate of their actual behavior. This left a sufficient proportion variability accessible to the sensorimotor system, which may have supported a faster learning rate."
28,440047,0,"So far, we have shown that a simple reservoir-based network model may acquire task structures. The more interesting question is that why the network is capable of doing so and how this network model may help us to understand the functions of the OFC."
29,440047,1,"Encoding of the task space \n  We place a reservoir network as the centerpiece of our model. Reservoir networks are large, distributed, nonlinear dynamical recurrent neural networks with fixed weights. Because of recurrent networks’ complicated dynamics, they are especially useful in modeling temporal sequences including languages [42, 43]. Neurons in reservoir networks exhibit mixed selectivity that maps inputs into a high dimensional space. Such selectivity has been shown to be crucial in complex cognitive tasks, and experimental works have provided evidence that neurons in the prefrontal cortex exhibit mixed selectivity [44–46]. In our model, the reservoir network encodes the combinations of inputs that constitute the task state space. States are encoded by the activities of the reservoir neurons, and the learned action values are represented by the weights of the readout connections. \n  There are several reasons why we choose reservoir networks to construct our model. First reason is that we would like to pair our network model with reinforcement learning. Reservoir networks have fixed internal connections; the training occurs only at the readout. The number of parameters for training is thus much smaller, which could be important for efficient reinforcement learning. Generality is another benefit offered by reservoir networks. Because the internal connections are fixed, we may use the same network to solve a different problem by just training a different readout. The reservoir can serve as a general-purpose task state representation network layer. Lastly, our results as well as several other studies show that neurons in reservoir networks–even with untrained connections weights–show properties similar to that observed in the real brain [24, 25, 47], suggesting training within the network for specific tasks may not play a role as important as previously thought. \n  The fact that the internal connections are fixed in a reservoir network means that the selectivity of the reservoir neurons is also fixed. This may seem at odds with the experimental findings of many OFC neurons shifting their encodings rapidly during reversals [48]. However, these observations may be interpreted differently when we take into account rewards. The neurons that were found to have different responses during reversals might in fact encode a combination of sensory events and rewards. On the other hand, there is evidence that OFC neurons with inflexible encodings during reversals might be more important for flexible behavior [49]. \n  The choice of a reservoir network as the center piece of task event encoding may appear questionable to some. We do not train the network to learn task event sequences. Instead, we use the dynamic patterns elicited by task event sequences as bases for learning. This approach has obvious weaknesses. One is that the chaotic nature of network dynamics limits how well the task states can be encoded in the network. We have illustrated the network works well for relatively simple tasks. However, when we consider tasks that have many stages or many events, the combination of possible states grows quickly and may exceed the capacity of the network. The fact that we do not train the internal network connections does not help in this regard. However, the purpose of our network model is not to solve very complicated tasks. Instead, we would like to argue this is a more biologically-realistic model than many other recurrent networks. First, it does not depend on supervised learning to learn task event sequences [47, 50]. Second, although the network performance may appear to be limited by task complexity, the real brain, however, also has limited capacity in learning multi-stage tasks [37]. Lastly, we show that a reservoir network can describe OFC neuronal responses during value-based decision making. Several other studies have also shown that reservoir networks may be a useful model of the prefrontal cortex [24, 25]."
30,440047,2," Reward input to the reservoir \n  One key observation is that reward events must also be provided as inputs to the reservoir layer for the network model to perform well. Including reward events allows the network to establish associations between sensory stimuli and rewards, thus facilitates task structure acquisition. Although reward modulates neural activities almost everywhere in the cortex, the OFC plays a central role in establishing the association between sensory stimuli and rewards [9, 48, 51, 52]. Anatomically, The OFC receives visual sensory inputs from inferior temporal and perirhinal cortex, as well as reward information from the brain areas in the reward circuitry, including the amygdala and ventral striatum, allowing it to have the information for establishing the association between visual information and reward [30–32]. Removing the reward input to the reservoir mimics the situation when animals cannot rely on such an association to learn tasks. In this case, the reservoir is still perfectly functional in terms of encoding task events other than rewards. This is similar to the situation when animals have to depend on their other memory structures in the brain–such as hippocampus or other medial temporal lobe structures–for learning. Consistent with this idea, it has been shown both the OFC and the ventral striatum are important for model-based RL [53]. The importance of the reward input to the reservoir explains the key role that the OFC plays in RL. \n  Several recent studies reported that selective lesions in the OFC did not reproduce the behavior deficits in reversal learning previously seen if the fibers passing through or near the OFC were spared [29]. Since these fibers probably carry the reward information from the midbrain areas, these results do not undermine the importance of reward inputs. Presumably, when the lesion is limited to the OFC, the projections that carrying the reward information are still available to or might even be redirected to other neighboring prefrontal structures, including ventromedial prefrontal cortex, which might take over the role of the OFC and contribute to the learning in animals with selective OFC lesions."
31,440047,3," Model-based reinforcement learning \n  The acquisition of task structure is a prerequisite for model-based learning. Therefore, it is interesting to ask whether our network model is able to achieve model-based learning. The two-stage task that we model has been used in human literature to study model-based learning [5, 6, 33–36]. Our model, although exhibiting behavior similar to human subjects, can be categorized as the Reward-as-cue agent that was described and categorized as a form of model-free reinforcement learning agent by Akam et al. [37]. Yet, with reward incorporated as part of the task state space, goal-directed behavior can be achieved by searching in the state space for a task event sequence that ends with the desired goal and associating the sequence with appropriate actions. Thus, our network could in theory support model-based learning by providing the task structure to the downstream network layers."
32,440047,4," Extending the network \n  The performance of our network depends on several factors. First, it is important that reservoir should be able to distinguish between different task states. The number of possible task states may be only 4 or 8 as in our examples, or may be impossibly large even if the number of inputs increases only modestly. The latter is due to the infamous combinatorial explosion problem. One may alleviate the problem by introducing learning in the reservoir to enhance the representation of relevant stimulus combinations and weed out irrelevant ones. A recent study showed that the selectivity pattern in the prefrontal neurons may be better explained by a random network with Hebbian learning [54]. Second, the dynamics of the reservoir should allow information to be maintained long enough in a decipherable form until the decision is made. The recent developed gated recurrent neural networks may provide a solution with units that may maintain information for long periods [55]. Third, the model exhibits substantial variability between runs, suggesting the initialization may impact its performance. Further investigation is needed to make the model more robust. Last, we show that a reinforcement learning algorithm is capable of solving the relatively simple tasks in this study. However, it has been shown that reinforcement learning is in general not very efficient for extracting information from reservoir networks. Especially, when the task demands the information to be held for an extended period, for example, across different trials, the current learning algorithm fails to extract such relevant information from the reservoir. A possible solution is to introduce additional layers to help with the readout [25]."
33,440047,5," Testable predictions \n  Our model makes several testable predictions. First, because of the reservoir structure, the inputs from the same source should be represented evenly in the network. For example, in a visual task, different visual stimuli should be represented at roughly the same strength in the OFC, even if their task relevance may be drastically different. Second, we should be able to find neurons encoding all relevant task parameters in the network, even when a particular combination of task parameters is never experienced by the brain. Third, reducing the number of inputs may make the network to be more efficient in certain tasks. This may seem counter-intuitive. But removing inputs reduces the number of states that the network has to encode, thus improves learning efficiency for tasks that do not require those additional states. For example, if we remove the reward input to the SEL, which is essential for learning tasks with volatile rewards, the network should however be more efficient at learning tasks in a more stable environment. Indeed, animals with OFC lesions were found to perform better than control animals when reward history was not important [56]."
34,440047,6," Summary \n  Our network does not intend to be a complete model of how the OFC works. Instead of creating a complete neural network solution of reinforcement learning or the OFC, which is improbable at the moment, we are aiming at the modest goal of providing a proof of concept that approaches the critical problem of how the brain acquires the task structure with a biologically realistic neural network model. By demonstrating the network’s similarity to the experimental findings in the OFC, our study opens up new possibilities in future investigation."
35,2347850,0,"As expected, we found that psychosis patients had deficits in ED set shifting. This deficit has been previously documented in chronic schizophrenia19,28–30 and in first-episode psychosis,23 although some \n studies in first-episode psychosis suggest that there may either be no deficit in this domain31 or that the deficit may be slight.22,32 Given that a number of previous studies have investigated ED set shifting in schizophrenia and early psychosis, here we focus our \n interpretation on the results concerning simple reinforcement learning and reversal learning."
36,2347850,1," Elliott et al18 and Pantelis et al28 previously demonstrated reversal learning deficits in chronic schizophrenia. Both these groups extracted reversal learning performance from a version of the ID/ED test: the same approach that we employ in this study. Waltz and Gold20 showed profound reversal deficits in 34 patients with chronic schizophrenia using a different method; they employed a probabilistic reversal task adapted from Robbins and colleagues.33,34 Our results demonstrate that reversal learning deficits are also present in many patients near the time of initial presentation to psychiatric services. We note that these deficits were not universal however, and many patients performed at comparable levels to controls (see figure 4)."
37,2347850,2," In contrast to Waltz and Gold,20 who argued that patients performed adequately on rule acquisition, we were able to detect subtle abnormalities in simple reinforcement learning (SD learning), possibly because of our large sample size. Our study thus provides further support to long-held contentions that there are reinforcement learning abnormalities in psychosis. Discrimination learning has been shown to be impaired by caudate tail lesions35; previous data supports caudate dysfunction in psychosis.13,36,37 Specifically, the tail of the caudate is itself connected to the medial temporal lobe, an area that is strongly implicated in the pathogenesis of psychotic illness38 as well as playing a role in discrimination learning. Research in rhesus monkeys has shown that lesions to the medial temporal lobe rhinal cortex, and to the inferior temporal cortex, result in mild and severe deficits, respectively, in discrimination learning, possibly through an inferior temporal-frontal-thalamic network.39–43 Thus, the SD deficit we note is also consistent with previous evidence for disruptedfrontotemporal connectivity in psychosis.44,45"
38,2347850,3," Patients with affective and nonaffective psychosis did not differ significantly in reversal learning errors (or indeed in EDS errors). Previous research has identified deficits in reversal learning to be present in bipolar mania,46 consistent with other recent research implicating orbitofrontal cortex dysfunction in mania (including manic psychosis), such as the presence of impairment on the Iowa Gambling Test.47 Interestingly, reversal learning is intact in euthymic bipolar disorder without a history of psychosis, suggesting a state-dependent deficit in nonpsychotic bipolar disorder.48"
39,2347850,4," Lesion studies in rodents and nonhuman primates have demonstrated a key role for the orbitofrontal cotrex and ventral striatum in reversal learning.49–53 Moreover, this evidence is corroborated from human functional imaging studies33,54,55 and from studies of human patients with orbitofrontal lesions.56,57 These regions are critical for motivational and goal-directed processing10; thus, the present study suggests that there is dysfunction of orbitofrontal/ventral striatal circuitry in psychosis. This contention is consistent with the findings of our correlational analysis in patients, which demonstrates that the greater the reversal impairment, the more severe the negative symptoms (ie, the greater the impairment in motivational and goal-directed behavior). We note that the specificity of this correlation should be viewed with caution because the magnitude of the significant correlation coefficient between negative symptoms and reversal errors (? = 0.3) differed only slightly from the nonsignificant correlation between positive symptoms and reversal errors (? = 0.2)."
40,2347850,5," Interestingly, we found that the patient group made few errors at the compound discrimination stage, which is in contrast with recent results reported by Jazbec et al.29 They studied 34 patients with chronic schizophrenia and found pronounced deficits in compound discrimination. It is possible that this process may deteriorate with disease progression, though longitudinal research will be required to examine this conjecture."
41,2347850,6," Our study does have a number of limitations. Although we found deficits in SD learning, the ID/ED test is not solely or primarily a test of this cognitive domain. Given that the test starts with SD learning, it is conceivable that some psychosis patients might have had trouble adjusting to the task environment in general, leading to an apparent specific deficit in this domain. In addition, there was only  small range in scores in SD learning, which limits the power of correlation and regression analyses to detect associations with clinical variables. For this reason, the failure to detect association between SD errors and clinical variables should not be overinterpreted. SD learning, and its association with clinical variables, merits further investigation in early psychosis in other cognitive paradigms that focus on SD learning in more detail."
42,2347850,7," Another limitation of the current study is that the majority of patients were taking second-generation antipsychotic medications. Such medications act on dopaminergic and serotonergic systems, and ascending serotonin and dopamine neurotransmitter systems are known to play a modulatory role in reinforcement learning processes.51,52,58 There are, however, a number of reasons why our current results are unlikely to be secondary to medication effects. First, we note that in a recent functional magnetic resonance imaging study in healthy volunteers, a low dose of the dopamine D2/D3 receptor ntagonist, sulpiride, did not modulate brain activations during reversal learning or impair behavioral reversal performance.59 Secondly, we observed a correlation between the level of negative symptoms and reversal errors, consistent with the theory that both these measures are secondary to one underlying pathological process. Finally, we have, in recent studies, demonstrated behavioral and physiological abnormalities during tests of reinforcement learning and motivational modulations in unmedicated first-episode psychosis patients.13,15 Future studies should examine reversal learning in unmedicated patients with psychosis, its relation to symptoms, and the extent to which reinforcement and reversal learning deficits can be modulated by pharmacological interventions. The relationship between reinforcement learning and reversal deficits and functional impairments also merits investigation."
43,1816166,0,"Analysis of time complexity and space complexity \n In this algorithm, for one sequence, many iterations of training are required to get its lowest energy. Therefore, the time complexity of the algorithm is determined by the length of the amino acid sequence (N) and the number of training iterations (I), that is, the time complexity is O(N?Ã—?I). The time complexity of the ant colony algorithm for solving HP two-dimensional structure prediction is O(N?Ã—?(N???1)?Ã—?M?Ã—?I/2), where N is the sequence length, I is the number of iterations, and M is the number of ants. The time complexity of particle swarm optimization is O(N?Ã—?I?Ã—?M), where N is the sequence length, I is the number of iterations, and M is the number of particles. Obviously, the time complexity of the method in this paper is the smallest of the three methods, and the larger the sequence length, the more prominent the time advantage. \n  The space complexity is composed of state-transfer function matrix and state-action value matrix. The rows of both matrices represent states, and the columns all represent actions. The number of rows in new state-transfer function matrix is 3N?1?12 and the number of columns is 3. The number of rows in state-action value matrix is 3N?12 and the number of columns is 3. So the space complexity is O3N?1?12Ã—3+3N?12Ã—3."
44,1816166,1," Case study \n  Sequence 12 is a zinc finger protein 528 (fragment), which is a transcription factor with a finger-like domain and plays an important role in gene regulation. Taking sequence 12 as an example, a series of optimized structures with the lowest energy obtained by the method of this paper under rigid criterion are given, as shown in Fig. 2a-c. The results of the last 100 samples of the method and the greedy algorithm and reinforcement learning with partial states in the training exploration process are given, as shown in Fig. 3a-c. The greedy algorithm itself cannot converge, and the convergence of reinforcement learning with full and partial states in the test process is shown in Fig. 4a, b. \n  For reinforcement learning with full states, the agent can be trained to select the better action to obtain a lower energy structure after training for several million times, and then guarantee that the structure obtained after convergence is the optimal structure, and it can be considered that the training effect of reinforcement learning with full states is stable. However, the greedy algorithm is not ideal for training. Only several structures with the lowest energy are trained occasionally, and the accuracy of the lowest energy structure cannot be guaranteed. As a whole, reinforcement learning with full states is better than the greedy algorithm. This is because, for reinforcement learning, the agent can choose better actions based on the previous interaction with the environment during the exploration process. Therefore, as the number of training increases, the agent can select the optimal action more quickly and accurately. Also, because of the setting of the reward function, the agent is more concerned about the overall situation without being trapped in a local optimum. The calculation of each plot in the greedy algorithm is independent, and the previous experience does not help the development of the current plot. As a result, the calculation amount becomes larger and the correct structure cannot be stably obtained. \n  From the testing process, it can be found that reinforcement learning with full states can maintain the lowest energy and achieve stable convergence after reaching the minimum energy. In contrast, reinforcement learning with partial states has fluctuations, cannot be stably maintained, and cannot reach the convergence state. This is because each state in the full state space is uniquely determined and can only be transferred by a unique state-action pair, and the process has Markov properties. However, the state in the partial state space can be transferred by different state-action pairs, which has partial uncertainty."
45,1816166,2," Full state space compares to partial state space \n  The full state space and the partial state space are two different descriptions of the state space in the 2D-HP model under reinforcement learning framework. The same point of the full and partial state spaces is that different states corresponding to each amino acid are set in advance, but they differ in the rules of the state setting. For the full state space, the number of states of subsequent amino acids is always three times the number of previous amino acid states. The state of the subsequent amino acid is obtained by a specific action of the previous amino acid in a specific state. That is to say, each state is transferred by a certain state-action pair, and the whole process has Markov properties. For the partial state space, the number of states for each amino acid except the first amino acid is four. The four states of the subsequent amino acid can be transferred from the four states of the previous amino acid through four different actions, and the whole process does not have Markov properties. The advantage of the full state space is that it can accurately find the lowest energy of the sequence and stabilize the convergence. The disadvantage is that the state space dimension is too high and the memory requirement is high, and the sequence with long length cannot be calculated. The advantage of partial state space is that the required state space is small, and it is possible to calculate a sequence with a long length. The disadvantage is that it cannot converge and cannot find the lowest energy of the sequence. \n  Function approximation is especially suitable for solving problems with large state space. The method described above for pre-setting the state-action value matrix and updating the state-action value matrix during the training process takes up a large amount of memory. Function approximation can be used to map the state-action value matrix to an approximation function (such as a parameter approximation function). Updating the parameter values with experimental data during the training process is equivalent to updating the state-action value, and finally a suitable approximation function is obtained. It can save memory space and solve the problem of sequence length limitation."
46,1786347,0,"We find that a reinforcement learning model with separate parameters after a win or a loss (WL) gives a significantly better description of human behavior in a two-alternative probabilistic learning task with rule reversals than the other models tested. Our VOL model, implementing the model of Behrens et al. (2007), is flexible and able to adapt to changes in the level of expected uncertainty, or feedback validity (FV), and volatility. However, the WL model is a better fit than the others although it has constant parameters throughout all trials. Behrens et al. (2007), in a broadly similar decision making task, found their model to be a better fit to human behavior than reinforcement learning with constant parameters for each participant or separate parameters for volatile and stable blocks. The difference between the fit of our WL model and standard reinforcement learning (RL) applies even when they are compared using a method that penalizes the WL model. In particular, the advantage of the WL model was observed when the BIC of the models was compared, a method which strongly penalizes models with higher numbers of parameters such as our WL model (Lewandowsky and Farrell, 2011)."
47,1786347,1," Comparing the performance of ideal agents on our task, we find no significant difference between the HMM and WL models. Ideal agents have parameters which are chosen to maximize rewards given the model and always choose the option given by the model as the most favorable. Bayesian models are constructed to make optimal decisions, providing that the assumptions underlying the models are correct. Although the assumptions of the Bayesian models are based on the experimental structure used to generate rewards, the HMM or VOL models do not outperform the WL model on our task although the WL model does not adjust its learning rate to accommodate different levels of unexpected uncertainty and volatility. There is a small but significant improvement in performance of the WL model compared to the RL model when using ideal agents. All of the models, when used by ideal agents, far outperform human behavior."
48,1786347,2," In our WL model, we find that for both the learning rate and temperature parameters, there is a significant difference between the fit parameter values following a win and a loss. These differences are consistent with psychological studies reporting behavioral differences in response to wins and losses and with existing neuroscientific knowledge indicating the existence of different neural pathways linked to the processing of wins and losses (see e.g., Kravitz et al., 2012; Yechiam and Hochman, 2013b)."
49,1786347,3," In psychology, the concept of loss-aversion, which suggests that behavior changes more in response to losses than to gains of similar magnitude (Kahneman and Tversky, 1984) has prompted much investigation. As an alternative mechanism to loss-aversion, Yechiam and Hochman (2013b) proposed a loss-attention mechanism in which losses cause participants to attend more closely to a task and so losses decrease the amount of randomization."
50,1786347,4," These ideas of loss-aversion are often tested in studies of response to risk, that is where participants choose between alternatives with known outcome probabilities. An example (from Kahneman and Tversky, 1984) is a choice between a safe or risky option, where the risky option has an 85% chance of winning $1000 and a 15% chance of winning nothing and the safe option pays out $800 with certainty. People tend to prefer the safe option. In their examination of the loss-attention hypothesis, Yechiam and Hochman (2013a), used several tasks which involved repeated selections between a safe and a risky option where the probabilities had to be learnt from experience. They tested their loss-attention model by fitting a choice sensitivity parameter, the inverse of our temperature parameter, for each task. They found less randomization of responses in tasks in which losses were possible compared to tasks without losses. In our task, unlike that of Yechiam and Hochman (2013a), the participants could not avoid losses as there was no way to predict the outcome on individual trials. We find a higher temperature after individual losses, implying that participants are less likely to follow the underlying belief after a loss. This does not necessarily conflict with the idea of loss-attention, as adding randomness to a response after a loss may be a mechanism for testing an underlying belief without making a large adjustment to that belief."
51,1786347,5," In neuroscience, dopamine is related to reward and punishment and separate D1 and D2 dopamine receptors in the basal ganglia have been found to respond to reward and punishment, respectively (see e.g., Gerfen, 1992). This inspired the use of separate pathways to respond to reward and punishment in the computational neural models of reinforcement learning by Frank and colleagues (see e.g., Frank, 2005; Samson et al., 2010). Testing this, Kravitz et al. (2012) found different pathways in the striatum of mice to be involved in processing reward and punishments. Rather than indicating reward and punishment directly, Schultz and colleagues suggested that dopamine signals the difference between an expected reward and that actually received (see e.g., Schultz, 1998). This difference forms the prediction error which is calculated in reinforcement learning."
52,1786347,6," Following their neural models with separate pathways for learning after a win and a loss, (see e.g., Frank, 2005; Samson et al., 2010). Frank et al. (2007) use separate learning rate parameters following a win and a loss when using a reinforcement learning model to analyze human behavior in a probabilistic task. Like us, they find that the mean learning rate following a win is higher than that after a loss. They use just one temperature parameter and, as they are looking at associations between genetics and reinforcement learning parameters, they do not compare alternative models of behavior."
53,1786347,7," We are aware of only a few studies that have considered separate effects of reward and punishment when comparing alternative computational models of learning from experience, none of which compare Bayesian models which make assumptions about the nature of the environment. These studies are based on different learning tasks to ours, and fit different reward values following a win or a loss (e.g., Ito and Doya, 2009; Guitart-Masip et al., 2012). Guitart-Masip et al. (2012) had four fractal images which signalled whether participants should respond or not to gain rewards or avoid punishments, these associations had to be learnt from experience and there was no switch in associations. Guitart-Masip et al. (2012) fit a number of different reinforcement learning models to behavior, the best fit model did not scale rewards and punishments differently. Analyzing the decisions of rats in two-stage probabilistic decisions, Ito and Doya (2009) found that a reinforcement learning model with different reward values after a win and a loss was a better fit to the rats' behavior than reinforcement learning without differentiation between wins and losses. To maintain the symmetry of the task in which exactly one response is correct on each trial, we have taken a different approach and fit a separate learning rate, rather than reward value, following wins and losses."
54,1786347,8," As our Bayesian and reinforcement learning based models make different assumptions about the environment, comparing the fit of different models to human behavior can give insights into the assumptions people make about the environment. Our HMM, as with that of Hampton et al. (2006), as well as assuming that the two outcomes are coupled, assumes that there will be rule switches within probabilistic feedback. Our VOL model not only assumes that there will be rule switches, but also that the frequency of switches depends on the level of volatility in the environment. Hampton et al. (2006) compared a hidden Markov model to a reinforcement learning model that made no assumptions about the structure of the environment. They concluded that participants make assumptions about the structure of the environment. We also found that our hidden Markov model (HMM) was a better fit to behavior than a reinforcement learning model which did not assume that the outcomes were coupled. This uncoupled reinforcement learning model, however, was not as good a fit as our RL and WL models. From this we conclude that participants made some assumptions about the environment but have no evidence that they adjusted their rate of learning to the structure of the environment."
55,1786347,9," Bayesian models can optimize the number of rewards received when the assumed structure for the Bayesian inference exactly matches the underlying structure of the task. We examined the performance of our models when, rather than being fit to human behavior, the model parameters were selected to maximize the total reward achieved, we refer to this as an ideal agent using the model. In our task, the rewards obtained by an ideal agent using the WL model were not significantly different to those of the ideal HMM. Our ideal HMM has parameters to closely resemble the generative structure, but assumes that there is a small constant probability of a rule switch. In the experimental data, rule switches only occurred at the ends of blocks of 30 or 120 trials. The HMM also assumes that for each environmental state, there is a constant probability of each outcome. However, the experimental data was generated using two levels of FV, with outcomes randomized to give the correct proportion within a block. We do not believe that these differences between the generative process and the assumptions of the HMM significantly hamper the performance of the HMM. We believe that the ideal agent using the WL model is approaching an optimal level of response in this task. The ideal WL model had a small but significant advantage over the ideal RL model in our task. Our ideal HMM also performed significantly better than our VOL model, our implementation of the model of Behrens et al. (2007). The VOL model is more flexible in the situations in which it can learn."
56,1786347,10," The parameters for the ideal WL model, those which gave the best performance in the task, were learning rates of 0.48 after a win and 0.24 after a loss. Our participants also had significantly higher learning rates after a win than a loss, although generally higher than the ideal parameters, with means of 0.76 and 0.52 after a win and a loss, respectively."
57,1786347,11," Learning under expected uncertainty with volatility is not simple as indicated by the range of participants' responses. However, our task, having coupled outcomes in which one or the other response is correct, does not require any exploration, or trying the different alternatives to see if things have changed. We expected the participants to know that if the button press was incorrect, then the other button would have been correct. Exploration is an important feature of learning from experience (see e.g., Cohen et al., 2007). Tasks which have more than two options automatically require exploration, as negative feedback does not show what would have been the correct response. It will accordingly be more difficult to learn when there are more alternatives. It has been acknowledged that standard reinforcement algorithms are not suitable in complex situations in which there may be many possible states or actions (e.g., Botvinick et al., 2009). Wilson and Niv (2012) compared optimal performance between a Bayesian and non-Bayesian model in their probabilistic learning task and the Bayesian model clearly had superior performance. Our finding that our ideal WL reinforcement learning model performs as well as our HMM may be restricted to the case of coupled two alternative tasks. Additionally, the level of feedback validity might affect the relative performance of the different styles of responding. These issues remain to be investigated."
58,1786347,12," We assumed that whatever decision making processes the participants use to make their responses, these remain constant for the whole task. We have assumed that the task instructions give participants enough information to form a model. Some studies have found that a Bayesian model is a better fit to human behavior only in conditions when participants have been told to expect changes in rule (Payzan-LeNestour and Bossaerts, 2011; Wilson and Niv, 2012). In our study, participants were not given such information. The instructions given to participants can affect behavior in other ways, Taylor et al. (2012) found that probability matching was influenced by whether participants had an explanation for the probabilistic outcomes."
59,1786347,13," In summary, we conclude that, with distinctions between learning from a win and a loss, reinforcement learning provides a very good description of the participant responses to repeated trials under expected uncertainty and volatility. It is able to account for individual differences with parameters that remain constant throughout all trials although the feedback validity and volatility varied. Future research should explore whether the differential treatment of a win and a loss would lead to a similar robust performance in other experimental situations."
60,2183937,0,"These results establish that the AcbC contributes to learning of actions when the outcome is delayed. Lesions of the AcbC did not impair instrumental learning when the reinforcer was delivered immediately, but substantially impaired learning with delayed reinforcement, indicating that the AcbC 'bridges' action-outcome delays during learning. Lesions made after learning also impaired performance of the instrumental response in a delay-dependent fashion, indicating that the AcbC also contributes to the performance of actions for delayed reinforcement. Finally, the lesions did not impair the perception of relative reward magnitude as assessed by responding on identical concurrent interval schedules for reinforcers of different magnitude, suggesting that the impulsive choice previously exhibited by AcbC-lesioned rats [22] is attributable to deficits in dealing with delays to reinforcement."
61,2183937,1,"Effect of delays on instrumental learning in normal animals \n  Delays have long been known to retard instrumental learning [1,37]. Despite this, normal rats have been shown to acquire free-operant responding with programmed response-reinforcer delays of up to 32 s, or even 64 s if the subjects are pre-exposed to the learning environment [1]. Delays do reduce the asymptotic level of responding [1], though the reason for this phenomenon is not clear. It may be that when subjects learn a response with a substantial response-reinforcer delay, they never succeed in representing the instrumental action-outcome contingency fully. Alternatively, they may value the delayed reinforcer slightly less; finally, the delay may also retard the acquisition of a procedural stimulus-response habit and this might account for the decrease in asymptotic responding. It is not presently known to what degree responses acquired with a response-reinforcer delay are governed by declarative processes (the action-outcome contingency plus a representation of the instrumental incentive value of the outcome) or procedural mechanisms (stimulus-response habits), both of which are known to influence instrumental responding [38,39]; it is similarly not known whether the balance of these two controlling mechanisms differs from that governing responses learned without such a delay. \n  Effect of AcbC lesions on instrumental learning and performance with or without delays \n  In the absence of response-reinforcer delays, AcbC-lesioned rats acquired an instrumental response normally, responding even more than sham-operated controls. In contrast, blockade of N-methyl-D-aspartate (NMDA) glutamate receptors in the AcbC has been shown to retard instrumental learning for food under a variable-ratio-2 (VR-2) schedule [in which P(reinforcer | response) ? 0.5] [40], as has inhibition or over-stimulation of cyclic-adenosine-monophosphate-dependent protein kinase (protein kinase A; PKA) within the Acb [41]. Concurrent blockade of NMDA and DA D1 receptors in the AcbC synergistically prevents learning of a VR-2 schedule [42]. Once the response has been learned, subsequent performance on this schedule is not impaired by NMDA receptor blockade within the AcbC [40]. Furthermore, infusion of a PKA inhibitor [41] or a protein synthesis inhibitor [43] into the AcbC after instrumental training sessions impairs subsequent performance, implying that PKA activity and protein synthesis in the AcbC contribute to the consolidation of instrumental behaviour. Thus, manipulation of Acb neurotransmission can affect instrumental learning. However, it is also clear that excitotoxic destruction of the AcbC or even the entire Acb does not impair simple instrumental conditioning to any substantial degree. Rats with Acb or AcbC lesions acquire lever-press responses on sequences of random ratio schedules [in which P(reinforcer | response) typically declines from around 1 to 0.05 over training] at near-normal levels [44,45]. In such ratio schedules, where several responses are required to obtain reinforcement, there is no delay between the final response and reinforcement, but there are delays between earlier responses and eventual reinforcement. It is therefore of interest that when differences between AcbC-lesioned rats and shams have been observed, AcbC-lesioned animals have been found to respond somewhat less than shams on such schedules late in training, when the ratio requirement is high [44,45], consistent with our present results. However, lesioned rats are fully sensitive to changes in the instrumental contingency [27,44,45]. Our present results indicate that when AcbC-lesioned rats are exposed to a FR-1 schedule for food [P(reinforcer | response) = 1] in the absence of response-reinforcer delays, they acquire the response at normal rates. \n  In contrast, when a delay was imposed between responding and reinforcement, AcbC-lesioned rats were impaired relative to sham-operated controls, in a systematic and delay-dependent fashion. The observation that learning was not affected at zero delay rules out a number of explanations of this effect. For example, it cannot be that AcbC-lesioned rats are in some way less motivated for the food per se, since they responded normally (in fact, more than shams) when the food was not delayed. Thus although the Acb and its dopaminergic innervation are clearly very important in motivating behaviour e.g. [23,46-48], this is not on its own a sufficient explanation for the present results. An explanation in terms of a rate-dependent impairment is also not tenable, since the AcbC-lesioned rats were capable (in the zero-delay condition) of responding at a level greater than they exhibited in the non-zero-delay conditions. Depletion of Acb DA also impairs rats' ability to work on high-effort schedules, where many, or very forceful, responses are required to obtain a given amount of food [47,48]. However, in the present experiments the ratio requirement (one response per reinforcer) and the force required per press were both held constant across delays, so this effect cannot explain the present results. Similarly, although AcbC lesions are known to impair the control over behaviour by Pavlovian conditioned stimuli e.g. [23,29,49-52], there was no Pavlovian stimulus that was differentially associated with delayed as opposed to immediate reinforcement in this task, so this cannot explain the present results. \n  Our results also indicated that when there were programmed delays to reinforcement, AcbC-lesioned animals experienced longer response-reinforcer collection delays, partly due to their failure to collect the reinforcer as promptly as shams. These additional experienced delays probably retarded learning. However, in addition to this effect, there was a further deficit exhibited by AcbC-lesioned rats: even allowing for the longer response-collection delays that they experienced, their instrumental learning was impaired more by delays than that of sham-operated controls. Deficits in learning with delayed reinforcement may account for some of the variability in the effect of AcbC lesions or local pharmacological manipulations on instrumental learning across different schedules. \n  The fact that pre-exposure to the context improves instrumental learning in normal rats [1] suggests one possible mechanism by which AcbC lesions might retard learning when delays are present. When a reinforcer arrives, it may be associated either with a preceding response, or with the context. Therefore, in normal animals, pre-exposure to the context may retard the formation of context-reinforcer associations by latent inhibition, or it might serve to retard the formation of associations between irrelevant behaviours and reinforcement. Similarly, non-reinforced exposure to the context forces the subjects to experience a zero-response, zero-reinforcer situation, i.e. P(outcome | no action) = 0. When they are then exposed to the instrumental contingency, such that P(outcome | action) > 0, this prior experience may enhance their ability to detect the instrumental contingency ?P = P(outcome | action) - P(outcome | no action). In one aversive Pavlovian conditioning procedure in which a conditioned stimulus (CS) was paired with electric shock, AcbC lesions have been shown to impair conditioning to discrete CSs, but simultaneously to enhance conditioning to contextual (background) CSs [53], though not all behavioural paradigms show this effect [54,55]. It is therefore possible that enhanced formation of context-reinforcer associations may explain the retardation of response-reinforcer learning in AcbC-lesioned rats in the presence of delays. \n  The instrumental task used requires animals either to associate their response with the delayed food outcome (an action-outcome association that can be used for goal-directed behaviour), or to strengthen a stimulus-response association (habit) when the reinforcer eventually arrives [38,39]. Both mechanisms require the animal to maintain a representation of their past action so it can be reinforced (as a habit) or associated with food when the food finally arrives. This mnemonic requirement is not obviated even if the animal learns to predict the arrival of food using discriminative stimuli, and uses these stimuli to reinforce its responding (conditioned reinforcement): in either case, since the action precedes reinforcement, some trace of past actions or stimuli must persist to be affected by the eventual delivery of food. \n  A delay-dependent impairment was also seen when AcbC lesions were made after training. This indicates that the AcbC does not only contribute to the learning of a response when there is an action-outcome delay: it also contributes to the performance of a previously-learned response. Again, AcbC-lesioned rats were only impaired when that previously-learned response was for delayed (and not immediate) reinforcement. Of course, learning of an instrumental response depends upon the animal being able to perform that response; preventing an animal from pressing a lever (a performance deficit) would clearly impair its ability to learn an instrumental response on that lever to obtain food. In the present set of experiments, it is clear that AcbC-lesioned rats were just as able to perform the response itself (to press the active lever and to discriminate it physically from the inactive lever) as controls, as shown by their normal performance in the zero-delay condition, so it is not clear whether the delay-dependent impairments in learning and performance can be attributed to the same process. Again, since responding was unaffected in the zero-delay condition, many alternative interpretations (such as a lack of motivation to work for the food) are ruled out. It may be that AcbC-lesioned rats are impaired at representing a declarative instrumental action-outcome contingency when the outcome is delayed, or in forming or executing a procedural stimulus-response habit when the reinforcing event does not follow the response immediately. It may also be that they represent the action-outcome contingency normally but value the food less because it is delayed, and that this affects responding in a free-operant situation even though there is no alternative reinforcer available. \n  Excitotoxic lesions of the whole Acb do not prevent rats from detecting changes in reward value (induced either by altering the concentration of a sucrose reward or by changing the deprivational state of the subject) [27]. Such lesions also do not impair rats' ability to respond faster when environmental cues predict the availability of larger rewards [28], and nor does inactivation of the Acb with local anaesthetic or blockade of AMPA glutamate receptors in the Acb [56]; the effects of intra-Acb NMDA receptor antagonists have varied [57,58]. AcbC-lesioned rats can still discriminate large from small rewards [24,25]. Similarly, DA depletion of the Acb does not affect the ability to discriminate large from small reinforcers [59-61], and systemic DA antagonists do not affect the perceived quantity of food as assessed in a psychophysical procedure [62]. Our study extends these findings by demonstrating that excitotoxic AcbC lesions do not impair rats' ability to allocate their responses across two schedules in proportion to the experienced reinforcement rate, even when the two schedules are identical except in the magnitude of the reinforcements they provide, thus demonstrating their sensitivity to reinforcer magnitude is quantitatively no worse than shams'. In this experiment, there was substantial undermatching, but this is common [33,63] see also [64,65]; differential cues signalling the two rewards might have improved matching but were not used in the present experiments since it is known that AcbC lesions can themselves affect rats' sensitivity to cues signalling reinforcement [23,29,49-52]. Given that AcbC-lesioned subjects showed a reduced probability of switching between two identical RI schedules, it may be the case that an enhanced sensitivity to the COD accounts for the better matching exhibited by the AcbC-lesioned rats [34]. Alternatively, the lesion may have enhanced reinforcer magnitude discrimination or improved the process by which behaviour allocation is matched to environmental contingencies. In summary, the present results suggest that AcbC damage leads to pathological impulsive choice (preferring a small, immediate reinforcer to a large, delayed reinforcer) [22] not through any relative lack of value of large reinforcers, but through a specific deficit in responding for delayed reinforcement."
62,2183937,2," Contribution of the AcbC to reinforcement learning \n  The term 'reinforcement learning' simply means learning to act on the basis of reinforcement received; it is a term used in artificial intelligence research [66] that does not specify the mechanism of such learning [67,68]. Our present results indicate that the AcbC is a reinforcement learning structure that is critical for instrumental conditioning when outcomes are delayed, consistent with electrophysiological and functional neuroimaging evidence indicating that the ventral striatum responds to recent past actions [10,15] and to predicted future rewards [8-15], and with computational models suggesting a role for the striatum in predicting future primary reinforcement [20,21]. However, when reward is certain and delivered immediately, the AcbC is not necessary for the acquisition of instrumental responding. The delay-dependent role of the AcbC indicates that it plays a role in allowing actions to be reinforced by bridging action-outcome delays through a representation of past acts or future rewards. Acb lesions have also produced delay-dependent impairments in a delayed-matching-to-position task [69,70]; their effects on the delayed-matching-to-sample paradigm have also been studied, but a more profound and delay-independent deficit was observed, likely due to differences in the specific task used [71]. Finally, the AcbC is not alone in containing neurons that respond to past actions and future rewards. The dorsal striatum is another such structure [10,15,72,73]; expression of stimulus-response habits requires the dorsal striatum [74,75], and the rate at which rats learn an arbitrary response that delivers electrical stimulation to the substantia nigra is correlated with the degree of potentiation of synapses made by cortical afferents onto striatal neurons, a potentiation that requires DA receptors [76,77]. The prelimbic area of rat prefrontal cortex is important for the detection of instrumental contingencies and contributes to goal-directed, rather than habitual, action [78,79]. Similarly, the orbitofrontal cortex and basolateral amygdala encode reinforcement information and project to the AcbC, and lesions of these structures can produce impulsive choice see [24,80-82]. It is not yet known whether lesions of these structures also impair learning with delayed reinforcement."
63,908754,0,"We applied microstimulation in SN of patients undergoing DBS for the treatment of PD as they performed a reinforcement learning task. We found that microstimulation applied during the 500-ms post-reward interval impaired learning. These results demonstrate a causal relation between post-reward SN firing and human reinforcement learning as microstimulation is known to acutely enhance local neural firing (Histed et al., 2009). We hypothesized that the effect of SN microstimulation on learning would vary based on their relative proximity to dopaminergic (DA) neurons that guide reinforcement learning (Glimcher, 2011) or GABAergic neurons that exert inhibitory control on DA neurons (Damier et al., 1999a; Lobb et al., 2011; Ramayya et al., 2014b). As hypothesized, we observed the largest stimulation-related impairments in learning when the electrode was positioned near neurons with relatively high firing rates and narrow waveforms, properties characteristic of GABA neurons (Joshua et al., 2009; Matsumoto and Hikosaka, 2009; Ungless and Grace, 2012). Thus, our results suggest that microstimulation near GABA neurons impairs reinforcement learning."
64,908754,1," This finding provides direct evidence relating phasic SN neural firing to human reinforcement learning. It goes beyond animal electrophysiology studies that may not generalize to human learning because they typically involve long periods of intense training. It also goes beyond prior human studies of reinforcement learning; functional neuroimaging studies cannot test a causal role for SN neural activity (Montgomery et al., 2009), and pharmacological manipulations of DA in patients with PD (Frank et al., 2004; Rutledge et al., 2009) cannot distinguish phasic neural activity from tonic changes in DA throughout the brain (Niv et al., 2007). Ramayya et al. (2014a) also showed a stimulation-related decrease in performance. However, because rewards in that study were contingent on stimuli, but independent of actions, the observed stimulation-related decrease in performance could either be attributed to an impairment of learning or a selective strengthening of action-reward associations that competed with stimulus-reward learning. Our current study overcame this limitation by using an experimental design with consistent stimulus-response mapping, such that stimulus-reward and action-reward associations were always correlated. Thus, our finding of a stimulation-related impairment in performance suggests decreased learning."
65,908754,2," In Ramayya et al. (2014a), stimulation-related decreases in performance were correlated with an increased propensity to repeat the same action following reward, particularly when the electrode was positioned near putative DA neurons, suggesting that microstimulation near SN DA neurons enhanced action-reward learning. The current finding that stimulation near putative GABA neurons produced impairments in reinforcement suggests opposing roles of DA and GABA neurons during reinforcement learning. Specifically, if phasic bursts of SN DA neurons encode reward prediction errors that result in subsequent learning (Glimcher, 2011), and SN GABA neurons provide inhibitory inputs to local DA neurons (Tepper et al., 1995; Luscher and Ungless, 2006; Lobb et al., 2011; Henny et al., 2012; Pan et al., 2013), then one would observe enhanced learning when stimulating DA neurons (Ramayya et al., 2014a), but impaired reinforcement learning following microstimulation of SN GABA neurons. This explanation is also supported by our observation of opposing post-reward firing responses from putative DA and GABA neurons in the human (Ramayya et al., 2014b)."
66,908754,3," It is difficult to interpret whether the observed changes reinforcement learning were related to changes in stimulus-reward and/or action-reward learning because these forms of learning were perfectly correlated in the current experimental design. That we did not observe robust stimulation-related changes in learning near putative DA sites is difficult to interpret when considering our previous finding that microstimulation near putative DA neurons enhances action-reward learning (de Berker and Rutledge, 2014; Ramayya et al., 2014a). It is possible that we did not sample from a functional population of DA neurons in this study, as suggested by the absence of phasic post-reward bursts in activity from putative DA neurons in this study, unlike our previous studies (Ramayya et al., 2014a,b). Alternatively, it is possible that stimulation near SN DA neurons has a specific effect on action-reward learning that was not evident in this study because it was masked by simultaneous stimulus-reward learning."
67,908754,4," An alternative explanation for how microstimulation of SN GABA neurons might have resulted in impaired learning is that stimulation may have caused a behavioral change during the post-reward interval that impaired subjects' learning during those trials. Several studies have linked the firing of SN GABA neurons in the pars reticulata subregion (that contains the majority of SN GABA neurons; Nair-Roberts et al., 2008) to regulation of downstream movement and saccade-generating structures (e.g., superior colliculus; Carpenter et al., 1976; DeLong et al., 1983; Hikosaka and Wurtz, 1983). If microstimulation of SN GABA neurons suppressed orienting saccades that likely occurred in response to the presentation of salient reward stimuli (in this case, a silver dollar and the sound of cash register; Hikosaka and Wurtz, 1983), then reward stimuli presented during stimulation trials might be associated with diminished salience and result in reduced learning. However, this is unlikely to be the case because non-human primate studies have shown that SN microstimulation has a limited influence on visually-guided saccades (Mahamed et al., 2011)."
68,908754,5," We note several limitations to our study. First, we are unable to provide direct histochemical evidence that electrophysiological parameters (spike rate and waveform duration) indicate distinct neuronal populations, however, a large body of evidence from animal studies suggest that these electrophysiological criteria may be used to identify distinct midbrain neuronal populations (Ungless and Grace, 2012). Second, we did not observe stimulation-related changes in learning near putative DA neurons in this study, whereas we observed such changes in our previous microstimulation study (Ramayya et al., 2014a). This likely reflects reduced sampling of DA neurons during this experiment, which is consistent with the fact that we did not observe post-reward bursts of activity in this study (a marker of DA activity), in contrast to Ramayya et al. (2014a). Finally, the population we studied—patients undergoing DBS surgery for PD—is known to have degeneration of DA neurons in SN. Even though this poses the challenge of interpreting findings concerning the functional role of SN neurons in patients who have degenerative disease, histological studies in PD patients (Damier et al., 1999b), and electrophysiological studies in rat models of PD (Hollerman and Grace, 1990; Zigmond et al., 1990), and humans (Zaghloul et al., 2009; Ramayya et al., 2014b) indicate that a significant population of viable neurons remain in the parkinsonian SN. Taken together with the clear evidence of learning that subjects demonstrated during the task, we suggest that the neural processes we describe reflect the subpopulation of healthy neurons that remain in the SN."
